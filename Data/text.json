{
    "text": "THE FAST FOURIER \nTRANSFORM \nAND ITS APPLICATIONS \nE. Oran Brigham \nAvantek, Inc. \nPrentice Hall \nEnglewood Cliffs, New Jersey 07632 \nLibrary of Congress Cataloging-in-Publication Data \nBrigham, E. Oran \nThe fast Fourier transform and its applications I E. Oran Brigham. \np. \ncm. -\n(Prentice-Hall signal processing series) \nContinues: The fast Fourier transform. \nBibliography: p. \nIncludes index. \nISBN 0-13-307505-2 \nI. Fourier transformations. I. Title. II. Series \nQA403.B75 1988 \n515.7'23-dcI9 \nEditorial/production supervision and \ninterior design: Gertrude Szyferblatt \nCover design: Diane Saxe \nManufacturing buyer: Barbara Kittle/Cindy Grant \nÂ© 1988 by Prentice-Hall, Inc. \nA Division of Simon & Schuster \nEnglewood Cliffs, New Jersey 07632 \nAll rights reserved. No part of this book may be \nreproduced, in any form or by any means, \nwithout permission in writing from the publisher. \nPrinted in the United States of America \n10 9 8 7 6 5 4 3 \nISBN \n0-13-307505-2 \nPrentice-Hall International (UK) Limited, LClIldon \nPrentice-Hall of Australia Pty. Limited, Syd/ley \nPrentice-Hall Canada Inc., Toronto \nPrentice-Hall Hispanoamericana, S.A., Mexico \nPrentice-Hall of India Private Limited, New De/hi \nPrentice-Hall of Japan, Inc., Tokyo \nSimon & Schuster Asia Pte. Ltd., Singapore \nEditora Prentice-Hall do Brasil, Ltda., Rio de Janeiro \nTo Cami \nA very special daughter \nCONTENTS \nPREFACE \nxiii \nCHAPTER 1 \nINTRODUCTION \n1 \n1.1 \nThe Ubiquitous FFT \n1.2 \nInterpreting the Fourier Transform 4 \n1.3 \nDigital Fourier Analysis 7 \nCHAPTER 2 \nTHE FOURIER TRANSFORM \n9 \n2.1 \nThe Fourier Integral 9 \n2.2 \nThe Inverse Fourier Transform II \n2.3 \nExistence of the Fourier Integral \n13 \n2.4 \nAlternate Fourier Transform \nDefinitions 22 \n2.5 \nFourier Transform Pairs 23 \nCHAPTER 3 \nFOURIER TRANSFORM PROPERTIES \n30 \n3.1 \nLinearity 30 \n3.2 \nSymmetry 32 \n3.3 \nTime and Frequency Scaling 32 \n3.4 \nTime and Frequency Shifting 35 \nvii \nviii \n3.5 \n3.6 \n3.7 \n3.8 \n3.9 \nAlternate Inversion Formula 40 \nEven and Odd Functions 40 \nWaveform Decomposition 42 \nComplex Time Functions 44 \nSummary Table of Fourier Transform \nProperties 46 \nCHAPTER 4 \nCONVOLUTION AND CORRELATION \n4.1 \nConvolution Integral 50 \n4.2 \nGraphical Evaluation of the \nConvolution Integral \n51 \n4.3 \nAlternate Form of the \nConvolution Integral 54 \n4.4 \nConvolution Involving Impulse \nFunctions 57 \n4.5 \nTime-Convolution Theorem 60 \n4.6 \nFrequency-Convolution Theorem 64 \n4.7 \nCorrelation Theorem 65 \nCHAPTER 5 \nFOURIER SERIES AND SAMPLED WAVEFORMS \n5.1 \nFourier Series 74 \n5.2 \nFourier Series as a Special Case of the \nFourier Integral 77 \n5.3 \nWaveform Sampling 79 \n5.4 \nSampling Theorems 83 \nCHAPTER 6 \nTHE DISCRETE FOURIER TRANSFORM \n6.1 \nA Graphical Development 90 \n6.2 \nTheoretical Development 92 \n6.3 \nDiscrete Inverse Fourier \nTransform 97 \n6.4 \nRelationship Between the Discrete and \nContinuous Fourier Transform 98 \n6.5 \nDiscrete Fourier Transform \nProperties \n107 \nContents \n50 \n74 \n89 \nCHAPTER 7 \nDISCRETE CONVOLUTION AND CORRELATION \n118 \n7.1 \nDiscrete Convolution 118 \n7.2 \nGraphical Interpretation of Discrete \nConvolution 119 \n7.3 \nRelationship Between Discrete and \nContinuous Convolution \n121 \n7.4 \nGraphical Interpretation of Discrete \nCorrelation \n127 \nCHAPTER 8 THE FAST FOURIER TRANSFORM (FFT) \n8.1 \nMatrix Formulation \n131 \n8.2 \nIntuitive Development 132 \n8.3 \nSignal Flow Graph 136 \n8.4 \nDual Nodes 138 \n8.5 \nWP Determination 140 \n8.6 \nUnscrambling the FFT 141 \n8.7 \nFFT Computation FlowChart \n141 \n8.8 \nFFT BASIC and PASCAL Computer \nPrograms \n145 \n8.9 \nTheoretical Development of the Base-2 \nFFT Algorithm 148 \n8.10 \nFFT Algorithms for Arbitrary \nFactors 156 \nCHAPTER 9 FFT TRANSFORM APPLICATIONS \n9.1 \nFourier Transform Applications \n167 \n9.2 \nFFT Data-Weighting Functions \n178 \n9.3 \nFFT Algorithms for Real Data 188 \n9.4 \nInverse Fourier Transform \nApplications \n195 \n9.5 \nLaplace Transform Applications \n199 \nCHAPTER 10 \nFFT CONVOLUTION AND CORRELATION \n10.1 \nFFT Convolution of Finite-Duration \nWaveforms 204 \n131 \n167 \n204 \nx \n10.2 \n10.3 \n10.4 \nFFT Convolution of Infinite- and \nFinite-Duration Waveforms 211 \nEfficient FFT Convolution 223 \nFFT Correlation of Finite-Duration \nWaveforms 225 \nCHAPTER 11 \nTWO-DIMENSIONAL FFT ANALYSIS \n11.1 \nTwo-Dimensional Fourier \nTransforms 232 \n11.2 \nTwo-Dimensional FFTs 240 \n11.3 \nTwo-Dimensional Convolution and \nCorrelation 255 \n11.4 \nTwo-Dimensional FFT Convolution \nand Correlation 260 \nCHAPTER 12 \nFFT DIGITAL FILTER DESIGN \n12.1 \nFFT Time-Domain Digital Filter \nDesign 273 \n12.2 \nFFT Frequency-Domain Digital Filter \nDesign 280 \nCHAPTER 13 \nFFT MULTICHANNEL BAND-PASS FILTERING \n13.1 \nFFT Band-Pass Integrate and Sample \nFilters 291 \n13.2 \nFFT Band-Pass Filter Frequency-\nResponse Characteristics 299 \n13.3 \nMultichannel Band-Pass Filtering by \nShifted FFTs 303 \n13.4 \nSample Rate Considerations in FFT \nMultichannel Filtering 313 \n13.5 \nFFT Multichannel Demultiplexing 315 \nCHAPTER 14 \nFFT SIGNAL PROCESSING AND SYSTEM \nAPPLICATIONS \n14.1 \nSampling Band-Pass Signals 320 \n14.2 \nQuadrature Sampling 327 \n14.3 \nFFT Signal Detection 337 \nContents \n232 \n272 \n291 \n320 \nContents \nxi \n14.4 \nFFT Cepstrum Analysis: Echo and \nMultipath Removal 341 \n14.5 \nFFT Deconvolution 345 \n14.6 \nFFT Antenna Design Analysis 349 \n14.7 \nFFT Phase-Interferometer \nMeasurement System 355 \n14.8 \nFFT Time-Difference-of-Arrival \nMeasurement System 357 \n14.9 \nFFT System Simulation 360 \n14.10 \nFFT Power-Spectrum Analysis 365 \n14.11 \nFFT Beamforming 376 \nAppendix A \nThe Impulse Function: A Distribution \n386 \nA.l \nImpulse-Function Definitions 386 \nA.2 \nDistribution Concepts 388 \nA.3 \nProperties of Impulse Functions 390 \nA.4 \nTwo-Dimensional Impulse \nFunctions 392 \nBIBLIOGRAPHY \n394 \nINDEX \n446 \nPREFACE \nThe fast Fourier transform (FFT) is a widely used signal-processing and \nanalysis concept. Availability of special-purpose hardware in both the com-\nmercial and military sectors has led to sophisticated signal-processing sys-\ntems based on the features of the FFT. The implementation of FFT algo-\nrithms on large mainframe computers has made unprecedented solution \ntechniques readily achievable. Personal computers have generated yet a fur-\nther proliferation of FFT applications. To the student, the professional at \nhome, engineers, computer scientists, and research analysts, the FFT has \nbecome an invaluable problem-solving tool. \nPopularity of the FFT is evidenced by the wide variety of application \nareas. In addition to conventional radar, communications, sonar, and speech \nsignal-processing applications, current fields of FFT usage include biomed-\nical engineering, imaging, analysis of stock market data, spectroscopy, \nmetallurgical analysis, nonlinear systems analysis, mechanical analysis, \ngeophysical analysis, simulation, music synthesis, and the determination \nof weight variation in the production of paper from pulp. Clearly, an appli-\ncations text cannot address in depth such a breadth of technology. The \nobjective of this book is to provide the foundation from which one can ac-\nquire the fundamental knowledge to apply the FFT to problems of interest. \nThe book is designed to be user \nfriendly. We stress a pictorial, intuitive \napproach supported by mathematics, rather than an elegant exposition that \nis difficult to read. Every major concept is developed by a three-stage se-\nquential process. First, the concept is introduced by an intuitive graphical \ndevelopment. Second, a nonsophisticated (but theoretically sound) mathe-\nxiii \nxlv \nPreface \nmatical treatment is developed to support the intuitive arguments. The third \nstage consists of practical examples designed to review and expand the con-\ncept. This three-step procedure, with an emphasis on graphical techniques, \ngives meaning as well as mathematical substance to the basic properties and \napplications of the FFT. Readers should expect a high efficiency in trans-\nferring the development of the text into practical applications. \nThis book is a sequel to The Fast Fourier Transform. The focus of the \noriginal volume was on the Fourier transform, the discrete Fourier trans-\nform, and the FFT. Only a cursory examination of FFT applications was \npresented. This text extends the original volume with the incorporation of \nextensive developments of fundamental FFT applications. Applications of \nthe FFT are based on its unique property to rapidly compute the Fourier, \ninverse Fourier, or Laplace transforms. For this reason, we develop in detail \nthe methods for applying the FFT to transform analysis and interpreting \nresults. We then extend the development and apply the FFT to the com-\nputation of convolution and correlation integrals. All developments employ \na rich use of graphical techniques and examples to insure clarity of the \npresentation. We then build on these fundamentals and expand the basic \nFFT uses to a higher level of application topics. Topical areas include two-\ndimensional FFT analysis, FFT digital filter design, FFT multichannel band-\npass filtering, FFT signal processing, and FFT systems applications. \nThe text should provide an excellent basis for a senior level or intro-\nductory graduate course on digital signal processing. Course outlines em-\nphasizing a thorough examination of the Fourier Transform will find the text \nparticularly appealing. The added applications material allows students to \ndevelop the experience necessary to apply the FFT to problems spanning a \nwide variety of disciplines. Students are expected to have access to a digital \ncomputer. The text should serve equally well as a supplementary text for a \ncourse with broad systems analysis and signal-processing objectives. The \nbook should also be very attractive as a reference to the practicing signal-\nprocessing community because it offers not only a readable introduction to \nthe FFT, but a thorough and unified reference for applying the FFT to any \nfield of interest. Readers should also find that the material provides an ex-\ncellent self-study text. \nThe text is divided into five major subject areas: \n1. The Fourier Transform. In Chapters 2 through 6, we lay the foun-\ndation for the entire book. We investigate the Fourier transform, its inversion \nformula, and its basic properties; graphical explanation of each discussion \nlends physical insight to the concept. The transform properties of the con-\nvolution and correlation integrals are explored in detail. Numerous examples \nare presented to facilitate understanding. For reference in later chapters, \nFourier series and waveform sampling of baseband signals are developed in \nterms of Fourier transform theory. \nPreface \nxv \n2. The Discrete Fourier Transform. Chapters 6 and 7 develop the dis-\ncrete Fourier transform. A graphical presentation develops the discrete \ntransform from the continuous Fourier transform. This graphical presen-\ntation is substantiated by a theoretical development. Discrete transform \nproperties are derived. The relationship between the discrete and continuous \nFourier transform is explored in detail; numerous waveform classes are con-\nsidered by illustrative examples. Discrete convolution and correlation are \ndefined and compared with continuous equivalents by illustrative examples. \n3. The Fast Fourier Transform. In Chapter 8, we develop the FFT \nalgorithm. A simplified explanation of why the FFT is efficient is presented. \nWe follow with the development of \na signal flow graph, a graphical procedure \nfor examining the FFT. Based on this flow graph, we describe sufficient \ngeneralities to develop a computer flowchart and computer programs. Theo-\nretical developments of the various forms of the FFT are presented. \n4. Basic Applications of \nthe FFT. Chapters 9 through 11 focus on an \ninvestigation of the basic applications of the FFT. Application of the FFT \nto the computation of discrete and inverse discrete Fourier transforms is \npresented with emphasis on a graphical examination of resolution and com-\nmon FFT user mistakes (aliasing, time-domain truncation, noncausal time \nfunctions, and periodic functions). FFT data-weighting functions are ex-\namined in depth. Laplace transform computation using the FFT is presented \nwith graphical examples. FFT implementation of discrete convolution and \ncorrelation is developed by extensive graphical presentations. Computa-\ntional procedures are carefully defined and a computer program is provided. \nTwo-dimensional Fourier transforms, convolution, and correlation are de-\nveloped (graphically and by example), as in the one-dimensional case. Ap-\nplication of the FFT to two-dimensional Fourier transform and convolution \ncomputation are described and computer programs are provided. \nS. Signal Processing and System FFT Applications. The design and \napplication of digital filters using the FFT is explored from a practical usage \nperspective. A novel application of the FFT to multichannel band-pass fil-\ntering is developed in a manner from which the reader can readily expand \nthe results. \nBecause waveform sampling is fundamental to FFT signal-processing \napplications, band-pass and quadrature waveform sampling is addressed in \ndetail. The philosophy underlying the remaining discussions is to address a \nrange of FFT techniques that are applicable to sonar, seismic, radar, com-\nmunications, medical, optical, system analysis, and antenna applications. \nSpecific FFT application areas addressed include signal-to-noise enhance-\nment, matched filtering, deconvolution filtering, time-difference-of-arrival \nmeasurements, phase interferometry measurements, antenna analysis, sys-\ntem simulation, power spectrum analysis, and array beamforming. \nxvi \nPreface \nI would like to take this opportunity to express my sincere appreciation \nto the many individuals who have contributed to the contents of this book. \nA special note of thanks goes to Dr. Patty Patterson, who contributed sig-\nnificantly in correcting and improving the manuscript. Charlene Rushing and \nNeil Ishman contributed to the computer subroutines. \nTo my wife Vangee, I am indebted for her patience and understanding \nfor the many hours I have stolen from her life while preparing the manuscript. \nTo my daughter Cami, I thank you for your efforts, dedication, and enthu-\nsiasm towards a commitment to excellence; I hope some of your ideals are \nincorporated in this book. \n1 \nINTRODUCTION \nThe fast Fourier transform (FFT) is a fundamental problem-solving tool in \nthe educational, industrial, and military sectors. Since 1965 [1], FFT usage \nhas rapidly expanded and personal computers fuel an explosion of additional \nFFT applications. The single focus of this book is the FFT and its applications. \nIn this chapter, we survey briefly the broad application areas of the \nFFT to give the reader a perspective for its seemingly universal appeal. We \nwill establish the FFT as one of the major developments in signal-processing \ntechnology. The diverse applications of the FFT follow from the roots of \nthe FFT: the discrete Fourier transform and hence the Fourier transform. \nOur overview of the Fourier transform and its interpretation with respect \nto the time and frequency domains is presented. \n1.1 THE UBIQUITOUS FFT \nUbiquitous is defined as being everywhere at the same time. The FFT is \ncertainly ubiquitous because of the great variety of apparent unrelated fields \nof application. However, we know that the proliferation of applications \nacross broad and diverse areas is because they are united by a common \nentity, the Fourier transform. For years only the elitist theoretical mathe-\nmatician was capable of staying abreast of such a broad spectrum of tech-\nnologies. However, with the FFT, Fourier analysis has been reduced to a \nreadily available and practical procedure that can be applied effectively with-\n2 \nApplied Mechanics \nâ¢ structural dynamics \nâ¢ aircraft wing-flutter \nsuppression \nâ¢ machinery dynamics \ndiagnostics \nâ¢ nuclear power plant \nmodeling \nâ¢ vibration analysis \nSonics and Acoustics \nâ¢ acoustic imaging \nâ¢ passive sonar \nâ¢ ultrasonic transducers \nâ¢ array processing \nâ¢ architecture acoustic \nmeasurement \nâ¢ music synthesis \nBiomedical Engineering \nâ¢ diagnosis of airways \nobstruction \nâ¢ muscle fatigue \nmonitoring \nâ¢ assessing heart valve \ndamage \nâ¢ tissue structure \ncharacterization \nâ¢ gastric disturbances \ninvestigation \nâ¢ cardiac patients diagnosis \nâ¢ EeG data compression \nâ¢ artery dynamics \ninvestigation \nNumerical Methods \nâ¢ high-speed interpolation \nâ¢ conjugate gradient method \nâ¢ boundary value problems \nâ¢ Riccati and Dirichlet \nequations \nâ¢ Rayleigh's integral \nIntroduction \nChap. 1 \nâ¢ Wiener-Hopf integral \nequation \nâ¢ diffusion equation \nâ¢ numerical integration \nâ¢ Karhunen-Loeve transform \nâ¢ elliptic differential \nequations \nSignal Processing \nâ¢ matched filters \nâ¢ deconvolution \nâ¢ real-time spectral analysis \nâ¢ cepstrum analysis \nâ¢ coherence function \nestimation \nâ¢ speech synthesis and \nrecognition \nâ¢ random process generation \nâ¢ transfer function \nestimation \nâ¢ echo/reverbation removal \nInstrumentation \nâ¢ chromatography \nâ¢ microscopy \nâ¢ spectroscopy \nâ¢ x-ray diffraction \nâ¢ electrochronography \nRadar \nâ¢ cross-section measurement \nâ¢ moving target indicator \nâ¢ synthetic aperture \nâ¢ doppler processor \nâ¢ pulse compression \nâ¢ clutter rejection \nElectromagnetics \nâ¢ micros \ntrip line propagation \nâ¢ conducting bodies \nscattering \nFigure 1.1 Summary of FFT Applications. \nSec. 1.1 \nThe Ubiquitous FFT \nâ¢ antenna radiation patterns \nâ¢ dielectric substrate \ncapacitance \nâ¢ phased-array antenna \nanalysis \nâ¢ time-domain reflectometry \nâ¢ waveguide analysis \nâ¢ network analysis \nCommunications \nâ¢ systems analysis \nâ¢ transmultiplexers \nâ¢ demodulators \nâ¢ speech scrambler system \nâ¢ multichannel filtering \nâ¢ M \n-ary signaling \nâ¢ signal detection \nâ¢ high-speed digital filters \nâ¢ voice coding systems \nâ¢ video bandwidth \ncompression \nMiscellaneous \nâ¢ magnetotellurics \nâ¢ metallurgy \nâ¢ electrical power systems \nâ¢ image \nrestoration \n3 \nâ¢ nonlinear system analysis \nâ¢ geophysics \nâ¢ GaAs FET transient \nresponse \nâ¢ integrated circuit modeling \nâ¢ quality control \nFigure 1.1 (cont.) \nout sophisticated training or years of experience. The FFT has become a \nstandard analysis module because of its usefulness and availability. \nThe FFT is no longer a textbook novelty. In Fig. 1.1, we show an \nabbreviated listing of typical application areas of the FFT. Key reference \nmaterials in the FFT application fields shown are included in the bibliog-\nraphy. The FFT, once the province of engineers and scientists, has become \na technique used in areas ranging from the analysis of stock market trends \nto the determination of weight variations in the production of paper from \npUlp. Computer technology evolution, particularly that of the personal com-\nputer, has positioned the FFT as a handy and powerful analysis tool whose \navailability is no longer limited only to the signal-processing specialist. As \nshown in Fig. 1.1, the application fields of the FFT are extremely diverse. \nIn an age where it is virtually impossible to stay abreast of technology, it is \nstimulating to find an analysis concept that enables one to approach an un-\nfamiliar field with familiar tools. Certainly, the FFT has become one of the \nmajor developments in digital signal-processing technology. \nAs stated previously, the common bond throughout the varied appli-\ncation of the FFT is the Fourier transform. A key property of the Fourier \ntransform is its ability to allow one to examine a function or waveform from \nthe perspective of both the time and frequency domains. The Fourier trans-\nform is the cornerstone of this text. \n4 \nIntroduction \nChap. 1 \n1.2 INTERPRETING THE FOURIER TRANSFORM \nA simplified interpretation of the Fourier transform is illustrated in Fig. 1.2. \nAs shown, the essence of the Fourier transform of a waveform is to decom-\npose or separate the waveform into a sum of sinusoids of different frequen-\ncies. If these sinusoids sum to the original waveform, then we have deter-\nmined the Fourier transform of the waveform. The pictorial representation \nof the Fourier transform is a diagram that displays the amplitude and fre-\nquency of each of the determined sinusoids. \nFigure 1.2 also illustrates an example of the Fourier transform of a \nsimple waveform. The Fourier transform is the two sinusoids that add to \nyield the waveform. As shown, the Fourier transform diagram displays both \nthe amplitude and frequency of each sinusoid. We have followed the usual \nconvention and displayed both positive and negative frequency sinusoids \nfor each frequency; the amplitude has been halved accordingly. The Fourier \ntransform then decomposes the example waveform into its two individual \nsinusoidal components. \nThe Fourier transform identifies or distinguishes the different fre-\nquency sinusoids (and their respective amplitudes) that combine to form an \narbitrary waveform. Mathematically, this relationship is stated as \nS(f) = I: \nex> s(t)e - j2-rrft dt \n(1.1) \nwhere s(t) is the waveform to be decomposed into a sum of sinusoids, S(f) \nis the Fourier transform of s(t), andj = '\\!'-=t. An example of the Fourier \ntransform of a square-wave function is illustrated in Fig. 1.3(a). An intuitive \njustification that a square waveform can be decomposed into the set of sin-\nusoids determined by the Fourier transform is shown in Fig. 1.3(b). \nWe normally associate the analysis of periodic functions such as a \nsquare wave with Fourier series rather than Fourier transforms. However, \nas we will show in Chapter 5, the Fourier series is a special case of the \nFourier transform. \nIf the waveform s(t) is not periodic, then the Fourier transform will \nbe a continuous function of frequency, that is, s(t) is represented by the \nsummation of \nsinusoids of all frequencies. For illustration, consider the pulse \nwaveform and its Fourier transform, as shown in Fig. 1.4. In this example, \nthe Fourier transform indicates that one sinusoid frequency becomes indis-\ntinguishable from the next and, as a result, all frequencies must be \nconsidered. \nThe Fourier transform is then a frequency-domain representation of a \nfunction. As illustrated in both Figs. 1.3(a) and 1.4, the Fourier transform \nfrequency domain contains exactly the same information as that of the orig-\nWAVEFORM DEFINED \nFROM-,. TO+,. \nen \nFOURIER TRANSFORM \nSynthesize a summation of sinusoids \nwhich add to give the waveform \nT \n~\n2. \n.. '\" \n't, \nv- .. \n-T \nII \n2\" \n\\ \nf\\~h (\\ \n'j ~ \nIf\\ â¢ \n-I \nT \n. \nt \n6 \n6\" \nFourier transform \nConstruct a diagram which \ndisplays amplitude and \nfrequency of each sinusoid \nFourier Transform \n'h \nfrequency \nFigure 1.2 Interpretation of the Fourier transform. \n6 \nsIt) \n___ J I \nL - - - - - - . . . J I  FÂ· \n'2lt) = cos 1211fot) - 113 cos l&lIfot) \n+ 115 cos 1101rfo\" \nla) \nsalt) = cos 121Ifot) - 113 COS l&lIfot) \n+ 115 cos 1101rfot) \n- In cos 11411fot) \nIb) \nIntroduction \nSit) \nJ \nf \nÂ·1/6 \n-1/6 \n~It) \nÂ·1/6 \nS:Jlt) \nFigure 1.3 Fourier transform of a square-wave function. \nChap. 1 \n.. \ninal function; they differ only in the manner of \npresentation. Fourier analysis \nallows one to examine a function from another point of view, the transform \ndomain. As we will see in the discussions to follow, the method of Fourier \ntransform analysis employed, as illustrated in Fig. 1.2, is often the key to \nproblem-solving success. \nSec. 1.3 \nDigital Fourier Analysis \n7 \nsltl \nA \n(81 \nS(fl \n.1 \n2To \n(bl \nFigure 1.4 Fourier transform of a pulse waveform. \n1.3 DIGITAL FOURIER ANALYSIS \nBecause of the wide range of problems that are susceptible to attack by the \nFourier transform, we would expect the logical extension of Fourier trans-\nform analysis to the digital computer. Numerical integration of Eq. (1.1) \nimplies the relationship: \nN-\\ \nS(h) = L s(ti)e-j2-rrfkl'(ti+ \\ -\nti) \nk = 0, 1, ... , N -\n1 \n(1.2) \ni=O \nFor those problems that do not yield to a closed-form Fourier transform \nsolution, the discrete Fourier transform of \nEq. 0.2) offers a potential method \nof attack. However, careful inspection of Eq. 0.2) reveals that if there are \nN data points of \nthe function S(ti) and if we desire to determine the amplitude \nof N separate sinusoids, then computation time is proportional to N 2 , the \nnumber of multiplications. Even with high-speed computers, computation \nof the discrete Fourier transform requires excessive machine time for large \nN. \nAn obvious requirement existed for the development of techniques to \nreduce the computing time of the discrete Fourier transform; however, the \nscientific community met with little success. Then, in 1965, Cooley and \n8 \nIntroduction \nChap. 1 \nTukey published their mathematical algorithm [1], which has become known \nas the \"fast Fourier transform. \" The fast Fourier transform (FFT) is a com-\nputational algorithm that reduces the computing time of Eq. (1.2) to a time \nproportional to N log2 N. This increase in computing speed has completely \nrevolutionized many facets of scientific analysis. A historical review of the \ndiscovery of the FFT illustrates that this important development was almost \nignored [4, 5]. \nThe FFT has revolutionized the use of the discrete Fourier transform. \nIt is important to recognize that one's ability to apply the FFT relies prin-\ncipally on an understanding of the discrete Fourier transform and not the \nFFT algorithm. For this reason, this text emphasizes the fundamentals of \nthe Fourier and discrete Fourier transforms. \nREFERENCES \nI. COOLEY, J. W., AND J. W. TUKEY. \"An Algorithm for the Machine Calculation \nof Complex Fourier Series.\" Mathematics of \nComputation (1965), Vol. 19, No. \n90, pp. 297-301. \n2. BRACEWELL, RON. The Fourier Transform and Its Applications, 2d Rev. Ed. New \nYork: McGraw-Hill, 1986. \n3. PAPOULIS, A. Probability, Random Variables, and Stochastic Processes, 2d Ed. \nNew York: McGraw-Hill, 1984. \n4. COOLEY, J. W., R. L. GARWIN, C. M. RADER, B. P. BOGERT, ANDT. C. STOCKHAM. \n\"The 1968 Arden House Workshop of Fast Fourier Transform Processing.\" \nIEEE Trans. on Audio and Electroacoustics (June 1969), Vol. AU-17, No.2, \npp.66-75. \n5. COOLEY, J. W., P. W. LEWIS, AND P. D. WELCH. \"Historical Notes on the Fast \nFourier Transform.\" IEEE Trans. on Audio and Electroacoustics (June 1967), \nVol. AU-15, No.2, pp. 76-79. \n6. IEEE Trans. on Audio and Electroacoustics, Special Issue on the Fast Fourier \nTransform (June 1969), Vol. AU-17, No.2. \n7. BRIGHAM, E. O. The Fast Fourier Transform. Englewood-Cliffs, NJ: Prentice \nHall, 1974. \n8. RAMIREZ, R. W. The FFT: Fundamentals and Concepts. Englewood-Cliffs, NJ: \nPrentice Hall, 1985. \n9. BURRIS, C. S., AND T. W. PARKS. DFT-FFT & Convolution Algorithms & Im-\nplementation. New York, Wiley, 1985. \nto. ELLIOT, D. F., AND K. R. RAo. Fast Transforms, Algorithms, Analyses, Appli-\ncations. Orlando, FL: Academic Press, 1982. \n2 \nTHE FOURIER TRANSFORM \nA principal analysis tool in many oftoday's scientific challenges is the Four-\nier transform. Possibly the most well-known application of this mathematical \ntechnique is the analysis oflinear time-invariant systems. But, as emphasized \nin Chapter 1, the Fourier transform is essentially a universal problem-solving \ntechnique. Its importance is based on the fundamental property that one can \nexamine a particular relationship from an entirely different viewpoint. Si-\nmultaneous visualization of a function and its Fourier transform is often the \nkey to successful problem solving. \n2.1 THE FOURIER INTEGRAL \nThe Fourier integral is defined by the expression \nH(f) = J:\", h(t)e -j2Trft dt \n(2.1) \nIf \nthe integral exists for every value of the parameter f, then Eq. (2.1) defines \nH(f), the Fourier transform of h(t). Typically, h(t) is termed a function of \nthe variable time and H(f) is termed a function of the variable frequency. \nWe use this terminology throughout the book: t is time and f is frequency. \nFurther, a lowercase symbol represents a function of time; the Fourier trans-\nform of this time function is represented by the same uppercase symbol as \na function of frequency. \n9 \n10 \nThe Fourier Transform \nChap. 2 \nIn general, the Fourier transform is a complex quantity: \nH(f) = R(f) + j J(f) = I \nH(f) I \nej8(f) \n(2.2) \nwhere R(f) is the real part of the Fourier transform, \nJ(f) is the imaginary part of the Fourier transform, \nI \nH(f) is the amplitude or Fourier spectrum of h(t) and is given by \nR2(f) + J2(f) , \n8(f) is the phase angle of the Fourier transform and is given by \ntan- 1 [J(f)/R(f)]. \nExample 2.1 \nExponential Waveform \nTo illustrate the various defining terms of the Fourier transform, consider the func-\ntion of time \nFrom Eq. (2.1), \nHence, \nh(t) = l3e -at \n= 0 \nt>O \nt<O \nl3a \nR(f) = a 2 + (2'TrN \n-2'Trfl3 \nI(f) = a 2 + (2'Trff \n[ -2'TrfJ \n8(f) = tan - I -a-\n(2.3) \n(2.4) \nEach of these functions is plotted in Fig. 2.1 to illustrate the various forms of FOUl;er \ntransform presentation. \nSec. 2.2 \nThe Inverse Fourier Transform \nhit) \n(3 \n(a) \n(b) \n(e) \nFigure 2.1 \n(a) Example of a time-domain function, (b) real and imaginary pre-\nsentations of the Fourier transform, and (c) magnitude and phase presentations \nof the Fourier transform. \n2.2 THE INVERSE FOURIER TRANSFORM \nThe inverse Fourier transform is defined as \n11 \n(2.5) \nInversion transformation, Eq. (2.5), allows the determination of a function \nof time from its Fourier transform. If \nthe functions h(t) and H(f) are related \nby Eqs. (2.1) and (2.5), the two functions are termed a Fourier transform \npair, and we indicate this relationship by the notation \nh(t) ~ \nH(f) \n(2.6) \nExample 2.2 Inverse Fourier Transform of Example 2.1 \nConsider the frequency function determined in the previous example: \nFrom Eq. (2.5), \nh \n- J'\" [ \n130: \n-' \n21rfl3 \n] \nj27fJt df \n(1) -\n_'\" \n0:2 + (21rf)2 \nJ 0:2 + (21rf)2 \ne \nBecause ei27fJt = cos(27rft) + j sin(27rft), then \nh(1) = J'\" [130: cos(21rft) + 21rfl3 Sin(21rft)] df \n_00 \n0:2 + (21rf)2 \n0:2 + (27rf)2 \n(2.7) \n+ j Joo [130: sin(27rft) _ 27rfl3 COS(27rft)] df \n-00 \n0:2 + (21rf)2 \n0:2 + (21rff \nThe second integral of Eq. (2.7) is zero because each integrand term is an odd func-\ntion. This point is clarified by examination of Fig. 2.2; the first integrand term in \nthe second integral of Eq. (2.7) is illustrated. Note that the function is odd, that is, \n12 \nThe Fourier Transform \nChap. 2 \nFigure 2.2 Integration of an odd function. \ng(t) = \n- g( - t). Consequently, the area under the function from - fo to fo is zero. \nTherefore, in the limit as f \n0 approaches infinity, the integral of the function remains \nzero; the infinite integral of any odd function is zero. \nEquation (2.7) becomes \n~a Joc \ncos(2-rrtf) \n27r~ Joc \nf sin(2-rrtf) \nh(t) = (2-rr)2 \n-oc (a/27r)2 + p df + (27r)2 \n_oc (a/2-rrf + p df \nFrom a standard table of integrals, \nJ\noc \ncos(ax) dx \n_oc b2 + x 2 \n= ~ \ne- ab \nb \nJ\noc x sin(ax) d \n-ab \nb2 \n2 \nX = -rre \n-oc \n+ x \nHence, Eq. (2.8) can be written as \na>O \na>O \n(2.8) \nh(t) = ~ \n[_-rr_ e-(2'11\"t)(al2'11\")] + 2-rr~ [-rre-(2'11\"t)(a/2'11\")] \n(2-rr)2 \n(a/2-rr) \n(2-rrf \n(2.9) \n=.@.e-al+.@.e-al=~e-al \nt>O \n2 \n2 \nThe time function \nh(t) = \n~e-al \nt > 0 \nand the frequency function \n~ \nH(f) = a + j(27rf) \nare related by both Eqs. (2.1) and (2.5) and hence are a Fourier transform pair: \n~e \n-at (t > 0) 0 \na + j(27rf) \n(2.10) \nSec. 2.3 \nExistence of the Fourier Integral \n13 \n2.3 EXISTENCE OF THE FOURIER INTEGRAL \nTo this point, we have not considered the validity of Eqs. (2.1) and (2.5); \nthe integral equations have been assumed to be well-defined for all functions. \nIn general, for most functions encountered in practical scientific analysis, \nthe Fourier transform and its inverse are well-defined. We do not intend to \npresent a highly theoretical discussion of the existence of the Fourier trans-\nform but rather to point out conditions for its existence and to give examples \nof these conditions. Our discussion follows that of Papoulis [3]. \nCondition 1. \nIf h(t) is integrable in the sense \nJ:oo I \nh(t) I \ndt < 00 \n(2.11) \nthen its Fourier transform H(f) exists and satisfies the inverse Fourier \ntransform of Eq. (2.5). \nIt is important to note that Condition 1 is a sufficient but not necessary \ncondition for the existence of a Fourier transform. There are functions that \ndo not satisfy Condition 1 but have a transform satisfying Eq. (2.5). This \nclass of functions is covered by Condition 2. \nExample 2.3 Symmetrical Pulse Waveform \nTo illustrate Condition 1, consider the pulse time waveform \nh(t) = A \nI \nt I \n< To \nA \nt = Â±To \n-2 \n(2.12) \n= 0 \nI \nt I> To \nwhich is shown in Fig. 2.3. Equation (2.11) is satisfied for this function; therefore, \nthe Fourier transform exists and is given by \nf\nTO \nH(f) = \nAe - j2\",ft dt \n-To \nf TO \nfTO \n= A \ncos(27rft) dt - jA \nsin(27Tft) dt \n-Th \n-Th \nThe second integral is equal to zero because the integrand is odd: \nA \nI \nTo \nH(f) = 2 f sin(27rft) \n7T \n-To \n(2.13) \n= 2ATo sin(27rTof) \n27TTof \nThose terms that obviously can be canceled are retained to emphasize the [sin(af)]/ \naf characteristic of the Fourier transform of a pulse waveform, as shown in Fig. \n2.3. \n14 \nThe Fourier Transform \nChap. 2 \nhIt) \nA \nFigure 2.3 Fourier transform of a symmetrical-pulse time-domain waveform. \nBecause this example satisfies Condition 1, then H(f) as given by Eq. (2.13) \nmust satisfy Eq. (2.5). \nh(t) = foo 2ATo sin(211\"Tof) ei2'rrft df \n-00 \n211\"Tof \nf\noo sin(211\"Tof) \n. . \n= 2ATo \n_00 \n211\"Tof \n[cos(211\"ft) + J sm(211\"ft)] df \nThe imaginary integrand term is odd; therefore, \nh(t) = ~ \nfoo sin(211\"Tof) cos(211\"ft) df \n11\" \n-00 \nf \nFrom the trigonometric identity \nsin(x) cos(y) = Usin(x + y) + sin(x -\ny)] \nh(t) becomes \nh(t) = ~ \nfoo sin[211\"f(To + t)] df + ~ \nfoo sin[211\"f(To -\nt)] df \n211\" \n-00 \nf \n211\" \n-00 \nf \nA \n2\" \n-\n-~ \nFigure 2.4 Graphical evaluation of Eq. (2.19). \nA T -t \n2\" ITO-ti \n-~ \n2 \n(2.14) \n(2.15) \n(2.16) \nSec. 2.3 \nExistence of the Fourier Integral \nand can be rewritten as \nh(t) = A(To + t) foo sin[21Tf(To + t)] df \n_00 \n21Tf(To + t) \nBecause \n+ A(To -\nt) foo sin[21Tf(To -\nt)] df \n-00 \n21Tf(To -\nt) \nf\noo sin(21Tax) dx = _1_ \n-00 \n21Tax \n2 1 a 1 \n(I 1 \ndenotes magnitude or absolute value), then \nh(t) = ~ To + t + ~ To -\nt \n2 1 \nTo + t 1 \n2 1 \nTo -\nt 1 \n15 \n(2.17) \n(2.18) \n(2.19) \nEach term of Eq. (2.19) is illustrated in Fig. 2.4; by inspection, these terms add to \nyield \nh(t) = A \n1 \nt 1 < To \nA \nt = Â±To \n(2.20) \n= -\n2 \n= 0 \n1 \nt I> To \nThe existence of the Fourier transform and the inverse Fourier transform has \nbeen demonstrated for a function satisfying Condition 1. We have established the \nFourier transform pair (Fig. 2.3): \nh(t) = A (I t 1 \n< To) ~ \n2ATo sin(21TTof) \n21TTof \nExample 2.4 General Pulse Time Waveform \nConsider the pulse time waveform \nh(t) = A \nA \n= -2 \n0< t < 2To \nt = 0; t = 2To \n= 0 \notherwise \nwhich is shown in Fig. 2.5(a). The Fourier transform is given by \nf 2TO \nH(f) = Jo \nAe \n-j2\",!t dt \nf2~ \nf2~ \n= A Jo \nCOS(21Tft) dt - jA Jo \nsin(21Tft) dt \n= (A/21Tf) sin(21Tft) I~TO + j(A/21Tf) COS(21Tft) I~TO \n_ 2ATo sin[21T(2To)f] \n. {2AT COS[21T(2To)f] \n2ATo} \n-\n21T(2To)f \n+ J \n0 \n21T(2To)f \n-\n21T(2To)f \n(2.21) \n(2.22) \n(2.23) \n16 \nhIt) \nA 1----... \no \n(a) \n2To \nIH(f)1 \n1 \n1 \n3 \n2 \n(b) \n2T \n0 r;; \n2T \n0 r;; \nBlf) \n71' \n(e) \nThe Fourier Transform \nChap. 2 \nFigure 2.5 (a) General pulse waveform, \n(b) Fourier transform amplitude funcÂ· \ntion, and (c) Fourier transform phase \nfunction. \nThe amplitude spectrum is given by \nI \nH(f) I \n2ATo \n. 2 \n2 \n2'l1'(2To)f ism [2'l1'(2To)fl + cos [2'l1'(2To)fl \n-\n2 cos[2'l1'(2To)fl + 1}1/2 \n(2.24) \n2ATo \n{ \n112 \n2'l1'(2To)f 2 -\n2 cos[2'l1'(2To)f} \n= 2ATo I \nsin[2'l1'Tofl I \n2'l1'Tof \nBecause cos(x) -\n1 = - sin2(x) and sin(x) = 2 sin(x/2) cos(x/2), then the phase \nangle is given by \n(f) = \n-I \n{COS[2'l1'(2To)fl - I} \ne \ntan \nsin[2'l1'(2To)fl \n= tan- I {-Sin[hTofl} \ncos[2'l1'Tofl \n= -hTof \n(2.25) \nSec. 2.3 \nExistence of the Fourier Integral \n17 \nThe amplitude spectrum I \nH(f) I and phase angle 6(f) of the Fourier transform of \nh(t) are shown in Figs. 2.5(b) and (c), respectively. Note that the tan -1(X) function \nis restricted to -1T < 6 < 1T by normal convention. \nCondition 2. If h(t) = [3(t) sin(27Tft + ex) (where f and ex are arbitrary \nconstants), if [3(t + k) < [3(t), and if for I \nt I \n> A. > 0, the function h(t)/ \nt is absolutely integrable in the sense of Eq. (2.11), then H(f) exists \nand satisfies the inverse Fourier transform, Eq. (2.5). \nAn important example is the function [sin(at)]/at, which does not sat-\nisfy the integrability requirements of Condition 1. \nExample 2.5 Pulse Frequency Waveform \nConsider the function \nh(t) = 2Afo sin(21Tfot) \n21Tfot \n(2.26) \nillustrated in Fig. 2.6. From Condition 2, the Fourier transform of h(t) exists and is \ngiven by \nH(f) = foe 2Af \n0 sin(21Tf \not) e - j2\"TfJI dt \n-oe \n21Tfot \nA foe sin(21Tfot) \n. . \n= -\n[cos(21Tft) - J sm(21Tft)] dt \n1T -oe \nt \n(2.27) \n= ~ \nfoe sin(21Tf \not) COS(21Tft) dt \n1T \n-00 \nt \nThe imaginary term integrates to zero because the integrand term is an odd function. \nSubstitution of the trigonometric identity of Eq. (2.16) gives \nH(f) = ~ \nJX sin[21Tt(fo + f)] dt + ~ \nfoe sin[21Tt(fo -\nf)] dt \n21T -oe \nt \n21T -oe \nt \n= A(fo + f) foe sin[21Tt(fo + f)] dt \n-00 \n21Tt(f \n0 + f) \n+ A(fo -\nf) foe sin[21Tt(fo -\nf)] dt \n-oe \n21Tt(fo -\nf) \nhIt) = \n2Afo sin I~Qt) \not \nFigure 2.6 Fourier transform of A [sin(at)/atl. \nHIt) \n(2.28) \n18 \nThe Fourier Transform \nChap. 2 \nEquation (2.28) is of the same form as Eq. (2.17); identical analysis techniques yield \nH(f) = A \nA \n= -2 \nI \nf 1< fo \nf = Â±fo \n= 0 \nI \nf I> fo \n(2.29) \nBecause this example satisfies Condition 2, H(f) [Eq. (2.29)], must satisfy the \ninverse Fourier transform relationship, Eq. (2.5): \nf\nfO \nh(t) = \nAei2'1fft df \n-fo \n= A ffo cos(21rft) df = A sin(2'lTft) I \nfo \n- fo \n2'lTt \n- fo \n= 2Af \n0 sin(2'lTf \n01) \n2'lTf \not \nBy means of Condition 2, the Fourier transform pair \n2Afo sin(2'lTfot) 0 \nH(f) = A \n2'lTfot \nhas been established and is illustrated in Fig. 2.6. \nI \nf 1< fo \n(2.30) \n(2.31) \nCondition 3. Although not specifically stated, all functions for which Con-\nditions 1 and 2 hold are assumed to be of bounded variation, that is, \nthey can be represented by a curve of finite height in any finite time \ninterval. By means of Condition 3, we extend the theory to include \nsingular (impulse) functions. \nIf h(t) is a periodic or impulse function, then H(f) exists only if one \nintroduces the theory of distributions. Appendix A has an elementary dis-\ncussion of distribution theory; with the aid of this development, the Fourier \ntransform of singular functions can be defined. It is important to develop \nthe Fourier transform of impulse functions because their use greatly sim-\nplifies the derivation of many transform pairs. \nImpulse function 8(t) is defined as [Eq. (A.8)] \nJ:oo 8(t -\nto)x(t) dt = x(to) \n(2.32) \nwhere x(t) is an arbitrary function continuous at to. Application of the defi-\nnition of Eq. (2.32) yields straightforwardly the Fourier transform of many \nimportant functions. \nExample 2.6 Impulse Function \nConsider the function \nh(t) = KB(t) \n(2.33) \nSec. 2.3 \nExistence of the Fourier Integral \n19 \nhIt) = \nK 61t) \nHlf)=K \nK \nK \n(Q \n------------~----------~ \n------------+------------~ \nFigure 2.7 Fourier transform of an impulse function. \nThe Fourier transform of h(t) is easily derived using the definition of Eq. (2.32): \nH(f) = Loooo K8(t)e - j2 \n.. \nft dt = Keo = K \n(2.34) \nThe inverse Fourier transform of H(f) is given by \n(2.35) \nBecause the integrand of the second integral is an odd function, the integral is zero; \nthe first integral is meaningless unless it is interpreted in the sense of distribution \ntheory. From Eq. (A.2l), Eq. (2.35) exists and can be rewritten as \nh(t) = K f'oo ~2\"ft df = K Loooo cos(27rft) df = K8(t) \nThese results establish the Fourier transform pair \nK8(t) ~ \nH(f) = K \nwhich is illustrated in Fig. 2.7. \nSimilarly, the Fourier transform pair, as shown in Fig. 2.8, \nh(t) = K ~ \nK8(f) \n(2.36) \n(2.37) \n(2.38) \ncan be established where the reasoning process concerning existence is exactly as \nargued previously. \nExample 2.7 Periodic Functions \nTo illustrate the Fourier transform of periodic functions, consider \nh(t) = A cos(27'ifot) \n(2.39) \nhIt)Â· K \nHIt) = K 61t) \nK \nK \nFigure 2.8 Fourier transform of a constant-amplitude waveform. \n20 \nThe Fourier Transform \nChap. 2 \nhld- A COl 12wfot) \nHlf) s Rlf) \n,f\\, f' \nf', A \nf', \n(1 f\\ \nI \nI \nI \nI \nI \nI \nt \nI \nV V \nV V \nV \nFigure 2.9 Fourier transform of A cos(at). \nThe Fourier transform is given by \nH(f) = L\"'\"\" A cos(27rfot)e-j2 \n.... f' dt \n= ~ \nf\"\" \n[~2\"\"fo, + e-j2 \n.... fo']e-j2 \n.... f' dt \n2 \n-00 \n= ~ \nf\"\" [e-j2 \n.... t<f-fo) + e-j2 \n.... \n'(fo+f)] dt \n2 \n-00 \nA \nA \n= :.za(f -\nfo) + 28(f + fo) \n(2.40) \nwhere arguments identical to those leading to Eq. (2.36) have been employed. The \ninversion formula yields \nh( t) = Loooo [48(f + f 0) + 48(f - f 0) ] ej2 \n.... f' df \n= ~~2 \n.... fO' + ~e \n-j2 \n.... fo' \n2 \n2 \n= A cos(2'TTf \not) \nThe Fourier transform pair \n~A \nA \nA cos(2'TTfot) ~ \n:.za(f - fo) + 28(f + fo) \nis illustrated in Fig. 2.9. \nf hIt) - A sin l2llfot) \nHIt) = illt) \nA \n1\"1 \" \" l\\ \" \", \nI \nI \n, \nI \nI \nt \nV \nV \nV \nV \nV \nFigure 2.10 Fourier transform of A sin(at). \n(2.41) \n(2.42) \nSec. 2.3 \nExistence of the Fourier Integral \nSimilarly, the Fourier transform pair (Fig. 2.10) \ncan be established. Note that the Fourier transform is imaginary. \nExample 2.8 Sequence of Impulse Functions \n21 \n(2.43) \nThe Fourier transform of a sequence of equidistant impulse functions is another \nsequence of equidistant impulses r3]. \n00 \n100 \n( \n) \nh(t) = ,,~oo 8(t -\nnT) ~ \nH(f) = T \nn~oo \n8 f - ~ \n(2.44) \nA graphical development of this Fourier transform pair is illustrated in Fig. 2.11. \nâ¢â¢â¢ \nh3(t) = 1 + 2 ~ cos 12111<fo t) \nk=1 \ns \nhslt) = 1 + \n2k \n~ \n1 cos 127rkfot) \nh It) = E Sit - nT) \nCD \nn =-<XI \nâ¢â¢â¢ \nT \n2T \nla) \ng \nfo 2fo 3fo \nIb) \nHS(f) \ng \nIe) \nH If) = ! E Sif -\n!!.) \nTn=-oo \nT \nÂ©> \nâ¢â¢â¢ \nâ¢ â¢â¢ \n(d) \nFigure 2.11 Graphical development of the Fourier transform of a sequence of \nequidistant impulse functions. \n22 \nThe Fourier Transform \nChap. 2 \nThe importance of the Fourier transform pair of Eq. (2.44) becomes obvious in future \ndiscussions of discrete Fourier transforms. \nInversion Formula Proof \nBy means of distribution theory concepts, it is possible to derive a \nsimple formal proof of the inversion formula of Eq. (2.5). \nSubstitution of H(f) [Eq. (2.1)] into the inverse Fourier transform of \nEq. (2.5) yields \nJ:= H(f)e j 2-rr!t df = J:= ej2-rr!t df J:= h(x)e -j2-rr!x dx \nBecause [Eq. (A.21)] \nJ:oc ej 2-rr!t dt = 8(t) \nthen an interchange of integration in Eq. (2.45) gives \nJ:= H(f)ej2-rrft df = J:= hex) dx J:= ej2-rrf(t-x) df \n= J:oo h(x)8(t -\nx) dx \n(2.45) \n(2.46) \nBut by the definition of the impulse function of Eq. (2.32), Eq. (2.46) simply \nequals h(t). This statement is valid only if h(t) is continuous. \nI However, if \nit is assumed that \n(2.47) \nthat is, if h(t) is defined as the mid value at a discontinuity, then the inversion \nformula stilI holds. Note that in the previous examples we carefully defined \neach discontinuous function consistent with Eq. (2.47). \n2.4 ALTERNATE FOURIER TRANSFORM DEFINITIONS \nIt is a well-established fact that the Fourier transform is a universally ac-\ncepted tool of modern analysis. Yet, to this day, there is not a common \ndefinition of the Fourier integral and its inversion formula. To be specific, \nthe Fourier transform pair is defined as \nH(w) = QI J:oo h(t)e -jwt dt \nh(t) = Q2 J:oo H(w)ejwt dw \nw = 27rf \n(2.48) \n(2.49) \n1 See Appendix A. The definition of the impulse response is based on the continuity of \nthe testing function h(t). \nSec. 2.5 \nFourier Transform Pairs \n23 \nwhere the coefficients a \\ and a2 assume different values depending on the \nuser. Some set a\\ = 1 and a2 = 1/21T; others set a\\ = a2 = 1/y'2\";; or set \na\\ = 1/21T and a2 = 1. Equations (2.48) and (2.49) impose the requirement \nthat a \\ \na2 = 1/21T. Various users are then concerned with the splitting of the \nproduct a\\a2. \nTo resolve this question, we must define the relationship desired be-\ntween the Fourier transform and the Laplace transform and the definition \nwe wish to assume for the relationship between the total energy computed \nin the time domain and the total energy computed in w, the radian frequency \ndomain. For example, Parseval's Theorem states: \nJ:~ \nh2(t) dt = 21TaT J:~ \n1 \nH(w) 12 dw \n(2.50) \nIf \nthe energy computed in t is required to be equal to the energy computed \nin w, then a \\ = 1/y'2\";. However, if the requirement is made that the Laplace \ntransform, universally defined as \nL[h(t)] = J:~ \nh(t)e-st dt = J:~ \nh(t)e-(cx+jw)t dt \n(2.51) \nshall reduce to the Fourier transform when the real part of s is set to zero, \nthen a comparison of Eqs. (2.48) and (2.51) requires a \\ = 1, i.e., a2 = \n1/21T, which is in contradiction to the previous hypothesis. \nA logical way to resolve this conflict is to define the Fourier transform \npair as follows: \nH(f) \nJ:~ \nh(t)e -j27f!t dt \nh(t) = J:~ \nH(f)ej27f!t df \nWith this definition, Parseval's Theorem becomes \n(2.52) \n(2.53) \nand Eq. (2.52) is consistent with the definition of the Laplace transform. \nNote that as long as integration is with respect to f, the scale factor 1/21T \nnever appears. For this reason, the latter definition of the Fourier transform \npair was chosen for this book. \n2.5 FOURIER TRANSFORM PAIRS \nA pictorial table of Fourier transform pairs is given in Fig. 2.12. This graph-\nical and analytical catalog is by no means complete, but does contain the \nmost frequently encountered transform pairs. \nTime Domain \n~ \nhltl \nhltl \n2Alol \n1 \n3 \nt;, \n2f~ \nhit) \nI \nK \nhit) \nK \nIII < To \nh(t) = A \nA \n-\n2 \nIt \nI = To <0 H(f) = 2AT\" sin(2'ITTof) \n21TTof \n= 0 \nIII > To \n_ \nf sin(21Tf \n01) o \nH(f) = A \nIf \nI \n< fo \nh(1) -\n2A 0 \n2 f \n'IT 01 \nA \n-\n2 \nIf \nI = fo \n= 0 \nIf \nI \n> fo \nh(t) = K 0 \nH(f) = K&(f) \nh(t) = K&(t) 0 \nH(f) = K \nFrequency Domain \nH(f) \n2ATo \nHill \nHltl = K W) \nK \nHltl \nK \nN \nen \n.. . \n-5T \nTime Domain \nhIt) \n1 \n... \n-3T \n-T \nT \n3T \n5T \nt \n-4T \n-2T \n2T \n4T \nhIt) \nA \nhIt) \nA \nh(t) = ,,~~ \n8(t -\nnT) 0 \nH(j) = ~ \n,,~~ \n8 (f - ~) \nh(t) = A cos(2-rrfotl 0 \nH(f) = ~ \n8(j -\nfo) \nA \n+ \"2 8(j + fo) \nh(t) = A sin(2-rrfotl 0 \nH(f) = -j~8(j -\nfo) \n+j~8(j+fo) \nFigure 2.12 Catalog of Fourier transform pairs_ \n_l \n.-\nFrequency Domain \n_1 \nT \n1 \nT \ntlc'i(t+to) \n2 \n-to \ntl c'i(t+to) \n2 \n-to \n\"(f) \nH(t) \nH(t) \n1 \nT \nl \nT \ntl 1l(I-to) \n2 \nto \nto \nI \n-~ \n1l(1-lo) \nN \naI \nTime Domain \nFrequency Domain \nhit' \n'I' Hlf, \nA L \nh(t) = -\n2~0 t + A ~ \nH(f) = 2ATo sin~~;o~of) \nIt I < 2To \n= 0 \nIt I > 2To \n, \n, \n-2To \n2To \n1 \n1 \n;t \n2To To \n2To \nhit' \nA2T 1 Hit' \nh(t) = A cos(27rfot) ~ \nH(f) = A2TO[Q(f + fo) \nA \n0 \nAI \nIt I < To \n+ Q(f -\nfo)1 \n= 0 \nIt I > To \nQ(f) = sin(27rTo\nf) \n2TrTof \n\"0/1\\0\" I \noO{llA \n\"v v\" Â· ... v \nv \n-to \nto \nhit' \nAfÂ· \n~ \nA \nA \n(Trf) \nh(t) = T q(t) \nH(f) = 2 + 2 cos \nf,. \nI Hit' \nA \nAte I \nAf,. \n( \nI) \n+ -q t +-\nIf \nI oS f, \n4 \n2fc \nAf.. \n( \nI \n) \n= 0 \nIf \nI > f, \n+ -q t --\n4 \n2f, \nfe \nte \n1 \nTo \nsin(2Trf,t) \nq(t) =--\nTrt \nI\\) \n~ \n-To \nTime Domain \nhIt) \nTo \nhIt) \n~r \nhIt) \nFrequency Domain \nI \nI \n('ITt) <0 \nI \nh(t) \"2 + \"2 cos \nTo \nHeIl = \"2 Q(j) \n, H(I) \nIt I \noS To \n+ ~ l \nQ (! + ~) \n4 \n2To \nTo \n= 0 \nIt! > To \n+ Q (! - 2~J] \nQ(f) = sin(21TTof) \n1 \n'IT! \nTo \n1 \nH(t) \nI \n<0 \na 2 \nh(t) = \"2 a exp( - altl) \nHef) = -2--\na + 4'IT2j2 \nH(t) \n( \n)\n1/2 \n( \n2P) \nh(t) =; \nexp(-at2 ) <0 H(j) = \nexp -: \nFigure 2.12 (cont.) \n28 \nThe Fourier Transform \nChap. 2 \nPROBLEMS \n2.1. Determine the real and imaginary parts of the Fourier transform of each of the \nfollowing functions: \n(a) h(t) = e -al'l \n-oo<t<oo \nt>O \n(b) h(t) ~{i \n{ -A \n(c) h(t) = \n~ \nt = 0 \nt<O \nt < 0 \nt = 0 \nt>O \n{ \n1 \ncos(21rf \not) \nt > 0 \n(d) h(t) = 2' \nt = 0 \no \nt < 0 \n(0) h(t) ~{~ \na < t < b; a, b > 0 \nt = a; t = b \nelsewhere \n(f) h(t) = \n{~e \n-0<' sin(2'lTfot) \nt~O \nt<O \n(g) h(t) = H \n8(t + a) + 8(t -\na) + 8(t + ~) + 8(t -\n~) \n] \n2.2. Determine the amplitude spectrum I \nH(f) I \nand phase 6(f) of the Fourier trans-\nform of h(t): \n1 \n(a) h(t) = \n-\n-\n00 < t < 00 \nt \n(b) h(t) = e - \"\", \n-\n00 < t < 00 \n(c) h(t) = A sin(21rfot) \n0 :S t < 00 \n(d) h(t) = Ae-O<' cos(2'lTfot) \nO:s t < 00 \n{ At \n0 < t < To \n(e) h(t) = \n0 \nelsewhere \n(f) h(t) = cos2(21rfot) \n(g) h(t) = cos(2'lTfot) \n4 \nI \nt I :s-\nfo \n= 0 \notherwise \n2.3. Determine the inverse Fourier transform of each of the following: \n( \n(f) = sin(21rfn cos(2'lTfn \na) H \n2'lTf \n(b) H(f) = (1 -\np)2 \nI \nf I \n< 1 \n= 0 \notherwise \nChap. 2 \nReferences \nf \n(c) H(f) \n(p + a)(P + 4a) \n(d) H(f) = A cos(2-rrfto) \nREFERENCES \n29 \n1. ARsAc, J. Fourier Transforms and the Theory of \nDistributions. Englewood Cliffs, \nNJ: Prentice Hall, 1966. \n2. BRACEWELL, R. The Fourier Transform and its Applications, 2d Ed. New York: \nMcGraw-Hill, 1986. \n3. PAPOULIS, A. The Fourier Integral and Its Applications, 2d Ed. New York: \nMcGraw-Hill, 1984. \n4. CHAMPENEY, D. C. Fourier Transforms and Their Physical Application. New \nYork: Academic Press, 1973. \n3 \nFOURIER TRANSFORM \nPROPERTIES \nIn dealing with Fourier transforms, there are a few properties that are basic \nto a thorough understanding. A visual interpretation of these fundamental \nproperties is of equal importance to knowledge of their mathematical rela-\ntionships. The purpose of this chapter is to develop not only the theoretical \nconcepts of the basic Fourier transform pairs, but also the meaning of these \nproperties. For this reason, we use ample analytical and graphical examples. \n3.1 LINEARITY \nIf \nx(t) and yet) have the Fourier transforms X(f) and Y(f), respectively, then \nthe sum x(t) + yet) has the Fourier transform X(f) + Y(f). This property \nis established as follows: \nf:\", [x(t) + y(t)]e -j2-rrft dt = f:\", x(t)e -j2-rrft dt \n+ f:\",y(t)e-j2-rrftdt \n(3.1) \n= X(f) + Y(f) \nThe Fourier transform pair \nx(t) + yet) ~ \nX(f) + Y(f) \n(3.2) \nis of \nconsiderable importance becaus~ \nit reflects the applicability of \nthe Four-\nier transform to linear-system analysis. \n30 \nSec. 3.1 \nLinearity \nExample 3.1 Addition of a Constant and a Sinusoid \nTo illustrate the linearity property, consider the Fourier transform pairs \nx(t) = K ~ \nX(f) = K8(f) \n~ \nA \nA \ny(t) = A cos(27rfot) \"\"'====#' Y(f) = 28(f -\nfo) + 28(f + fo) \nBy the linearity theorem, \nx(t) + y(t) = K + A cos(2'TTfot) ~ \nX(f) + Y(f) = K8(f) + 1\n8(f - fo) \nA \n+ 2 8(f + fo) \n31 \n(3.3) \n(3.4) \n(3.5) \nFigures 3.1(a), (b), and (c) illustrate each ofthe Fourier transform pairs, respectively. \nxltl \nXlfl \nK \nK \nAI2 \nK \nA/2 \nFigure 3.1 The linearity property. \n32 \nFourier Transform Properties \nChap. 3 \n3.2 SYMMETRY \nIf \nh(t) and H(f) are a Fourier transform pair, then \nH(t) 0 \nh(-f) \n(3.6) \nThe Fourier transform pair of Eq. (3.6) is established by rewriting Eq. (2.5): \nh( - t) = f:\"\" H(f)e -j27fft df \nand by interchanging the parameters t and f: \nh( - f) = f:\"\" H(t)e -j27fft dt \nExample 3.2 Pulse Time and Frequency Waveforms \nTo illustrate this property, consider the Fourier transform pair: \nh(t) = A (I t I \n< To) 0 \n2ATo sin(2'lTTof) \n21TTof \nillustrated previously in Fig. 2.3. By the symmetry theorem, \n2ATo sin2~;~ot) 0 \nh( - f) = h(f) = A \nI \nf I \n< To \n(3.7) \n(3.8) \n(3.9) \n(3.10) \nwhich is identical to the Fourier transform pair of Eq. (2.31) illustrated in Fig. 2.6. \nUtilization of the symmetry theorem can eliminate many complicated mathematical \ndevelopments; a case in point is the deVelopment of the Fourier transform pair of \nEq. (2.31). \n3.3 TIME AND FREQUENCY SCALING \nIf the Fourier transform of h(t) is H(f), then the Fourier transform of h(kt), \nwhere k is a real constant greater than zero, is determined by substituting \nt' = kt in the Fourier integral equation: \nf\"\" h(kt)e-j27fft dt = foo h(t')e-j27ft'(flk) dt' = ! H(t) \n-00 \n-00 \nk \nk \nk \n(3.11) \nFor k negative, the term on the right-hand side changes sign because the \nlimits of integration are interchanged. Therefore, time scaling results in the \nFourier transform pair: \n(3.12) \nSec. 3.3 \nTime and Frequency Scaling \n33 \nWhen dealing with time scaling of impulses, extra care must be ex-\nercised; from Eq. (A.lO), \n1 \n8(at) = r;18(t) \nExample 3.3 Time Scale Expansion \n(3.13) \nThe time-scaling Fourier transform property is well-known in many fields of scientific \nendeavor. As shown in Fig. 3.2, time scale expansion corresponds to frequency scale \ncompression. Note that as the time scale expands, the frequency scale not only \ncontracts, but the amplitude increases vertically in such a way as to keep the area \nconstant. This is a well-known concept in radar and antenna theory. \nFrequency Scaling \nIf the inverse Fourier transform of H(f) is h(t), the inverse Fourier \ntransform of H(kf) \n, where k is a real constant, is given by the Fourier trans-\nform pair: \nrttG) ~ \nH(kf) \n(3.14) \nThe relationship of Eq. (3.14) is established by substituting f' = kf into the \ninversion formula: \nJ\nOO H(kf)ej2Trft df = Joo H(f')ej2Trf'(tlk) df' = _1_h(l.-) \n-00 \n-00 \nk \nI \nk I k \nFrequency scaling of impulse functions is given by \n1 \n8(af) = r;18(f) \nExample 3.4 Frequency-Scale Expansion \n(3.15) \n(3.16) \nAnalogous to time scaling, frequency-scale expansion results in a contraction of the \ntime scale. This effect is illustrated in Fig. 3.3. Note that as the frequency scale \nexpands, the amplitude of the time function increases. This is simply a reflection of \nthe symmetry property of Eq. (3.6) and the time-scaling relationship of Eq. (3.12). \nExample 3.5 Infinite Sequence of Impulse Functions \nMany texts state Fourier transform pairs in terms of the radian frequency oo. For \nexample, Papoulis [2] gives \n00 \n~ \n2'lT \n00 \n( \n2n'lT) \nh(t) = n~oo \n8(1 - nn ~ \nH(oo) = T n~oo \n8 00 - T \n(3.17) \nBy the frequency-scaling relationship of Eq. (3.16), we know that \n2'lT i 8[2'lT(f - !!.)] =1. i 8(f - !!.) \nT n=-oo \nT \nT n=-oo \nT \n(3.18) \n34 \nFourier Transform Properties \nChap. 3 \nhit) \nHIt) \nhl~ \n2HI2f) \nA \nhl!i' \n4H14t) \nA \nFigure 3.2 Time-scaling property. \nand Eq. (3.17) can be rewritten in terms of the frequency variable f: \nh(t) = n~QO \n8(t - nD ~ \nH(f) = ~ \nn~QO \n8(f - ~) \n(3.19) \nwhich is Eq. (2.44). \nSec. 3.4 \nTime and Frequency Shifting \nhIt' \nA \n21112t, \n2A \n.I2. \n!R. \n2 \n2 \nHlf, \nHI!d \n2 \n4hl4t' \nHI~ \n4 \n4A \nFigure 3.3 Frequency-scaling property. \n3.4 TIME AND FREQUENCY SHIFTING \n35 \nIf h(t) is shifted by a constant to, then by substituting s = t - to, the Fourier \ntransform becomes \nJ:\"\" h(t -\nto)e-j 271\"ft dt = J:\"\" h(s)e-j271\"f(s+to ) ds \ne -j271\"fto J:\"\" h(s)e -j271\"fs ds \ne - j271\"fto H(f) \n(3.20) \n36 \nFourier Transform Properties \nChap, 3 \nThe time-shifted Fourier transform pair is \nh(t -\nto) 0> H(f)e -j2'fTfto \n(3.21) \nExample 3.6 Phase Shifting \nA pictorial description of this pair is illustrated in Fig, 3.4. As shown, time shifting \nresults in a change in the phase angle 8(f) = tan -'[l(f)IR(f)], Note that time shifting \nhIt) \n2A \nh(t- t') \n2A \nh(t- 2t') \n2A \nh(t, 4t') \n2A \nR(f) \nR(f) \nR(f) \nA \nA \n,j2 \nFigure 3.4 Time-shifting property, \n1(1) \n1(1) \nA \n,.fi \n1(1) \nI(f) \nSec. 3.4 \nTime and Frequency Shifting \n37 \ndoes not alter the magmtude of the Fourier transform. This follows because \nH(f)e -j2Trfto = H(j)[cos(27Tfto) - j sin(27Tfto)] \nand hence the magnitude is given by \nwhere H(f) has been assumed to be real for simplicity. These results are easily \nextended to the case of H(f), a complex function. \nFrequency Shifting \nIf H(f) is shifted by a constant fo, its inverse transform is multiplied \nby ei2-rrtfo \nh(t)ej2-rr.Jo ~ \nH(f -\nfo) \n(3.23) \nThis Fourier transform pair is established by substituting s \nthe inverse Fourier transform-defining relationship: \nf - fo into \nExample 3.7 Modulation \nej2-rrtfo f:\", H(s)ej2-rrst ds \nei2-rrtfo h(t) \n(3.24) \nTo illustrate the effect offrequency shifting, let us assume that the frequency function \nH(f) is real. For this case, frequency shifting results in a mUltiplication of the time \nfunction h(t) by a cosine whose frequency is determined by the frequency shift fo \n(Fig. 3.5). This process is commonly known as modulation. \nExample 3.8 Down Conversion by Frequency Multiplication \nA practical application of frequency shifting is illustrated in Fig. 3.6. Multiplication \nof a sinusoid of frequency 2fo with another sinusoid of frequency 3fo results in two \nsinusoids. One sinusoid has a frequency that is the sum of the frequencies of the \nmultiplied sinusoids, that is, 5fo. The second sinusoid has a frequency determined \nby the difference of the two frequencies, fo. This difference-frequency sinusoid is \ncommonly referred to as the down conversion term or component. \n38 \nFourier Transform Properties \nChap. 3 \nhIt) \nH(t) \nY2[H(t-fo)\" H(f \n.. \nfo)) \nY2[H(f-2to)\" H(f+2fo)) \nFigure 3.5 Frequency-shifting property. \nSec. 3.4 \nTime and Frequency Shifting \nh,(t)= cos[271j31.ltl \nh2/t)= cos[27T(21.ltl \nh,(t)h2/t)= cos [271j21.ltlÂ· \ncos (271j3I. It] \n(a) \n(b) \n39 \nÂ·51. -41. -31. -21. -I. \nI. 21. 31. 41. 51. \nH2/I) \n-51. -41. -31. -21. -I. \nI. 21. 31. 41. 51. \nH,II) â¢ H2/I) \n-51. -41.-31. -21. -I. \nI. 21. 31. 41. 51. 61. I \nIc) \nFigure 3.6 Examples of sum and difference frequencies produced by frequency \nmUltiplication. \n40 \nFourier Transform Properties \nChap. 3 \n3.5 ALTERNATE INVERSION FORMULA \nThe inversion formula of Eq. (2.5) can also be written as \nh(t) = [J:oo H*(f)e -j2-rrft df r \n(3.25) \nwhere H*(f) is the conjugate of H(f); that is, if H(f) = R(f) + jI(f), then \nH*(f) = R(f) - jI(f). The relationship of Eq. (3.25) is verified by simply \nperforming the conjugation operations indicated. \nh(t) = [J:ooH*(f)e-j2-rrftdfr \n[ J: \n00 R(f)e - j2-rrft df - j J: \n00 I(f)e - j2-rrft df \n] * \n[J_oooo [R(f) cos(27rft) - I(f) sin(27rft)] df \n- j J:oo [R(f) sin(21Tft) + I(f) COS(21Tft)] dfJ * \n= J:oo [R(f) cos(27rft) - I(f) sin(21Tft)] df \n+ j J:oo [R(f) sin(21Tft) + I(f) COS(21Tft)] df \n= J:oo [R(f) + jI(f)][cos(27rft) + j sin(21Tft)] df \n= J:oo H(f)ej2-rrft df \n(3.26) \nThe significance of the alternate inversion formula is that now both the \nFourier transform and its inverse contain the common term e -j2-rrft. This \nsimilarity is of considerable importance in the development of fast Fourier \ntransform computer programs. \n3.6 EVEN AND ODD FUNCTIONS \nIf \nhe(t) is an even function, that is, he(t) = he( - t), then the Fourier trans-\nform of he(t) is an even function and is real: \nSec. 3.6 \nEven and Odd Functions \nThis pair is established by manipulating the defining relationships: \nH(f) = J:oo heCt)e -J27ffl dt \n= J:oo heCt) cos(27rft) dt - j J:oo hAt) sin(27rft) dt \n= J_oooo heCt) cos(27rft) dt = RAf) \n41 \n(3.28) \nThe imaginary term is zero because the integrand is an odd function. Because \ncos(27rft) is an even function, then hAt) cos(27rft) = heCt) cos[27r( - f)t] \nand H e(f) = H e( - f); the frequency function is even. Similarly, if H(f) is \ngiven as a real and even frequency function, the inversion formula yields \nh(t) = J:oo HAf)eJ27ffl dt = J:oo Re(f)eJ27ffl df \n= J:oo Re(f) cos(27rft) df + j J:oo Re(f) sin(27rft) df \n= J:oo RAf) cos(27rft) df = heCt) \nExample 3.9 Even Time and Frequency Functions \n(3.29) \nAs shown in Fig. 3.7, the Fourier transform of an even time function is a real and \neven frequency function; conversely, the inverse Fourier transform of a real and \neven frequency function is an even function of time. \nOdd Functions \nIf hoCt) = \n- ho( - t), then hoCt) is an odd function, and its Fourier \ntransform is an odd and imaginary function, \nH(f) = J:oohoCt)e-J27ffldt \n= J:oo hoCt) cos(27rft) dt - j J:oo hoCt) sin(27rft) dt \n(3.30) \n- j J:oo hoCt) sin(27rft) dt = j1o(f) \nIlf) \nIe) \nIb) \nIe) \nFigure 3.7 Fourier transform of an even function. \n42 \nFourier Transform Properties \nChap. 3 \nThe real integral is zero because the multiplication of an odd and an even \nfunction is an odd function. Because sin(27Tft) is an odd function, then hoCt) \nsin(27Tft) = - ho(t) sin[27T( - f)t] and Ho(f) = - Ho( - f); the frequency \nfunction is odd. For H(f) given as an odd and imaginary function, then \nh(t) = J:= H(f)ej27rft dt = j J:= lo(f)ej27rft df \n= j J:= lo(f) COS(27Tft) df + P J:= lo(f) sin(27Tft) df \n(3.31) \n= \n- J:= lo(f) sin(27Tft) df = ho(t) \nand the resulting hoCt) is an odd function. The Fourier transform pair is thus \nestablished: \n(3.32) \nExample 3.10 Odd Time and Frequency Functions \nAn illustrative example of this transform pair is shown in Fig. 3.8. The function h(t) \ndepicted is odd; therefore, the Fourier transform is an odd and imaginary function \nof \nfrequency. If a frequency function is odd and imaginary, then its inverse transform \nis an odd function of time. \nhCt) \nRCf) \nFigure 3.8 Fourier transform of an odd function. \n3.7 WAVEFORM DECOMPOSITION \nAn arbitrary function can always be decomposed or separated into the sum \nof an even and an odd function: \nh(t) = hCt) + h(t) \n2 \n2 \n= [h;t) + h( ~ \nt) ] + [h;t) _ h( ~ \nt) ] \n(3.33) \n= hAt) + hoCt) \nThe terms in brackets satisfy the definitions of even and odd functions, \nSec. 3.7 \nWaveform Decomposition \n43 \nrespectively. From Eqs. (3.27) and (3.32), the Fourier transform of Eq. (3.33) \nis \nH(f) = R(f) + jI(f) = HeCf) + Ho(f) \n(3.34) \nwhere He(f) = R(f) and Ho(f) = jICf). We show in Chapter 9 that decom-\nposition can increase the speed of computation of the FFT. \nExample 3.11 Exponential Waveform Decomposition \nTo demonstrate the concept of waveform decomposition, consider the exponential \nfunction [Fig. 3.9(a)] \nh(t) = e \n-at \nt~O \n(3.35) \nFollowing the developments leading to Eq. (3.33), we obtain \nh(t) = [e;at] + [e;at] \n{[e-at] \n[eat]} + {[e-a\n,] \n[ea\n,]} \n-2-\n,\"\"0 + 2 \ntsO \n-2-\n,\"'0 -\n2 \ntsO \n(3.36) \n= {e-al'l} + {[e-a\n,] _ [ea\n,] } \n2 \n2 \n,\"'0 \n2 'so \n= {he(t)} + {ho(t)} \nFigures 3.9(b) and (c) illustrate the even and odd decompositions, respectively. \nFigure 3.9 Waveform decomposition \nproperty. \nhCt) = \n.Â·at \nCa) \nh.Ct) = \n%e.altl \nCb) \n%e-at \nCe) \n44 \nFourier Transform Properties \nChap. 3 \n3.8 COMPLEX TIME FUNCTIONS \nFor ease of \npresentation, we have to this point considered only real functions \nof time. The Fourier transform, Eq. (2.1), the inversion integral, Eq. (2.5), \nand the Fourier transform properties hold for the case of h(t), a complex \nfunction of time. If \n(3.37) \nwhere hr(t) and hi(t) are the real and imaginary parts of \nthe complex function \nh(t), respectively, then the Fourier integral, Eq. (2.1), becomes \nH(f) = J:oo [hr(t) + jhi(t)]e -j2-rrft dt \n= J:oo [hr(t) cos(27rft) + hi(t) sin(27rft)] dt \n- j J:oo [hr(t) sin(27rft) -\nh;(t) cos(27rft)] dt \n= R(f) + jl(f) \nTherefore, \n(3.38) \nR(f) = J:oo [hr(t) cos(27rft) + hi(t) sin(27rft)] dt \n(3.39) \nl(f) = \n- J:oo [hr(t) sin(27rft) -\nhi(t) cos(27rft)] dt \n(3.40) \nSimilarly, the inversion formula, Eq. (2.5), for complex functions yields \nhr(t) = f:oo [R(f) cos(27rft) - l(f) sin(27rft)] df \n(3.41) \nhi(t) = J:oo [R(f) sin(27rft) + l(f) cos(27rft)] df \n(3.42) \nIf h(t) is real, then h(t) = hr(t) and the real and imaginary parts of the \nFourier transform are given by Eqs. (3.39) and (3.40), respectively: \nRe(f) = f:oo hr(t) cos(27rft) dt \nlo(f) = \n- J:oo hr(t) sin(27rft) dt \n(3.43) \n(3.44) \nRAf) is an even function because Re(f) = ReC - f). Similarly, lo( - f) = \n- 10 (f) and lo(f) is odd. \nSec. 3.8 \nComplex Time Functions \n45 \nFor h(t) purely imaginary, h(t) = jhi(t) and \nRo(f) J:= hi(t) sin(27rft) dt \n(3.45) \nIAf) = J:= hi(t) cos(27rft) dt \n(3.46) \nRo(f) is an odd function and I Af) is an even function. Table 3.1 lists various \ncomplex time functions and their respective Fourier transforms. \nExample 3.12 Simultaneous Fourier Transforms \nWe can employ the relationships of Eqs. (3.43), (3.44), (3.45), and (3.46) to simul-\ntaneously determine the Fourier transform of two real functions. To illustrate this \npoint, recall the linearity property of Eq. (3.2): \nx(t) + y(t) 0 \nX(f) + Y(f) \n(3.47) \nLet x(t) = h(t) and y(t) = jg(t), where both h(t) and g(t) are real functions. It \nfollows that X(f) = H(f) and Y(f) = jG(f). Because x(t) is real, then from Eqs. \n(3.43) and (3.44) \nx(t) = h(t) 0 \nX(f) = H(f) = R.(f) + jIo(f) \n(3.48) \nSimilarly, because y(t) is imaginary, then from Eqs. (3.45) and (3.46) \nHence, \ny(t) = jg(t)OY(f) = jG(f) = Ro(f) + j1e(f) \n(3.49) \nh(t) + jg(t) 0 \nH(f) + jG(f) \nTABLE 3.1 \nProperties of the Fourier Transform for \nComplex Functions \nReal \nTime domain \nh(t) \nImaginary \nReal even, imaginary odd \nReal odd, imaginary even \nReal and even \nReal and odd \nImaginary and even \nImaginary and odd \nComplex and even \nComplex and odd \nFrequency domain \nH(f) \nReal part even \nImaginary part odd \nReal part odd \nImaginary part even \nReal \nImaginary \nReal and even \nImaginary and odd \nImaginary and even \nReal and odd \nComplex and even \nComplex and odd \n(3.50) \n46 \nFourier Transform Properties \nChap. 3 \nwhere \nThus, if \nH(f) = Re(f) + j1o(f) \nG(f) = I e(f) - jRo(f) \nz(t) = h(t) + jg(t) \nthen the Fourier transform of z(t) can be expressed as \nZ(f) = R(f) + jI(f) \n= [R~f) + R( ~ \nf)] + [R~) _ R( ~ \nf) ] \n+ j[ \nI~) + 1(; \nf)] + j[ \nI~) _ 1(; \nf) ] \nand from Eqs. (3.51) and (3.52) \nH(f) = [R~) + R(~f)] + j[I~) _ I(;f)] \nG(f) = [I~) + 1(; f)] _ j[ \nR~f) _ R( ~ \nf) ] \n(3.51) \n(3.52) \n(3.53) \n(3.54) \n(3.55) \n(3.56) \nThus, it is possible to separate the frequency function Z(f) into the Fourier trans-\nforms of h(t) and g(t). As is demonstrated in Chapter 9, this technique is used ad-\nvantageously to increase the speed of computation of the FFT. \n3.9 SUMMARY TABLE OF FOURIER TRANSFORM \nPROPERTIES \nFor future reference, the basic properties of the Fourier transform are sum-\nmarized in Table 3.2. These relationships are of considerable importance \nthroughout the remainder of the book. \nPROBLEMS \n3.1. Let \n{1_~ \nh(t) = \nI \nt 1< 2 \nt = Â±2 \nI \nt I> 2 \n{\n-A \nx(t) = :i \nI \nt 1< 1 \nt = Â± 1 \nI \nt I> 1 \nChap. 3 \nProblems \n47 \nTABLE 3.2 \nProperties of Fourier Transforms \nTime domain \nEquation number \nFrequency domain \nLinear addition \n(3.2) \nLinear addition \nx(t) + y(/) \nX(f) + Y(f) \nSymmetry \n(3.6) \nSymmetry \nH(t) \nh( - f) \nTime scaling \nInverse scale change \nh(kt) \n0.12) \nmH(f) \nInverse scale change \nFrequency scaling \nmh(Â£) \n0.14) \nH(kf) \nTime shifting \n(3.21) \nPhase shifting \nh(t -\n10) \nH(f)e - j2nJto \nModulation \n0.23) \nFrequency shifting \nh(t)~htJo \nH(f -\nfo) \nEven function \n(3.27) \nReal function \nh,(t) \nH,(f) = R,(f) \nOdd function \n0.30) \nImaginary \nhoW \nH o(f) = jI \no(f) \nReal function \n0.43) \nReal part even \nh(t) = h,(t) \n0.44) \nImaginary part odd \nH(f) = RAf) + jIo(f) \nImaginary function \n(3.45) \nReal part odd \nh(/) = jh;(t) \n(3.46) \nImaginary part even \nH(f) = Ro(f) + jl,(f) \nSketch h(t), x(t), and [h(t) -\nx(l)]. Use the Fourier transform pair of Eq. \n(2.21) and the linearity theorem to find the Fourier transform of [her) - x(t)]. \n3.2. Consider the functions h(t) illustrated in Fig. 3.10. Use the linearity property \nto derive the Fourier transform of h(t). \n3.3. Use the symmetry theorem and the Fourier transform pairs of Fig. 2.12 to \ndetermine the Fourier transform of the following: \n( ) h( ) = A 2 sin2(21TTot) \na \nt \n(1Ttf \n0:2 \n(b) h(t) = (0:2 + 41T2t2) \n(e) h(t) = exp( - :2t2) \n3.4. Derive the frequency-scaling property from the time-scaling property by means \nof the symmetry theorem. \n3.5. Consider \n{\nA2 \nA21tl \nh(t) = \n0 \n2To \nI \nt 1< 2To \nI \nt I> 2To \n48 \nFourier Transform Properties \nChap. 3 \nh(t) \nâ¢â¢â¢ \nA \nâ¢â¢â¢ \n-510 \n-310 \n-10 \n10 \n310 \n510 \n(a) \nhIt) \nâ¢â¢â¢ \nâ¢ â¢â¢ \nA \n-510 \n-310 \n-10 \n10 \n310 \n510 \n-A \n(b) \nFigure 3.10 Functions for Problem 3.2. \nSketch the Fourier transform of h(2t), h(4t), and h(8t). (The Fourier transform \nof h(t) is given in Figure 2.12.) \n3.6. Derive the time-scaling property for the case where k is negative. \n3.7. By means of the shifting theorem, find the Fourier transform of the following \nfunctions: \n(a) h(t) = A sin[2'T1'fo(t -\nto)] \n'TI'(t -\nto) \n(b) h(t) = K'&(t -\nto) \nA 2 --lt- tol \n{ \nA2 \n(c) h(t) = \n0 \n2To \n3.8. Show that \nI \nt -\nto I \n< 2To \nI \nt -\nto I > 2To \nh(at -\n[3) 0 \n_1_e-J27rf3fl\"H(i) \n10.1 \na \n3.9. Show that I \nH(f) I = I \ne - j2-rrfto H(f) I, that is, the magnitude of a frequency \nfunction is independent of the time delay. \n3.10. Find the inverse Fourier transform of the following functions by using the \nfrequency-shifting theorem: \nf \n- A sin[2'T1'To(f -\nfo)] \n(a) H( ) -\n'TI'(j -\nfo) \n0. 2 \nChap. 3 \nReferences \n49 \n3.11. Review the derivations leading to Eqs. (2.9), (2.13), (2.20), (2.29), (2.30), and \n(2.36). Note the mathematics that result are real for the Fourier transform of \nan even function. \n3.12. Decompose and sketch the even and odd components of the following \nfunctions: \n(a) h(t) = {~ \nl<t<2 \notherwise \n1 \n(b) h(t) = [2 _ (t -\n2)2] \n{ -t+1 \nO<t~1 \n(c) h(t) = \n0 \notherwise \n(d) h(t) = 1 + t + t 2 + t3 \n(e) h(t) = 1 + sin(2'TTjt) \n3.13. Prove each of the properties listed in Table 3.1. \n3.14. If h(t) is real, show that I H(f) I is an even function. \n3.15. By making a substitution of a variable in Eq. (2.32), show that \n1-\n00\n00 x(t)B(at -\nto) dt = rhx(~) \n3.16. Prove the following Fourier transform pairs: \n(a) dh(t) 0> j2'TTjH(f) \ndt \n(b) [-j2'TTt]h(t) 0> dH(f) \ndj \n3.17. Use the derivative relationship of \nProblem 3.16(a) to find the Fourier transform \nof a pulse waveform given the Fourier transform of a triangular waveform. \nREFERENCES \nI. BRACEWELL, R. The Fourier Transform and its Applications, 2d Ed. New York: \nMcGraw-Hill, 1986. \n2. PAPOULIS, A. The Fourier Integral and Its Applications, 2d Ed. New York: \nMcGraw-Hill, 1984. \n3. CHAMPENEY, D. C. Fourier Transforms and Their Physical Application. New \nYork: Academic Press, 1973. \n4 \nCONVOLUTION AND \nCORRELATION \nIn Chapter 3, we investigated those properties that are fundamental to the \nFourier transform. However, there exists a class of Fourier transform re-\nlationships whose importance far outranks those previously considered. \nThese properties are the convolution and correlation theorems, which are \nto be discussed at length in this chapter. \n4.1 CONVOLUTION INTEGRAL \nConvolution of two functions is a significant physical concept in many di-\nverse scientific fields. However, as in the case of many important mathe-\nmatical relationships, the convolution integral does not readily unveil itself \nas to its true implications. To be more specific, the convolution integral is \ngiven by \nyet) = J:oo x(T)h(t -\nT) dT = x(t) * h(t) \n(4.1) \nFunction yet) is said to be the convolution of the functions x(t) and h(t). \nNote that it is extremely difficult to visualize the mathematical operation of \nEq. (4.1). We develop the true meaning of convolution by graphical analysis. \n50 \nSec. 4.2 \nGraphical Evaluation of the Convolution Integral \n4.2 GRAPHICAL EVALUATION OF THE CONVOLUTION \nINTEGRAL \n51 \nLet x(t) and h(t) be two time functions given by graphs, as represented in \nFigs. 4.1(a) and (b), respectively. To evaluate Eq. (4.1), functions X(T) and \nh(t -\nT) are required; X(T) and h(T) are simply x(t) and h(t), respectively, \nwhere the variable t has been replaced by the variable T. h( -T) is the image \nof h(T) about the ordinate axis and h(t -\nT) is simply the function h( - T) \nshifted by the quantity t. Functions X(T), h( -T), and h(t -\nT) are shown in \nFig. 4.2. To compute the integral of Eq. (4.1), it is necessary to multiply \nand integrate the functions X(T) [Fig. 4.2(a)] and h(t -\nT) [Fig. 4.2(c)] for \neach value of t from -\n00 to + \n00. As illustrated in Figs. 4.3(a) and (h), this \nproduct is zero for the choice of \nthe parameter t = - t I. The product remains \nzero until t is reduced to zero. As illustrated in Figs. 4.3(c) and (h), the \nproduct of X(T) and h(tl -\nT) is the function emphasized by shading. The \nintegral of this function is simply the shaded area beneath the curve. As t \nis increased to 2tl and further to 3tl, Figs. 4.3(d), (e), and (h) illustrate the \nrelationships of the functions to be multiplied as well as the resulting inte-\ngrations. For t = 4tl, the product again becomes zero, as shown by Figs. \n4.3(t) and (h). This product remains zero for all t greater than 4t \n1 [Figs. 4.3(g) \nand (h)]. If \nt is allowed to be a continuum of values, then the convolution \nof x(t) and h(t) is the triangular function illustrated in Fig. 4.3(h). \nThe procedure described is a convenient graphical technique for eval-\nuating convolution integrals. Summarizing the steps: \n1. Folding. Take the mirror image of h(T) about the ordinate axis. \n2. Displacement. Shift h( -T) by the amount t. \n3. Multiplication. Multiply the shifted function h(t -\nT) by X(T). \n4. Integration. The area under the product of h(t -\nT) and X(T) is the value \nof the convolution at time t. \nx(t) \nhIt) \n% t------, \n(a) \n(b) \nFigure 4.1 \nExample waveforms for convolution. \n52 \nx(T) \n(II \nh(-T) \n-1 \n(bl \nh(t-TI \n(el \nT \nT \nT \nConvolution and Correlation \nChap. 4 \nFigure 4.2 Graphical illustration of \nfold-\ning and displacement operations. \n-tl \n0 \nt, \n2tl 3t, 4tl 5tl \nt \nIhl \nFigure 4.3 Graphical example of convolution; I. = 1/2. \nExample 4.1 Convolution Procedure \nTo illustrate further the rules for graphical evaluation of the convolution integral, \nconvolve the functions illustrated in Figs. 4.4(a) and (b). First, fold h(T) to obtain \nh( -T), as illustrated in Fig. 4.4(c). Next, displace or shift h( -T) by the amount t, \nSec. 4.2 \nGraphical Evaluation of the Convolution Integral \nhIT) =e-T \nx(T) \nT \n(a) \n(b) \nh(-T) \nFOLDING \neT \n(e) \nh(t 'T) \nDISPLACEMENT ===> \n(d) \nT \nx(T)h(t -T) \nMULTIPLICATION ===> \nIe) \nt = \nt' \nT \nINTEGRATION ===> \n,..,-t' \n(f) \nFigure 4.4 Convolution procedure: folding, displacement, multiplication, and \nintegration. \n53 \n54 \nConvolution and Correlation \nChap. 4 \nas shown in Fig. 4.4(d). Then, multiply h(1 -\nT) by X(T) [Fig. 4.4(e)] and, finally, \nintegrate to obtain the convolution result for time If [Fig. 4.4(f)]. \nThe result illustrated in Fig. 4.4(f) can be determined directly from Eq. (4.1): \ny(l) = J\"\" x(T)h(t -\nT) dT = (' (l)e -(t-T) dT \n-~ \nJo \n(4.2) \n= e-/(eTI~) = e-/[e l \n-\n1] = 1 - e- I \nNote that the general convolution integration limits of -00 to +00 be-\ncome 0 to t for Ex. 4.1. It is desired to develop a straightforward approach \nto find the correct integration limits. For Ex. 4.1, the lower nonzero value \nof the function h(t -\nT) = e -(t-T) is - 00 and the lower nonzero value for \nX(T) is O. When we integrated, we chose the largest of these two values as \nour lower limit of integration. The upper nonzero value of h(t -\nT) is t; the \nupper nonzero value of \nX(T) is 00. We chose the smallest of these two for our \nupper limit of integration. \nA general rule for determining the limits of integration can then be \nstated as follows: \nGiven two functions with lower nonzero values of L I and L2 and upper \nnonzero values of U \nI and U \n2, choose the lower limit of integration as \nmax[L \nI ,L2 ] and the upper limit of integration as min[ U \nI, \nU \n2]' \nIt should be noted that the lower and upper nonzero values for the \nfixed function X(T) do not change; however, the lower and upper nonzero \nvalues of \nthe sliding function h(t -\nT) change as t changes. Thus, it is possible \nto have different limits of integration for different ranges of t. A graphical \nsketch similar to Fig. 4.4 is also an extremely valuable aid in choosing the \ncorrect limit of integration. \n4.3 ALTERNATE FORM OF THE CONVOLUTION \nINTEGRAL \nThe previous graphical illustration is but one of the possible interpretations \nof convolution. Equation (4.1) can also be written equivalently as \ny(t) = J:oo h(T)X(t -\nT) dT \nHence, either h(T) or X(T) can be folded and shifted. \n(4.3) \nTo see graphically that Eqs. (4.1) and (4.3) are equivalent, consider \nthe functions illustrated in Fig. 4.5(a). It is desired to convolve these two \nfunctions. The series of graphs on the left in Fig. 4.5 illustrate the evaluation \nof Eq. (4.1); the graphs on the right illustrate the evaluation of Eq. (4.3). \nThe previously defined steps of (1) folding, (2) displacement, (3) multipli-\nSec. 4.3 \nAlternate Form of the Convolution Integral \n55 \nâ¢ \nT \n(e) \nT \nh(Â·TI \nr\" \n<== \nFOLDING ~ \n.. \nT \n(b) \nT \nh(t-TI \n~ \nDISPLACEMENT ~ \nT \n(e) \nd--\nT \nx(Tlh(t-TI \nh(Tlx(t-TI \nÂ¢:MULTlPLICATION::::::Â» \nT \n(d) \nT \nvet) \nvet) \n(e) \nFigure 4.5 Graphical example of convolution by Eqs. (4.1) and (4.3). \ncation, and (4) integration are illustrated by Figs. 4.5(b), (c), (d), and (e), \nrespectively. As indicated by Fig. 4.5(e), the convolution of x(-r) and h(T) is \nthe same irrespective of which function is chosen for folding and \ndisplacement. \n56 \nConvolution and Correlation \nExample 4.2 Equivalence of Eqs. (4.1) and (4.3) \nLet \nh(t) = e-' \nt~O \n= 0 \nt < 0 \nand \nx(t) \nsin t \n= 0 \notherwise \nFind h(t) * x(t) using both Eqs. (4.1) and (4.3). \nFrom Eq. (4.1) \ny(t) \ny(t) \n'!T \nt ~-\n2 \nt~O \nChap. 4 \n(4.4) \n(4.5) \n(4.6) \nThe integral limits are easily determined by using the procedure described previously. \nThe lower and upper nonzero values of the function X(T) are 0 and '!T/2, respectively. \nFor the function h(t -\nT) = e-<r- T ), the lower nonzero value is -00 and the upper \nnonzero value is t. We take the maximum of the lower nonzero values for our lower \nlimit of integration, i.e., O. The upper limit of integration is a function of t. For 0 ~ \nt ~ \n'!T/2, the minimum of the upper nonzero values is t and hence the upper limit of \nintegration is t. For t ~ \n'!T/2, the minimum of the upper nonzero values is '!T/2 and \nconsequently the upper limit of integration for this range of t is '!T/2. A graphical \nsketch of the convolution process also yields these integration limits. \nEvaluating Eq. (4.6), we obtain \ny(t) = 1 \n!(:,n t -\n00' t + ,-') \n-\n(1 + e'Tf/2) \n2 \nSimilarly, from Eq. (4.3), we obtain \ny(t) = {Coo h(T)X(t -\nT) dT \nl\n!o' e -T[sin(t -\nT)] dT \ny(t) = I' \ne -T[sin(t -\nT)] dT \nt--rr/2 \no \nt~O \n0< t ~ \n~ \n2 \n'!T \nt ~-\n2 \n'!T \n0< t <\"2 \n'!T \nt ~-\n2 \nt<O \n(4.7) \n(4.8) \nSec. 4.4 \nConvolution Involving Impulse Functions \n57 \nAlthough Eqs. (4.8) are different from Eqs. (4.6), evaluation yields identical \nresults to Eq. (4.7). \n4.4 CONVOLUTION INVOLVING IMPULSE FUNCTIONS \nThe simplest type of convolution integral to evaluate is one in which either \nx(t) or h(t) is an impulse function. To illustrate this point, let h(t) be the \nsingular function shown graphically in Fig. 4.6(a) and let x(t) be the rectan-\nhit) \n-T \nT \nla) \nxlt) \nA \nIbl \nhltl.xlt) \nA \n-\n-T \nT \nleI \nFigure 4.6 Illustration of convolution involving impulse functions. \n58 \nConvolution and Correlation \nChap. 4 \ngular function shown in Fig. 4.6(b). For these example functions, Eq. (4.1) \nbecomes \nyet) = f:\"\" [8(T -\nT) + B(T + T)]x(t -\nT) dT \n(4.9) \nRecall from Eq. (2.28) that \nf:\"\" B(T -\nT)X(T) dT = x(T) \nHence, Eq. (4.9) can be written as \nyet) = x(t -\nT) + x(t + T) \n(4.10) \nFunction yet) is illustrated in Fig. 4.6(c). Note that convolution of the func-\ntion x(t) with an impulse function is evaluated by simply reconstructing x(t) \nwith the position of the impulse function replacing the ordinate of x(t). As \nwe will see in the developments to follow, the ability to visualize convolution \ninvolving impulse functions is of considerable importance. \nExample 4.3 Convolution with Impulses \nLet h(t) be a series of impUlse functions, as illustrated in Fig. 4.7(a). To evaluate \nthe convolution of h(r) with the rectangular pulse shown in Fig. 4.7(b), we simply \nhit) \nÂ·2T \nÂ·T \nT \n2T \n(e) \nxlt) \nA \n(b) \nh(t).xlt) \n-\nr-- -\n-\nr-- -\nÂ·2T \nÂ·T \nT \n2T \nt \nIe) \nFigure 4.7 Convolution with an infinite sequence of impulse functions. \nSec. 4.4 \nConvolution Involving Impulse Functions \n59 \nreproduce the rectangular pulse at each of the impulse functions. The resulting con-\nvolution results are illustrated in Fig. 4.7(c). \nExample 4.4 Linear-System Convolution \nConvolution of two functions is a significant physical concept in many diverse sci-\nentific fields. A linear system is characterized by an output that is determine'd by \nthe convolution of the system input and the system impulse response. To demon-\nstrate, consider Fig. 4.8. As shown, if \nthe system input is an impulse, the output is \nthe impulse response of the system. It \nis the impulse response of \na system that allows \none to express the system output in terms of the input. To illustrate this point, we \nassume the system is time-invariant, that is, if the impulse is delayed by a time t = \no \nx(d \nx(tÂ· TI \nT \nkox(t' + \nk1 x(tÂ· T1' \n+k2 x (t Â·T2' \nx(t' \nx (t' \nLINEAR SYSTEM \nh(t'h..t \n(a' \n(b, \n(e' \n(d, \nv(t' \nv(t' = \nhIt' \nv(t' = h(tÂ· TI \no \nT \nv(tI = ko hIt' + k,h(tÂ·T\" +ka h(tÂ·T.' \nv(t' \nFigure 4.8 Graphical development of the characterization of a linear system by \nthe convolution integral. \n60 \nConvolution and Correlation \nChap. 4 \nT, then the output response is also delayed by the same duration of time. As shown \nin Fig. 4.8(b), the input o(t -\nT) results in an output h(t -\nT). \nWe also assume that the system is linear. Linear means that if the input Xj(t) \nproduces the output Yi(t), then the input ktxt(t) + k2X2(t) produces the output \nkty t \n(t) + k2Y2(t). As illustrated in Fig. 4.8(c), a system input composed of a series \nof delayed impulses of varying amplitudes yields an output consisting of delayed \nimpulse response functions whose amplitudes are determined by the amplitude of \nthe input impulse causing the response. The sum of these individual impulse re-\nsponses is the system output and is computed by the sum \n3 \ny(t) = L k;(t -\nT;) \n(4.11) \ni=1 \nTo extend Eq. (4.11) to include a general waveform, consider Fig. 4.8(d). We \ndivide the input into small elements of width ET ; the element has a height of x(nE T ) \nand a width of E T â¢ If \nwe assume that this element represents an impulse with area \n[x(nET)][ET\n], we know from the previous discussions that the output corresponding \nto this input is given by [h(t -\nnET)][x(nET)[ETl This output is shown in Fig. 4.8(d). \nTo determine the output due to all elements, we compute the sum \n(4.12) \nn= -oc \nIf we let ET --? 0 and n --? 00, such that nET --? T, we obtain \n(4.13) \n4.5 TIMEÂ·CONVOLUTION THEOREM \nPossibly the most important and powerful tool in modern scientific analysis \nis the relationship between Eq. (4.1) and its Fourier transform. This rela-\ntionship, known as the time-convolution theorem, allows one the complete \nfreedom to convolve mathematically (or visually) in the time domain by a \nsimple multiplication in the frequency domain. That is, if h(t) has the Fourier \ntransform H(f) and x(t) has the Fourier transform X(f), then h(t) * \nx(t) has \nthe Fourier transform H(f)X(f). The convolution theorem is thus given by \nthe Fourier transform pair: \nh(t) * x(t) <Q H(f)X(f) \n(4.14) \nTo establish this result, first form the Fourier transform of both sides \nof Eq. (4.1): \nSec. 4.5 \nTime-Convolution Theorem \n61 \nwhich is equivalent to (assuming the order of integration can be changed) \nY(f) = f:\"\" X(T) [f:\"\" h(t - T)e -j2-rrft dtJ dT \nBy substituting CJ\" = t -\nT, the term in the brackets becomes \nf:\"\" h(CJ\")e -j2-rrf(CT+T) dCJ\" = e -j2-rr/T f:oo h(CJ\")e -j2-rrfCT dCJ\" \n= e - j2-rr/T H(f) \nEquation (4.16) can then be rewritten as \nH(f)X(f) \nThe converse is proven similarly. \nExample 4.5 Convolution of Pulse Waveforms \n(4.16) \n(4.17) \n(4.18) \nTo illustrate the application of the convolution theorem, consider the convolution \nof the two pulse functions shown in Figs. 4.9(a) and (b). As we have seen previously, \nthe convolution of two rectangular functions is a triangular function, as shown in \nFig. 4.9(e). Recall from the Fourier transform pair of Eq. (2.21) that the Fourier \ntransform of a rectangular function is the [sin(fÂ»)/f function illustrated in Figs. 4.9(c) \nand (d). The convolution theorem states that convolution in the time domain cor-\nresponds to multiplication in the frequency domain; therefore, the triangular wave-\nform of Fig. 4.9(e) and the [sin2(f))/j2 function of Fig. 4.9(f) are Fourier transform \npairs. Thus, we can use the theorem as a convenient tool for developing additional \nFourier transform pairs. \nExample 4.6 Infinite Pulse-Train Waveform \nOne of the most significant contributions of distribution theory results from the fact \nthat the product of a continuous function and an impulse function is well-defined \n(Appendix A); hence, if h(t) is continuous at t = to, then \nh(t)8(t -\nto) = h(to)8(t -\nto) \n(4.19) \nThis result, coupled with the convolution theorem, allows one to eliminate the te-\ndious derivation of many Fourier transform pairs. To illustrate, consider the two \ntime functions h(t) and x(t) shown in Figs. 4.IO(a) and (b). As described previously, \nthe convolution of these two functions is the infinite pulse train illustrated in Fig. \n4.IO(e). It is desired to determine the Fourier transform of this infinite sequence of \npulses. We simply use the convolution theorem: the Fourier transform of h(t) is the \nsequence of impulse functions, the transform pair of Eq. (2.44), as illustrated in Fig. \n4.IO(c); and the Fourier transform of a rectangular function is the [sin(fÂ»)/f function \nshown in Fig. 4.IO(d). Multiplication of these two frequency functions yields the \ndesired Fourier transform. As illustrated in Fig. 4.1O(f), the Fourier transform of a \npulse train is a sequence of impulse functionsÂ· whose amplitude is weighted by a \n62 \nA \nIe) \nhIt) \nHlf) \nÂ·1 \n2To \nConvolution and Correlation \nxlt) \nA \nIe) \nHlf)Xlf) \nIf) \nXII) \nId) \nFigure 4.9 Graphical example of the convolution theorem. \nChap. 4 \n[sin(f)]lf function. This is a well-known result in the field of radar systems. It is to \nbe noted that the multiplication of the two frequency functions must be interpreted \nin the sense of distribution theory; otherwise, the product is meaningless. We can \nsee that the ability to change from a convolution in the time domain to multiplication \nin the frequency domain often renders unwieldy problems rather straightforward. \nSec. 4.5 \n... \nTime-Convolution Theorem \n-T \nhit) \nT \n(a) \nH(f) \n1 \nf \n-1 \n1 \nf \nf \n(e) \nh(t).x(t) \nA \n;----\n;---;-\n,/ \n... \nÂ·T \nT \n2ATO \n-T-\n(e) \nH(f)X(f) \n, \n, \n, \n, \n(f) \nx(t) \nA \nX(f) \n(d) \nFigure 4.10 Example application of the convolution theorem. \n63 \n64 \nConvolution and Correlation \nChap. 4 \n4.6 FREQUENCY-CONVOLUTION THEOREM \nWe can equivalently go from convolution in the frequency domain to mul-\ntiplication in the time domain by using the frequency-convolution theorem: \nthe Fourier transform of the product h(t)x(t) is equal to the convolution H(f) \n\"' \n1'1 \nA \n2\" \n-1 \nf \nhIt) \nA \n1'1 \n1'1 \n(a) \nJ \nÂ·1 \nf \nH(I) \nA \n2\" \n1 \nf \n(e) \nMULTIPLICATION \nx(t)h(t) \nA \n\" \nv v \nv \n(e) \nH(f).X(1) \nATo \n(I) \n1 \nf \nx(t) \n(b) \nX(f) \n2To \n1 \n2To \n(d) \nFigure 4.11 \nGraphical example of the frequency-convolution theorem. \nSec. 4.7 \nCorrelation Theorem \n* X(f). The frequency-convolution theorem is \nh(t)x(t) ~ \nH(f) * X(f) \n65 \n(4.20) \nThis pair is established by simply substituting the Fourier transform pair of \nEq. (4.14) into the symmetry Fourier transform relationship of Eq. (3.6). \nExample 4.7 Modulated Pulse Waveform \nTo illustrate the frequency-convolution theorem, consider the cosine waveform of \nFig. 4.11(a) and the rectangular waveform of Fig. 4.11(b). It is desired to determine \nthe Fourier transform of \nthe product of these two functions [Fig. 4.11 (e)]. The Fourier \ntransforms of the cosine and rectangular waveforms are given in Figs. 4.11(c) and \n(d), respectively. Convolution of these two frequency functions yields the function \nshown in Fig. 4.11(0; Figs. 4.11(e) and (0 are thus Fourier transform pairs. This is \nthe well-known Fourier transform pair of a single frequency-modulated pulse. \n4.7 CORRELATION THEOREM \nAnother integral equation of importance in both theoretical and practical \napplication is the correlation integral: \n(4.21) \nA comparison of the above expression and the convolution integral, Eq. \n(4.1), indicates that the two are closely related. The nature of \nthis relationship \nis best described by the graphical illustrations of Fig. 4.12. The functions to \nbe both convolved and correlated are shown in Fig. 4.12(a). Illustrations on \nthe left depict the process of \nconvolution as described in the previous section; \nillustrations on the right graphically portray the process of correlation. As \nevidenced in Fig. 4.12(b), the two integrals differ in that there is no folding \nof one of the integrands in correlation. The previously described rules of \ndisplacement, multiplication, and integration are performed identically for \nboth convolution and correlation. For the special case where either x(t) or \nh(t) is an even function, convolution and correlation are equivalent; this \nfollows because an even function and its image are identical and, thus, fold-\ning can be eliminated from the steps in computing the convolution integral. \nExample 4.8 Correlation Procedure \nCorrelate graphically and analytically the waveforms illustrated in Fig. 4.13(a). \nAccording to the rules for correlation, we displace h(T) by the shift t, mUltiply \nby X(T), and integrate the product x(T)h(t + T), as illustrated in Figs. 4.13(b), (c), \nand (d), respectively. \n66 \nhlH) \nCONVOLUTION \nConvolution and Correlation \nChap. 4 \n-\nT \nla) \nFOLDING \nT \nIb) \n<:== \nDISPLACEMENT ~ \nIe) \n<:::;= \nMULTIPLICATION :::::::> \nT \nId) \nIe) \n... \nhlt+r) \nT \nXIT)hlt+T) \nzIt) \nCORRELATION \nSHADED \nAREA \nT \nFigure 4.12 Graphical comparison of convolution and correlation. \nSec. 4.7 \nCorrelation Theorem \n67 \nX(T) \nhiT) \na \na \nT \na \nT \n(a) \nJ \nh(Â·t+T) \nDISPLACEMENT \n---\n.t\\--\nT \n(b) \nx(T)h (Â·t+T) \nMULTIPLICATION \nc::::::> \na \nT \n(e) \nINTEGRATION \nÂ·a \n(d) \nFigure 4.13 Correlation procedure: displacement, multiplication, and integeration. \nFrom Eq. (4.21), for positive displacement t, we obtain \nz(t) = L~~ \nx(T)h(t + T) dT \nL\na - t \nQ \n= \n(1)--r dT \no \na \n= ~2Ia-t = .[4a _ \n1)2 \n2a \n0 \n2a \nO~t~a \n(4.22) \n68 \nConvolution and Correlation \nChap. 4 \nFor negative displacement, see Fig. 4.13(c) to justify the limits of integration. \nf\na \nQ \nz(t) = \n(1)- T dT \nt \na \n(4.23) \n-a ~ \nt ~ \n0 \nA general rule can be developed for determining the limits of integration for \nthe correlation integral (see Problem 4.14). \nRecall that convolution multiplication forms a Fourier transform pair. \nA similar result can be obtained for correlation. To derive this relationship, \nfirst evaluate the Fourier transform of Eq. (4.21) \nJ:\", z(t)e -J2'ITft dt = f-\"'\", [f-\"'\", x(T)h(t + T) dTJ e -J2'ITft dt \n(4.24) \nor (assuming the order of integration can be interchanged) \nZ(f) = J:\", X(T) [f-\"'\", h(t + T)e-J2'ITft dtJ dT \n(4.25) \nLet a = t + T and rewrite the term in brackets as \nf:\", h(a)e-J2'ITf(rr-T) da = eJ2'ITf-r f:\", h(a)e-J2'ITfrr da \n= eJ2'ITf-r H(f) \nEquation (4.25) then becomes \nZ(f) = f:\", x(T)eJ2'ITf-r H(f) dT \n(4.26) \n= H(f) [J:oo X(T) COS(27TfT) dT + j J:\", X(T) sin(27TiT) dTJ \n(4.27) \n= H(f)[R(f) + j/(f)] \nNow the Fourier transform of X(T) is given by \nX(f) = J:= x(T)e-J2'ITfT dT \n= f:\", X(T) COS(27TfT) dT - j J:\", X(T) sin(27TfT) dT \n(4.28) \n= R(f) - j/(f) \nThe bracketed term of Eq. (4.27) and the expression on the right in \nEq. (4.28) are called conjugates [defined in Eq. (3.25)]. Equation (4.27) can \nChap. 4 \nProblems \nbe written as \nZ(f) = H(f)x* \n(f) \nand the Fourier transform pair for correlation is \nJ:oo x(T)h(t + T) dT ~ \nH(f)X*(f) \n69 \n(4.29) \n(4.30) \nNote that if x(t) is an even function, then X(f) is purely real and X(f) \nX*(f). For these conditions, the Fourier transform of the correlation \nintegral is H(f)X(f), which is identical to the Fourier transform of the con-\nvolution integral. These arguments for identity of \nthe two integrals are simply \nthe frequency-domain equivalents of the previously discussed time-domain \nrequirement for equality of the two integrals. \nIf x(t) and h(t) are the same function, Eq. (4.21) is normally termed \nthe autocorrelation function; if \nx(t) and h(t) differ, the term crosscorrelation \nis normally used. \nExample 4.9 Autocorrelation Function \nDetermine the autocorrelation function of the waveform \nFrom Eq. (4.21), \nh(t) = e- al \n= 0 \nt>O \nt<O \nt> 0 \n= l'\" e -aTe -a(t+T) dT \nt < 0 \ne -alii \n2a \n-oo<t<oo \nPROBLEMS \n4.1. Prove the following convolution properties: \n(a) Convolution is commutative: [h(t) * \nx(t)] = [x(t) * h(t)] \n(b) Convolution is associative: h(t) * [g(t) * \nx(t)] = [h(t) * g(t)] * x(t) \n(4.31) \n(4.32) \n(c) Convolution is distributive over addition: h(t) * [g(t) + x(t)) = h(t) * g(t) \n+ h(t) * \nx(t) \n4.2. Determine h(t) * g(t), where \n(a) h(t) = e- al \nt>O \n= 0 \nt < 0 \ng(t) = e- bl \nt>O \n= 0 \nt < 0 \n70 \n(b) h(t) = te- t \nt ~ \n0 \n= 0 \nt < 0 \ng(t) = e -t \nt > 0 \n= 0 \nt < 0 \n(c) h(t) = te- t \nt ~ \n0 \n= 0 \nt < 0 \ng(t) = et \nt < - I \n=0 \nt>-I \n(d) h(t) = 2e 3t \nt > 1 \n= 0 \nt < 0 \ng(t) = 2e t \nt < 0 \n= 0 \nt > 0 \n(e) h(t) = sin(2'lTt) \n0 !5 t !5 ! \n= 0 \nelsewhere \ng(t) = I \n0 < t < i \n= 0 \nt < 0; t > i \n(0 h(t) = I -\nt \n0 < t < I \n= 0 \nt < 0; t > I \ng(t) = h(t) \n(g) h(t) = (a - I \nt I \n)3 \n-a!5t!5a \nelsewhere \n= 0 \ng(t) = h(t) \n(h) h(t) = e -at \n= 0 \ng(t) = I -\nt \n= 0 \nt>O \nt < 0 \nO<t<1 \nt < 0; t > I \nConvolution and Correlation \nChap. 4 \n4.3. Sketch the convolution of the functions x(t) and h(t) illustrated in Fig. 4.14. \n4.4. Sketch the convolution of the two odd functions x(t) and h(t) illustrated in \nFig. 4.15. Show that the convolution of two odd functions is an even function. \n4.5. Use the convolution theorem to graphically determine the Fourier transform \nof the functions illustrated in Fig. 4.16. \n4.6. Analytically, determine the Fourier transform of e- at2 * \ne-~t2. (Hint: Use the \nconvolution theorem.) \n4.7. Use the frequency convolution theorem to graphically determine the Fourier \ntransform of the product of the functions x(t) and h(t) illustrated in Fig. 4.17. \n4.8. Graphically determine the correlation of the functions x(t) and h(t) illustrated \nin Fig. 4.14. \n4.9. Let h(t) be a time-limited function that is nonzero over the range \n-To \nTo \n--!5t!5-\n2 \n2 \nShow that h(t) * h(t) is nonzero over the range - To !5 t !5 To; that is, h(t) * \nh(t) has a \"width\" twice that of h(t). \n4.10. Show that if h(t) = f(t) * g(t), then \ndh(t) = df(t) * g(t) = f(t) * dg(t) \ndt \ndt \ndt \nChap. 4 \nProblems \n71 \nx(t) \nhIt) \n(a) \nx(t) \nhIt) \n(b) \nxlt) \nhIt) \n(e) \nFigure 4.14 Functions x(t) and h(t) for Problems 4.3 and 4.8. \nx(tl \nh(tl \nFigure 4.15 Functions x(t) and h(t) for Problem 4.4. \n4.11. By means of the frequency-convolution theorem, graphically determine the \nFourier transform of a half-wave rectified waveform. Using this result, incor-\nporate the shifting theorem to determine the Fourier transform of a full-wave \nrectified waveform. \n4.12. Graphically find the Fourier transform of the following functions: \n(a) h(t) = A cos2(27rfot) \n(b) h(t) = A sin2(27rfot) \n(c) h(t) = A cos2(27rfot) + A cos2(7rfot) \n4.13. Graphically find the inverse Fourier transform of the following functions: \n(a) [Si~~;f)r \n1 \n(b) (1 + j27rff \n72 \n, \n\\ , \n\\ \n, \nI \nI \n\\ , \nhit) \nI.) \nhit) \nIb) \nConvolution and Correlation \nI \nI \nI \n/ \nI \n/ \nI \n\\ Si, It/f) \n\\ \nt -\n0 \nFigure 4.16 Functions for Problem 4.5. \nxlt) \nI.) \nxlt) \n.-Blt! \nIb) \nI \nI \n, \nhit) \nhit) \nFigure 4.17 Functions x(t) and h(t) for Problem 4.7. \n(c) e -12.,..fl \n(d) 1 - e- 1fl \n\\ \n\\ \n\\ \nChap. 4 \n4.14. Develop a set of rules for determining the limits of integration for the corre-\nlation integral. \nREFERENCES \n1. BRACEWELL, R. The Fourier Transform and Its Applications, 2d Rev. Ed. New \nYork: McGraw-Hill, 1986. \nChap. 4 \nReferences \n73 \n2. GUPTA, S. Transform and State VariabLe Methods in Linear Systems. New York: \nWiley, 1966. \n3. HEALY, T. J. \"Convolution Revisited.\" IEEE Spectrum (April 1969), Vol. 6, \nNo.4, pp. 87-93. \n4. PAPOULIS, A. The Fourier IntegraL and Its Applications, 2d Ed. New York: \nMcGraw-Hill, 1984. \n5 \nFOURIER SERIES AND \nSAMPLED WAVEFORMS \nIn the technical literature, Fourier series are normally developed indepen-\ndently of the Fourier integral. However, with the introduction of \ndistribution \ntheory, Fourier series can be theoretically derived as a special case of the \nFourier integral. This approach is significant in that it is fundamental in \nconsidering the discrete Fourier transform as a special case of the Fourier \nintegral. Also fundamental to an understanding of the discrete Fourier trans-\nform is the Fourier transform of sampled waveforms. In this chapter, we \nrelate both of these relationships to the Fourier transform and thereby pro-\nvide the framework for the development of the discrete Fourier transform \nin Chapter 6. \n5.1 FOURIER SERIES \nA periodic function y(t) with period To expressed as a Fourier series is given \nby the expression \nao \ny(t) = -\n+ L [an cos(27rnfot) + bn sin(27rnfot)] \n2 \n(5.1) \nn~1 \nwhere fo is the fundamental frequency equal to 1ITo. The magnitude of the \n74 \nSec. 5.1 \nFourier Series \nsinusoids or coefficients are given by the integrals \n2 f TO/2 \nan = -T \ny(t) cos(27rnfot) dt \no - To/2 \nn = 0, 1, 2, 3, ... \n2 f TO/2 \nbn = -T \ny(t) sin(27rnfot) dt \no - To/2 \nn = 1,2,3, ... \nBy applying the identities \nand \ncos(27rnfot) = !(e-i21Tnfot + e -j21Tnfo t ) \n2 \nsin(27rnfot) = ;/ej21Tnfot -\ne -j21Tnfo t ) \nthe expression of Eq. (5.1) can be written as \nao \n1 ~ \n'2 \nf \ny(t) = -\n+ -\nÂ£.J (an -\njbn)eJ 1Tn ot \n2 \n2 n~1 \n1 \n00 \n+ - L (an + jb \nn)e - j21Tnfot \n2 n~1 \n75 \n(5.2) \n(5.3) \n(5.4) \n(5.5) \n(5.6) \nTo simplify this expression, negative values of n are introduced in Eqs. (5.2) \nand (5.3). \n2 f TO/2 \na- n = -T \ny(t) cos( -27rnfot) dt \no - To/2 \n2 f TO/2 \n= -T \ny(t) cos(27rnfot) dt \no \n-To/2 \n(5.7) \n= an \nn = 1,2,3, ... \n2 f TO/2 \nb- n = -T \ny(t) sin( -27rnfot) dt \no - To/2 \n2 f TO/2 \n--T \ny(t) sin(27rnfot) dt \no \n-Tol2 \n(5.8) \n-b n \nn = 1,2,3, ... \nHence, we can write \n(5.9) \nn~1 \nn~ \n-I \n76 \nFourier Series and Sampled Waveforms \nand \nL jb \nne - j2'TtnJot = \nL jb nej2'TtnJ ot \nn=1 \nn= -I \nSubstitution of Eqs. (5.9) and (5.10) into Eq. (5.6) yields \nao \nI \n00 \ny(t) = -\n+ - L \n(an - jb n )ej 2'TtnJ ot \n2 \n2 n =-00 \nn= -= \nChap. 5 \n(5.10) \n(5.Il) \nEquation (5.Il) is the Fourier series expressed in exponential form; coef-\nficients an are, in general, complex. Because \nn = 0, Â± I, Â± 2, ... \nthe combination of Eqs. (5.2), (5.3), (5.7), and (5.8) yields \nI f TO/2 \nan = -\ny(t)e -j2'TtnJot dt \nTo \n- To/2 \nn = 0, Â±I, Â±2, .. \n(5.12) \nThe expression of the Fourier series in exponential form, Eg. (5.Il), and \nthe complex coefficients in the form of Eg. (5.12) is normally the preferred \napproach in analysis. \nExample 5.1 Triangular-Waveform Fourier Series \nDetermine the Fourier series of the periodic function illustrated in Fig. 5.1. \nFrom Eq. (5.12), because y(t) is an even function, then \n1 J\nTO!2 \n-T \ny(t) cos(27rnfot) dt \no - To!2 \nan = \nT\nl JO \n(T\n2 + T42t) cos(27rnfot) dt \no \n- To!2 \n0 \n0 \n+ ;0 L\nTO\n!2 (~O - ;6 t) cos(27rnf \not) dt \nn = 0, 1, 3, 5, ... \no \nn = 2,4,6, ... \n(5.13) \nl\n~:T' \n~, \nTo \nn = 1,3,5, ... \nan = \nn = 0 \nSec. 5.2 \nFourier Series as a Special Case of the Fourier Integral \n\\ \nHence, \n\\ , \nvlt) \nFigure 5.1 \nPeriodic triangular waveform. \n/ \n/ \n/ \n1 \n8 \n[ \n1 \n1 \nJ \ny(t) = To + 7r2To \ncos(27rfot) + 32 cos(67rfot) + 52 cos(107rfot) + ... \nwhere fo = liTo. \n5.2 FOURIER SERIES AS A SPECIAL CASE \nOF THE FOURIER INTEGRAL \n(5.14) \nConsider the periodic triangular function illustrated in Fig. 5.2(e). From Ex. \n5.1, we know that the Fourier series of this waveform is an infinite set of \nsinusoids. We will now show that an identical relationship can be obtained \nfrom the Fourier integral. \nTo accomplish the derivation, we utilize the convolution theorem, Eq. \n(4.14). Note that the periodic triangular waveform (period To) is simply the \nconvolution of the single triangle shown in Fig. 5.2(a) and the infinite se-\nquence of equidistant impulses illustrated in Fig. 5.2(b). The periodic func-\ntion yet) can then be expressed by \ny(t) = h(t) * \nx(t) \n(5.15) \nFourier transforms of both h(t) and x(t) have been determined previously \nand are illustrated in Figs. 5.2(c) and (d), respectively. From the convolution \ntheorem, the desired Fourier transform is the product of these two frequency \n77 \n78 \nFourier Series and Sampled Waveforms \nChap.S \nhltl \nxltl \nIbl \nleI \n1 \nYIn = HlflXIfI \nTo! \\ \n\\ \nXlfl \nlITo \n... \n... \n-2 \n2 \n-r;, \n-r;, \n-1 \n1 \n-r;, To \nleI \nIdl \nFigure 5.2 Graphical convolution theorem development of \nthe Fourier transform \nof a periodic triangular waveform. \nSec. 5.3 \nWaveform Sampling \n79 \nfunctions: \nY(f) = H(f)X(f) \n= H(f)-.l :i: 8 (f - !!:...) \nTo n~-oo \nTo \n(5.16) \n- -.l :i: H(!!:...) 8 (f - !!:...) \nTo n= _00 \nTo \nTo \nEquations (2.44) and (4.19) were used to develop Eq. (5.16). \nThe Fourier transform of the periodic function is then an infinite set \nof sinusoids (i.e., an infinite sequence of equidistant impulses) with ampli-\ntudes of H(nITo). Recall that the Fourier series of a periodic function is an \ninfinite sum of sinusoids with amplitudes given by Un, Eq. (5.12). But note \nthat because the limits of integration of Eq. (5.12) are from - To12 to To/2 \nand because \nh(t) = y(t) \nTo \nTo \n-- < t <-\n2 \n2 \n(5.17) \nthe function y(t) can be replaced by h(t) and Eq. (5.12) can be rewritten in \nthe form: \n1 f TO/2 \nUn = -\nh(t)e - j27rnfot dt \nTo \n-To/2 \n(5.18) \n= -.l H(nfo) = -.l H(!!:...) \nTo \nTo \nTo \nThus, the coefficients as derived by means of the Fourier integral and those \nof the conventional Fourier series are the same for a periodic function. Also, \na comparison of Figs. 5.2(c) and (f) reveals that except for a factor liTo, \nthe coefficients Un of the Fourier series expansion of y(t) equal the values \nof the Fourier transform H(f) evaluated at nlTo. \nIn summary, we point out again that the key to the preceding devel-\nopment is the incorporation of distribution theory into Fourier integral the-\nory. As will be demonstrated in the discussions to follow, this unifying con-\ncept is basic to a thorough understanding of the discrete Fourier transform \nand hence the fast Fourier transform. \n5.3 WAVEFORM SAMPLING \nIn the preceding chapters, we have developed a Fourier transform theory \nthat considers both continuous and impulse functions oftime. Based on these \ndevelopments, it is straightforward to extend the theory to include sampled \nwaveforms, which are of particular interest in this book. We have developed \n80 \nFourier Series and Sampled Waveforms \nChap. 5 \nsufficient tools to investigate in detail the theoretical as well as the visual \ninterpretations of sampled waveforms. \nIf the function h(t) is continuous at t = T, then a sample of h(t) at time \nequal to T is expressed as \nh(t) = h(t)8(t -\nT) = h(T)8(t -\nT) \n(5.19) \nwhere the product must be interpreted in the sense of distribution theory \n[Eq. (A.12)]. The impulse that occurs at time T has an area equal to the \nfunction value at time T. If h(t) is continuous at t = nT for n = 0, Â± 1, \nÂ±2, ... , \nh(t) = \n~ h(nT)8(t -\nnT) \n(5.20) \nn= -00 \nh(t) is termed the sampled waveform h(t) with sample interval T. Sampled \nh(t) is then an infinite sequence of equidistant impulses, each of whose \namplitude is given by the value of h(t) corresponding to the time of occur-\nrence of the impulse. Figure 5.3 illustrates graphically the sampling concept. \nSince Eq. (5.20) is the product of the continuous function h(t) and the se-\nquence of impulses, we can employ the frequency-convolution theorem, Eq. \n(4.17), to derive the Fourier transform of the sampled waveform. As illus-\ntrated in Fig. 5.3, the sampled function [Fig. 5.3(e)] is equal to the product \nof the waveform h(t) shown in Fig. 5.3(a) and the sequence of impulses d(t) \nillustrated in Fig. 5.3(b). We call d(t) the sampling function; the notation \nd (t) will always imply an infinite sequence of impulses separated by T. The \nFourier transforms of h(t) and d(t) are shown in Figs. 5.3(c) and (d), re-\nspectively. Note that the Fourier transform of the sampling function d(t) is \na(f); this function is termed the frequency-sampling function. From the \nfrequency-convolution theorem, the desired Fourier transform is the con-\nvolution of the frequency functions illustrated in Figs. 5.3(c) and (d). The \nFourier transform of the sampled waveform is then a periodic function, \nwhere one period is equal, within a constant, to the Fourier transform of \nthe continuous function h(t). This last statement is valid only if the sampling \ninterval T is sufficiently small. \nIf T is chosen too large, the results illustrated in Fig. 5.4 are obtained. \nNote that as the sample interval T is increased [Figs. 15.3(b) and 5.4(b)], the \nequidistant impulses of a(f) become more closely spaced [Figs. 5.3(d) and \n5.4(d)]. Because of the decreased spacing of the frequency impulses, their \nconvolution with the frequency function H(f) [Fig. 5.4(c)] results in the \noverlapping waveform illustrated in Fig. 5.4(0. This distortion of the desired \nFourier transform of a sampled function is known as aliasing. As described, \naliasing occurs because the time function was not sampled at a sufficiently \nhigh rate, i.e., the sample interval T is too large. It is then natural to pose \nthe question: How does one ensure that the Fourier transform of a sampled \nfunction is not aliased? An examination of Figs. 5.4(c) and (d) points up the \nSec. 5.3 \nWaveform Sampling \nhIt) \n1 \nHlf) \nIe) \nh(t).1 (t) \n\\ I \n\\ \n\\ \n(e) \nIf) \n1 \nT \nTt-\nIb) \n~f) \nId) \nl----j f \nT \nFigure 5.3 Graphical frequency-convolution theorem development of \nthe Fourier \ntransform of a sampled waveform. \n81 \nfact that convolution overlap occurs until the separation of the impulses of \nA(f) is increased to liT = 2Ie, where Ie is the highest frequency component \nof \nthe Fourier transform of \nthe continuous function h(t). That is, if \nthe sample \ninterval T is chosen equal to one-half the reciprocal of the highest frequency \ncomponent, aliasing does not occur. This is an extremely important concept \nin many fields of scientific application; the reason is that we need only retain \n82 \nFourier Series and Sampled Waveforms \nChap. 5 \nhit) \nA(t) \n-\nT I-\n(b) \nh(t)A(t) \n, \n.... \n, \\ \n\\ \n\\ \n\" \nI \", \nI \n\\ \n(e) \nLa \nH(f)'Il.(f) \n, I, \n1/ \" \nII I' \\ \nI \n~ \nII \\' \n1/ \nI, \nI \nI \nI -\n, , I \n, \n,\\ ,\\ \" \n1\\ \nII \n\\ \n, \n\\ \n\\ , \n.fe \nfe \n(f) \nIl.(f) \nCONVOLUTION \n1 \nT \n-fe \nfe \n.!. l-\nT \n(e) \n(d) \nFigure 5.4 Aliased Fourier transform of a waveform sampled at an insufficient rate. \nsamples of the continuous waveform to determine a replica of the continuous \nFourier transform. Furthermore, if \na waveform is sampled such that aliasing \ndoes not occur, these samples can be appropriately combined to reconstruct \nidentically the continuous waveform. This is merely a statement of the sam-\npling theorem that we investigate in Sec. 5.4. \nSec. 5.4 \nSampling Theorems \n83 \nExample 5.1 Time-Domain Aliasing \nFigure 5.5 illustrates the concept of aliasing from a time-domain viewpoint. Note \nthat the sample interval T was not chosen less than one-half the period of the highest \nfrequency component of the time waveform. As a result, the equally spaced sample \nvalues shown can represent at least two sinusoids of different frequencies. In the \ntime domain, aliasing is characterized by the inability to distinguish the frequency \nof the sinusoid that the sample values represent. \nFigure 5.5 Time-domain example of aliasing. \n5.4 SAMPLING THEOREMS \nThe sampling theorem states that if the Fourier transform of a function h(t) \nis zero for all frequencies greater than a certain frequency fe, then the con-\ntinuous function h(t) can be uniquely determined from a knowledge of its \nsampled values \nh(t) = L \nh(nT)8(t -\nnT) \n(5.21) \nn= -00 \nwhere T = 1I2f \ne' \nIn particular, h(t) is given by \nh(t) = T i \nh(nT) sin 27rfe(t -\nnT) \nn= -00 \n7r(t -\nnT) \n(5.22) \nConstraints of the theorem are illustrated graphically in Fig. 5.6. First, it is \nnecessary that the Fourier transform of h(t) be zero for frequencies greater \nthan fe. As shown in Fig. 5.6(c), the example frequency function is band-\nlimited at the frequency fe; the term band-limited is a shortened way of \nsaying that the Fourier transform is zero for I \nf I \n> fe. The bandwidth of a \nsignal is the width of the positive frequency band where the amplitude is \nnonzero. The bandwidth of the waveform illustrated in Fig. 5.6(c) is then \nfe. The second constraint is that the sample spacing be chosen as T = 11 \n2fe, that is, the impulse functions of Fig. 5.6(d) are required to be separated \nby liT = 2fe. This spacing ensures that when A(f) and H(f) are convolved, \nthere is no aliasing. Alternately, the functions H(f) and H(f) * ~(f), as \n84 \nhit) \nla) \nIe) \nI \nI \nI \nFourier Series and Sampled Waveforms \nChap. 5 \n.. . \nhit) Dolt) \nÂ·2T Â·T \nT 2T \nIe) \nHlf)x~lf) \nIf) \nCONVOLUTION \n\"It) \n1 \n- TI-\nIb) \n~If) \nId) \n1 \nT \n1-----1 \nT \n.. \nt \nFigure 5.6 Fourier transform of a waveform sampled at the Nyquist sampling rate. \nillustrated in Figs, 5.6(c) and (f), respectively, are equal in the interval I \nf I \n< fe, within the scaling constant T. If T > 1I2f \ne, then aliasing will result; \nif T < lI2fe, the theorem still holds. The requirement that T = lI2fe is \nsimply the maximum spacing between samples for which the theorem holds. \nFrequency liT = 2fe is known as the Nyquist sampling rate. Given that \nthese two constraints are true, the theorem states that h(t) [Fig. 5.6(a)] can \nbe reconstructed from a knowledge of the impulses illustrated in Fig. 5.6(e). \nSec. 5.4 \nSampling Theorems \n85 \nTo construct a proof \nofthe sampling theorem, recall from the discussion \non constraints of the theorem that the Fourier transform of the sampled \nfunction is identical, within the constant T, to the Fourier transform of the \nun sampled function, in the frequency range - f \nc ::5 f ::5 f \nc' From Fig. 5 \n.6(f) , \nthe Fourier transform of the sampled time function is given by H(f) * a \n(f). \nHence, as illustrated in Figs. 5.7(a), (b), and (e), the multiplication of a \nrectangular-frequency function of amplitude T with the Fourier transform \nIe) \nhit) \nCONVOLUTION \nT I--\nIe) \nÂ·f \ne \nT Olf) \nIb) \nqlt) \nId) \nFigure 5.7 Graphical derivation of the sampling theorem. \n86 \nFourier Series and Sampled Waveforms \nChap. 5 \nof the sampled waveform is the Fourier transform H(f): \nH(f) = [H(f) * Ll(f)]Q(f) \n(5.23) \nThe inverse Fourier transform of H(f) is the original waveform h(t), as \nshown in Fig. 5.7(f). But from the convolution theorem, h(t) is equal to the \nconvolution of the inverse Fourier transforms of H(f) * Ll (f) and of the \nrectangular-frequency function. Hence, h(t) is given by the convolution of \nh(t)il(t) [Fig. 5.7(c)] and q(t) [Fig. 5.7(d)]: \nh(t) = [h(t)il(t)] * q(t) \nL \n[h(nT)8(t -\nnT)] * q(t) \nn= -00 \nL \nh(nT)q(t -\nnT) \n(5.24) \nn= -00 \n= T :i h(nT) sin[21Tfe(t -\nnT)] \nn~ \n-= \n1T(t -\nnT) \nFunction q(t) is given by the Fourier transform pair of Eq. (2.31). Equation \n(5.24) is the desired expression for reconstructing h(t) from a knowledge of \nonly the samples of h(t). \nWe should note carefully that it is possible to reconstruct a sampled \nwaveform perfectly only if the waveform is band-limited. In practice, this \ncondition rarely exists. The solution is to sample at such a rate that aliasing \nis negligible; it may be necessary to filter the signal prior to quantization to \nensure that there exists, to the extent possible, a band-limited function. \nThe band-limited waveforms considered in this section are referred to \nas baseband signals. This nomenclature refers to signals whose frequency \nspectrum generally occupy the frequency range 0 :::; f < fe. A band-pass \nsignal is one whose frequency spectrum occupies the frequency range flow \n< f < fhigh and flowÂ» O. The sampling theorem developed here can be \napplied to either baseband or band-pass signals. However, more efficient \nsampling theorems for band-pass signals are developed in Chapter 14. \nFrequency-Sampling Theorem \nAnalogous to time-domain sampling is a sampling theorem in the fre-\nquency domain. If \na function h(t) is time-limited, that is, \nh(t) = 0 \nI \nt I> Te \n(5.25) \nthen its Fourier transform H(f) can be uniquely determined from equidistant \nChap.S \nProblems \n87 \nsamples of H(f). In particular, H(f) is given by \nH(f) = _1_ i \nH(~) \nsin[27rTc(f -\nn12TJ] \n2Tc n~ \n-= \n2Tc \n7r(f -\nn12Tc) \n(5.26) \nThe proof is similar to the proof of the time-domain sampling theorem. \nPROBLEMS \n5.1. Find the Fourier series of the periodic waveforms illustrated in Fig. 5.8. \n5.2. Determine the Fourier transform of the waveforms illustrated in Fig. 5.8. Com-\npare these results with those of Problem 5.1. \n5.3. By using graphical arguments similar to those of Fig. 5.4, determine the Ny-\nquist sampling rate for the time functions whose Fourier transform magnitude \nfunctions are illustrated in Fig. 5.9. \n5.4. Graphically justify the band-pass sampling theorem that states that \nCritical sampling frequency = \n2fhigh \nI\nÂ· \nd' \nfhigh \nargest mteger not excee mg (f. \n_ f \n) \nhigh \nlow \nwhere fhigh and flow are the upper and lower cutoff frequencies of the band-\npass spectrum. \n5.5. Assume that the function h(t) = cos(27rt) has been sampled at t = n/4, where \nn = 0, Â± 1, Â± 2, .... \nSketch h(t) and indicate the sampled values. Graphically \nand analytically determine Eq. (5.24) for h (t = 7/8), where the summation is \nonly over n = 2, 3, 4, and 5. \nx(t' \nI \nI \nI \nI \n-To \nTo \nt \n-----\n(b, \nFigure 5.8 Waveforms for Problems 5.1 and 5.2. \n88 \nFourier Series and Sampled Waveforms \nChap. 5 \n(a) \n(b) \nFigure 5.9 Functions for Problem 5.3. \n5.6. A frequency function (say a filter frequency response) has been determined \nexperimentally in the laboratory and is given by a graphical curve. If it is \ndesired to sample this function for computer storage purposes, what is the \nminimum frequency-sampling interval if the frequency function is to be later \ntotally reconstructed? State all assumptions. \nREFERENCES \n1. BRACEWELL, R. The Fourier Transform and Its Applications, 2d Ed. New York: \nMcGraw-Hill, 1986. \n2. PAPOULlS, A. The Fourier Integral and Its Applications, 2d Ed. New York: \nMcGraw-Hill, 1984. \n3. SCHWARTZ, M., AND L. SHAW. Signal Processing: Discrete Spectral Analysis, \nDetection, and Estimation. New York: McGraw-Hill, 1975. \n6 \nTHE DISCRETE FOURIER \nTRANSFORM \nNormally, a discussion of \nthe discrete Fourier transform is based on an initial \ndefinition of the finite-length discrete transform; from this assumed axiom, \nthose properties of the transform implied by this definition are derived. This \napproach is unrewarding in that at its conclusion there is always the un-\nanswered question: How does the discrete Fourier transform relate to the \ncontinuous Fourier transform? To answer this question, we find it preferable \nto derive the discrete Fourier transform as a special case of continuous \nFourier transform theory. Clearly, the discrete Fourier transform can be \ndefined independently of the Fourier transform. However, many applica-\ntions involving the continuous Fourier transform rely on a digital computer \nfor implementation, which leads to the use of the discrete Fourier transform \nand hence the FFT. Both approaches yield identical results; the distinction \nis in the interpretation of the results. \nIn this chapter, we develop a special case of the continuous Fourier \ntransform that is amenable to machine computation. The approach is to \ndevelop the discrete Fourier transform from a graphical derivation based on \ncontinuous Fourier transform theory. These graphical arguments are then \nsubstantiated by a theoretical development. Both approaches emphasize the \nmodifications of continuous Fourier transform theory that are necessary to \ndefine a computer-oriented transform pair. We also develop properties of \nthe discrete Fourier transform. \n89 \n90 \nThe Discrete Fourier Transform \nChap. 6 \n6.1 A GRAPHICAL DEVELOPMENT \nConsider the example function h(t) and its Fourier transform H(f), as il-\nlustrated in Fig. 6.1(a). It is desired to modify this Fourier transform pair \nin such a manner that the pair is amenable to digital computer computation. \nThis modified pair, termed the discrete Fourier transform, is to approximate \nas closely as possible the continuous Fourier transform. \nTo determine the Fourier transform of h(t) by means of digital analysis \ntechniques, it is necessary to sample h(t), as described in Chapter 5. Sam-\npling is accomplished by multiplying h(t) by the sampling function illustrated \nin Fig. 6.1 (b). The sample interval is T. Sampled function h(t) and its Fourier \ntransform are illustrated in Fig. 6.1(c). This Fourier transform pair represents \nthe first modification to the original pair, which is necessary in defining a \ndiscrete transform pair. Note that to this point the modified transform pair \ndiffers from the original transform pair only by the aliasing effect that results \nfrom sampling. As discussed in Sec. 5.3, if the waveform h(t) is sampled at \na frequency of at least twice the largest frequency component of h(t), there \nis no loss of information as a result of sampling. If \nthe function h(t) is not \nband-limited, i.e., H(f) Â¥- 0 for some I \nf I \n> fe, then sampling will introduce \naliasing, as illustrated in Fig. 6.1(c). To reduce this error, we have only one \nrecourse, and that is to sample faster, that is, choose T smaller. \nThe Fourier transform pair in Fig. 6.1(c) is not suitable for machine \ncomputation because an infinity of samples of h(t) is considered; it is nec-\nessary to truncate the sampled h(t) so that only a finite number of points, \nsay N, are considered. The rectangular, or truncation, function and its Four-\nier transform are illustrated in Fig. 6.1(d). The product of the infinite se-\nquence of impulse functions representing h(t) and the truncation function \nyields the finite-length time function illustrated in Fig. 6.1(e). Truncation \nintroduces the second modification of the original Fourier transform pair; \nthis effect is to convolve the aliased frequency transform of Fig. 6.1(c) with \nthe Fourier transform of the truncation function [Fig. 6.1(d)]. As shown in \nFig. 6.1(e), the frequency transform now has a ripple to it; this effect has \nbeen accentuated in the illustration for emphasis. To reduce this effect, recall \nthe inverse relation that exists between the width of a time function and its \nFourier transform (Sec. 3.3). Hence, if the truncation (rectangular) function \nis increased in length, then the [sin(f)]!f function approaches an impulse; \nthe more closely the [sin(f)]/f function approximates an impulse, the less \nripple or error is introduced by the convolution that results from truncation. \nTherefore, it is desirable to choose the length of the truncation function as \nlong as possible. We investigate the effect of truncation in detail in Sec. 6.4. \nThe modified transform pair of Fig. 6.1(e) is still not an acceptable \ndiscrete Fourier transform pair because the frequency transform is a con-\ntinuous function. For machine computation, only sample values of the fre-\nSec. 6.1 \nA Graphical Development \ng \n(al \nÂ· \n.. 11111111 ! \nl~u \n11111\"; \ng \n+1~lf) \nIb) -.1L-----+-~ \n____ \n~t~. \n1 f \nTo \n2\" \nhltl\"'oltlxhl \nI \n\\ \nIe) \ng \n\"t \nIdl \ng \n, \n, \n, \nI \nI \nf \nT \n1 \n2T \n\" \n, \n, \n. \nf \n\\ \nIe) ----+---+---+----\ng \nIg) \nÂ·1 \n2T \n1 \n2f \nI \n~llf) \n... t \n1 \nt \n1 \nt \n1 \nt \nt ! \nt \n! \n1111111 .. : \n-111-\nf \n;;; \nHlf) \nI---N----l \nFigure 6.1 Graphical development of the discrete Fourier transform. \n91 \n92 \nThe Discrete Fourier Transform \nChap. 6 \nquency function can be computed; it is necessary to modify the frequency \ntransform by the frequency-sampling function illustrated in Fig. 6.1(f). The \nfrequency-sampling interval is liTo. \nThe discrete Fourier transform pair of Fig. 6.1(g) is acceptable for the \npurposes of digital machine computation because both the time and fre-\nquency domains are represented by discrete values. As illustrated in Fig. \n6.1(g), the original time function is approximated by N samples; the original \nFourier transform H(f) is also approximated by N samples. These N samples \ndefine the discrete Fourier transform pair and approximate the original Four-\nier transform pair. Note that sampling in the time domain results in a periodic \nfunction offrequency; sampling in the frequency domain results in a periodic \nfunction of time. Hence, the discrete Fourier transform requires that both \nthe original time and frequency functions be modified such that they become \nperiodic functions. N time samples and N frequency values represent one \nperiod of the time- and frequency-domain waveforms, respectively. Because \nthe N values of time and frequency are related by the continuous Fourier \ntransform, then a discrete relationship can be derived. \n6.2 THEORETICAL DEVELOPMENT \nThe preceding graphical development illustrates the point that if a continuous \nFourier transform pair is suitably modified, then the modified pair is ac-\nceptable for computation on a digital computer. Thus, to develop this dis-\ncrete Fourier transform pair, it is only necessary to derive the mathematical \nrelationships that result from each of the required modifications: time-do-\nmain sampling, truncation, and frequency-domain sampling. \nConsider the Fourier transform pair illustrated in Fig. 6.2(a). To dis-\ncretize this transform pair, it is first necessary to sample the waveform h(t); \nthe sampled waveform can be written as h(t)ilo(t), where ilo(t) is the time-\ndomain sampling function illustrated in Fig. 6.2(b). The sampling interval is \nT. From Eq. (5.20), the sampled function can be written as \nh(t)ilo(t) = h(t) \n~ 8(t - kD \nk= -00 \n(6.1) \n~ h(kD8(t - kD \nk= -00 \nThe result of this multiplication is illustrated in Fig. 6.2(c). Note the aliasing \neffect that results from the choice of T. \nNext, the sampled function is truncated by multiplication with the rec-\nSec. 6.2 \nTheoretical Development \n93 \n'~ \ng ~ \n(a) \n. \nt \nf \nÂ·Â·IIIHllI f \nI \nii'j'll 11 I \n! \n... \ng \n+ \nrill \n1. \n. \n(b) \n-m-\nt \n., \n, \nf \nf \nl' \nf'Â·O'\" \nIH(I)Â·~(I)I \n, \n-\n, \nI \n, \n' Jlllll1.,.,.,.<o â¢ \ng \n(e) \n-m-\nt \n., \n, \n2f \n2f \nflO \n-i~-\ng \nI \n. \n(d) \nÂ·T \nTo - ~ \nt \n-, , \n2 \nTo T,\", \nh(t)\"o(t)x(t) \nIH(f).~(f).X(f)1 \n, \n-\n, \nI \n, \ng \n(e) \n., \n, \n2f \n2f \n(f) \nI\nll, \n(I) \n... t \nt \nt \nt \nt \nt \n! \n! \n! \nf ! \n1 \n! \n! \n! \n! \n111 t \n.:. \n-1'1-\nf \nT,\", \nTo \nI \n[H(f).~(f).X(f) \nJLl, (1)1 \nT,. , \nTo \n(9) \nFigure 6.2 Graphical derivation of the discrete Fourier transform pair. \ntangular function x(t), as illustrated in Fig. 6.2(d): \nx(t) \nT \nT \n- \"2 < t < To - \"2 \no \notherwise \n, \n\\ \n(6.2) \n94 \nThe Discrete Fourier Transform \nChap. 6 \nwhere To is the duration of the truncation function. An obvious question at \nthis point is: Why is the rectangular function x(t) not centered at zero or \nTo/2? Centering of x(t) at zero is avoided to alleviate notation problems. \nThe reason for not centering the rectangular function at T \n0/2 will become \nobvious later in the development. \nTruncation yields \n(6.3) \nN-\\ \n= L h(kT)8(t -\nkT) \nk=O \nwhere it has been assumed that there are N equidistant impulse functions \nlying within the truncation interval, that is, N = ToIT. The sampled truncated \nwaveform and its Fourier transform are illustrated in Fig. 6.2(e). As in the \nprevious example, truncation in the time domain results in rippling in the \nfrequency domain. \nThe final step in modifying the original Fourier transform pair to a \ndiscrete Fourier transform pair is to sample the Fourier transform of Eq. \n(6.3). In the time domain, this product is equivalent to convolving the sam-\npled truncated waveform of Eq. (6.3) and the time function 8\\(t), as illus-\ntrated in Fig. 6.2(t). Function 8\\ \n(t) is given by Fourier transform pair of Eq. \n(2.44) as \nr= -00 \nThe desired relationship is [h(t)80(t)X(t)] * 8\\(t); hence, \n[h(t)80(t)X(t)] * 8\\ (t) = [:~~ h(kT)8(t -\nkT) \n] \n* [To r~oo \n8(t -\nrTo) ] \nN-\\ \n(6.4) \n... + To L h(kT)8(t + To -\nkT) \n(6.5) \nk=O \nN-\\ \n+ To L h(kT)8(t -\nkT) \nk=O \nN-\\ \n+ To L h(kT)8(t -\nTo -\nkT) + \nk=O \nSec. 6.2 \nTheoretical Development \n95 \nNote that Eq. (6.5) is periodic with period To; in compact notation form, \nthe equation can be written as \nii(t) = To r~~ \n[:~~ h(kD8(t -\nkT -\nrTo) ] \n(6.6) \nWe choose the notation h(t) to imply that h(t) is an approximation to the \nfunction h(t). \nChoice of the rectangular function x(t), as described by Eq. (6.2), can \nnow be explained. Note that the convolution result of Eq. (6.6) is a periodic \nfunction with period To that consists of \nN samples. If \nthe rectangular function \nhad been chosen such that a sample value coincided with each end point of \nthe rectangular function, the convolution of the rectangular function with \nimpulses spaced at intervals of To would result in time-domain aliasing. That \nis, the Nth point of one period would coincide with (and add to) the first \npoint ofthe next period. To ensure that time-domain aliasing does not occur, \nit is necessary to choose the truncation interval as illustrated in Fig. 6.2(d). \n(The truncation function can also be chosen as illustrated in Fig. 6.1(d), but \nnote that the end points of the truncation function lie at the midpoint of two \nadjacent sample values to avoid time-domain aliasing.) \nTo develop the Fourier transform of Eq. (6.6), recall from the discus-\nsion on Fourier series, Sec. 5.1, that the Fourier transform of a periodic \nfunction h(t) is a sequence of equidistant impulses: \nwhere \n1 fTO-TI2 \nan = -\nh(t)e -j2-rrntITo dt \nTo \n-T12 \nSubstituting Eq. (6.6) in (6.8) yields \n1 \nfo =-\nTo \nn = 0, Â±1, Â±2, ... \n1 \nTo-TI2 \n~ \nN-) \nan = To J-T12 \nTo r~~ \nk~O h(kD8(t - kT -\nrTo)e-j 2-rrntITo dt \nIntegration is only over one period; hence, \nf\nTO-TI2 N-) \nan = \nL h( kD8(t - kDe - j2-rrntlTo dt \n-T12 \nk=O \nN-) \nfTO-TI2 \nL h(kD \ne -j2-rrntITo 8(t - kD dt \nk=O \n-T12 \nN-) \nL h(kT)e -j2-rrknTITo \nk=O \n(6.7) \n(6.8) \n(6.9) \n96 \nThe Discrete Fourier Transform \nChap. 6 \nBecause To = NT, Eq. (6.9) can be rewritten as \nN-l \nan = \n~ \nh(kDe -j2-rrknIN \nk~O \nand the Fourier transform of Eq. (6.6) is \nn = 0, Â±1, Â±2, ... \n(6.10) \n(6.11) \nFor a cursory evaluation of Eq. (6.11), it is not obvious that the Fourier \ntransform H(n/NT) is periodic, as illustrated in Fig. 6.2(g). However, there \nare only N distinct complex values computable from Eq. (6.11). To establish \nthis fact, let n = r, where r is an arbitrary integer; Eq. (6.11) becomes \nNow let n \nH(~) \n= N:i1 \nh(kDe -j2-rrkrIN \nNT \nk~O \nr + N; note that \ne-j2-rrk(r+NJIN = e-j2-rrkrINe -j2-rrk \ne - j2-rrkrlN \n(6.12) \n(6.13) \nbecause e-j2-rrk = COS(21Tk) - j sin(21Tk) = 1 for k integer valued. Thus, for \nn = r + N, \nH(r + N) = N:i1 \nh(kDe -j2-rrk(r + NJIN \nNT \nk~O \nN-l \n= ~ \nh(kDe -j2-rrkrIN \nk~O \n= H(~T) \n(6.14) \nTherefore, there are only N distinct values for which Eq. (6.11) can be \nevaluated; H(n/ND is periodic with a period of N samples. The Fourier \ntransform, Eq. (6.11), can be expressed equivalently as \nH(~) \n= N:i1 \nh(kT)e-j2-rrnkIN \nNT \nk~O \nn = 0, 1, ... ,N -\n(6.15) \nEquation (6.15) is the desired discrete Fourier transform; the expres-\nsion relates N samples of time and N samples of frequency by means of the \ncontinuous Fourier transform. The discrete Fourier transform is then a spe-\ncial case of the continuous Fourier transform. If it is assumed that the N \nsamples of the original function h(t) are one period of a periodic waveform, \nthe Fourier transform of this periodic function is given by the N samples, \nas computed by Eq. (6.15). Notation H(n/ND indicates that the discrete \nSec. 6.3 \nDiscrete Inverse Fourier Transform \n97 \nFourier transform is an approximation to the continuous Fourier transform. \nNormally, Eq. (6.15) is written as \nG(-.!!:....) = N~ \nI g(kne - j2TrnkiN \nNT \nk=O \nn = 0, 1, ... , N -\n1 \n(6.16) \nbecause the Fourier transform of the sampled periodic function g(kn is \nidentically G(nl \nNT). \n6.3 DISCRETE INVERSE FOURIER TRANSFORM \nThe discrete inverse Fourier transform is given by \ng(kn = -\nL G -.!!:.... ej2TrnkiN \n1 N-I \n( \n) \nN n=O \nNT \nk = 0, 1, ... , N -\n1 \n(6.17) \nTo prove that Eq. (6.17) and the transform relation, Eq. (6.16), form a dis-\ncrete Fourier transform pair, substitute Eq. (6.17) into Eq. (6.16). \nL - L G ~ \nej2TrrkiN e -j2TrnklN \nN-I[I N- 1 () \n] \nk=O \nN r=O \nNT \n_ ! N~1 G(~) \n[N~I ej2TrrklNe-j2TrnklN] \nN r=O \nNT \nk=O \n(6.18) \nThe identity of Eq. (6.18) follows from the orthogonality relationship: \nL ej2Trrki N \ne - j2Trnki \nN = \nN \nN-I \n{ \nk=O \nÂ° \nr = n \notherwise \n(6.19) \nThe discrete inversion formula, Eq. (6.17), exhibits periodicity in the \nsame manner as the discrete transform; the period is defined by N samples \nof g(kn. This property results from the periodic nature of ej2TrnklN. Hence, \ng(kT) is actually defined on the complete set of integers k = 0, Â± 1, Â± 2, \n. . . and is constrained by the identity \ng(kT) = g[(rN + k)T] \nr = 0, Â± 1, Â±2, ... \n(6.20) \nIn summary, the discrete Fourier transform pair is given by \ng(kT) = ! N~I G(-.!!:....)ej2TrnkIN ~ \nG(-.!!:....) = N~I g(kne-j2TrnkIN \nN n=O \nNT \nNT \nk=O \n(6.21) \nIt is important to remember that the transform pair of Eq. (6.21) requires \n98 \nThe Discrete Fourier Transform \nChap. 6 \nboth the time- and frequency-domain functions to be periodic: \nG(';T) = G[(rN\nN; n)] \nr = 0, Â± 1, Â± 2, ... \ng(kT) = g[(rN + k)T] \nr = 0, Â± 1, Â± 2, ... \n6.4 RELATIONSHIP BETWEEN THE DISCRETE \nAND CONTINUOUS FOURIER TRANSFORM \n(6.22) \n(6.23) \nThe discrete Fourier transform is of interest primarily because it approxi-\nmates the continuous Fourier transform. Validity of this approximation is \nstrictly a function of the waveform being analyzed. In this section, we use \ngraphical analysis to indicate for general classes of functions the degree of \nequivalence between the discrete and continuous transform. As will be \nstressed, differences in the two transforms arise because of the discrete \ntransform requirement for sampling and truncation. \nBand-Limited Periodic Waveforms: Truncation \nInternal Equal to Period \nConsider the function h(t) and its Fourier transform, as illustrated in \nFig. 6.3(a). We wish to sample h(t), truncate the sampled function to N \nsamples, and apply the discrete Fourier transform of Eq. (6.16). Rather than \napplying this equation directly, we develop its application graphically. Wave-\nform h(t) is sampled by multiplication with the sampling function, as illus-\ntrated in Fig. 6.3(b). Sampled waveform h(kT) and its Fourier transform are \nillustrated in Fig. 6.3(c). Note that for this example there is no aliasing. Also \nobserve that as a result of time-domain sampling, the frequency domain has \nbeen scaled by the factor liT; the Fourier transform impulse now has an \narea of AI2T rather than the original area of A12. The sampled waveform is \ntruncated by mUltiplication with the rectangular function, as illustrated in \nFig. 6.3(d); Fig. 6.3(e) illustrates the sampled and truncated waveform. As \nshown, we chose the rectangular function, so that the N sample values re-\nmaining after truncation equate to one period of the original waveform h(t). \nThe Fourier transform of the finite-length sampled waveform [Fig. \n6.3(e)] is obtained by convolving the frequency-domain impulse functions \nof Fig. 6.3(c) and the [sin(f)]!f frequency function of Fig. 6.3(d). Figure \n6.3(e) illustrates the convolution results; an expanded view of this convo-\nlution is shown in Fig. 6.4(b). A [sin(f)]!f function (dashed line) is centered \non each impulse of Fig. 6.4(a) and the resultant waveforms are additively \ncombined (solid line) to form the convolution result. \nWith respect to the original transform H(f), the convolved frequency \nfunction [Fig. 6.4(b)] is significantly distorted. However, when this function \nSec. 6.4 \nRelationship Between the Discrete and Continuous Fourier Transform \nhIt) \nI] I'\" \nA \nI , \n, , \n, \ng \nla) \n.. \nTo \n-1 \n1 \nf \n1';;1';; \nÂ· \n.. 1111111 li!li:Ullllll \n... \ng \ntr'\" \n1. \n. Ib) \nt \n-1 \n1 f \nT \nf \nhlt)<1o(t) \ni \nI \n~\"'.~'\" \ng \nI 1 \n! \nt \nIe) \n-1 \n1 \nf \n;=;;;=;; \noI \ng \n. Id) \n.. \nTo' f \nt \n-1 \n1 \nf \n1';;1';; \nh(t)<1o(t)x(t) \nIHlf) 'A.,(f).Xlf)1 \nA \ng \nIe) \n'f'd \nQ \nr'\" \n. II) \n... t \nt \nt \nt \nt \nt \nttl f ttl t \nt \nt \nt \nt \nt \nt \n.:. \n-To \nTo \nt \n-111+-\nf \n;=;; \n[h(t)<1o(t)xlt) \n).<1 1 \nIt) \n[HII).A.,(f).XII) \n)~1 \nIf) \nQ \nIg) \nFigure 6.3 Discrete Fourier transform of a bandÂ· \nlimited periodic waveform: the \ntruncation interval is equal to one period. \n99 \nis sampled by the frequency-sampling function illustrated in Fig. 6.3(1), the \ndistortion is eliminated. This follows because the equidistant impulses of the \nfrequency-sampling function are separated by liTo; at these frequencies, \nthe solid line of Fig. 6.4(b) is zero except at frequency Â± liTo. Frequency \n100 \nA \n2T \n(b) \nThe Discrete Fourier Transform \nChap. 6 \nFigure 6.4 Expanded illustration of the convolution of Fig. 6.3(e). \nÂ± UTo corresponds to the frequency-domain impulses of the original fre-\nquency function H(f). Because of time-domain truncation, these impulses \nnow have an area of ATol2T rather than the original area of A12. (Figure \n6.4(b) does not take into account that the Fourier transform of the truncation \nfunction x(t), as illustrated in Fig. 6.4(d) , is actually a complex frequency \nfunction; however, had we considered a complex function, similar results \nwould have been obtained.) \nMultiplication of the frequency function of Fig. 6.3(e) and the fre-\nquency-sampling function LlJ (f) implies the convolution of \nthe time functions \nshown in Figs. 6.3(e) and (t). Because the sampled truncated waveform [Fig. \n6.3(e)] is exactly one period of the original waveform h(t) and because the \ntime-domain impulse functions of Fig. 6.3(t) are separated by To, then their \nconvolution yields a periodic function, as illustrated in Fig. 6.3(g). This is \nsimply the time-domain equivalent to the previously discussed frequency \nsampling that yielded only a single impulse or frequency component. The \ntime function of Fig. 6.3(g) has a maximum amplitude of ATo as compared \nto the original maximum value of \nA as a result offrequency-domain sampling. \nExamination of Fig. 6.3(g) indicates that we have taken our original \ntime function, sampled it, and then multiplied each sample by To. The Four-\nier transform of this function is related to the original frequency function by \nthe factor A ToI2T. Factor To is common and can be eliminated. If \nwe desire \nto compute the Fourier transform by means of the discrete Fourier trans-\nSec. 6.4 \nRelationship Between the Discrete and Continuous Fourier Transform \n101 \nform, it is necessary to multiply the discrete time function by the factor T, \nwhich yields the desired A/2 area for the frequency function; Eq. (6.16) thus \nbecomes \nH(~) \n= T N~I h(kT)e -j2-rrnkIN \nNT \nk=O \n(6.24) \nWe expect this result because the relationship of Eq. (6.24) is simply the \nrectangular rule for integration of the continuous Fourier transform. \nThis example represents the only class of waveforms for which the \ndiscrete and continuous Fourier transforms are exactly the same within a \nscaling constant. Equivalence of the two transforms requires (1) the time \nfunction h(t) must be periodic, (2) h(t) must be band-limited, (3) the sampling \nrate must be at least two times the largest frequency component of h(t), and \n(4) the truncation function x(t) must be nonzero over exactly one period (or \ninteger multiple period) of h(t). \nBand-Limited Periodic Waveforms: Truncation \nInterval Not Equal to Period \nIf a band-limited periodic function is sampled and truncated to consist \nof other than an integer multiple of the period, the resulting discrete and \ncontinuous Fourier transform differs considerably. To examine this effect, \nconsider the illustrations of Fig. 6.5. This example differs from the preceding \nonly in the frequency of the sinusoidal waveform h(t). As before, function \nh(t) is sampled [Fig. 6.5(c)] and truncated [Fig. 6.5(e)]. Note that the sampled \ntruncated function is not an integer multiple of the period of h(t); therefore, \nwhen the time functions of Figs. 6.5(e) and (0 are convolved, the periodic \nwaveform of Fig. 6.5(g) results. Although this function is periodic, it is not \na replica of the original periodic function h(t). We would not expect the \nFourier transform of the time waveforms of Fig. 6.5(a) and (g) to be equiv-\nalent. It is of value to examine these same relationships in the frequency \ndomain. \nFourier transform of the sampled truncated waveform of Fig. 6.5(e) is \nobtained by convolving the frequency-domain impulse functions of Fig. \n6.5(c) and the [sin(f)]/f function illustrated in Fig. 6.5(d). This convolution \nis graphically illustrated in an expanded view in Fig. 6.6. Sampling of the \nresulting convolution at frequency intervals of liTo yields the impulses as \nillustrated in Fig. 6.6 and, equivalently, Fig. 6.5(g). These sample values \nrepresent the Fourier transform ofthe periodic time waveform of Fig. 6.5(g). \nNote that there is an impulse at zero frequency. This component represents \nthe average value of the truncated waveform; because the truncated wave-\nform is not an even number of cycles, the average value is not expected to \nbe zero. The remaining frequency-domain impulses occur because the zeros \n102 \nThe Discrete Fourier Transform \nChap. 6 \nhit) \nâ¢ Hlf) \ng \nell \n-1 \n1 \nT, \nT, \nla) \nÂ·Â·Â·111111111 il;l'i'llllllllÂ·Â·~ 0 \n~r'l) \nL \n--M-\nt \n-1 \n1 \nf \nIb) \nf \nf \nhlt)~olt) \n~lll\"'Â·~'\" \ng \n1,1 \nt L \n-1 \n-1 \n1 \n1 f \nT \nT, \nT, \nT \nIe) \n'1'\" \n~ \ng \nI \n.. \n.. \n-T \nTo - ~ \nt \n-1 \n1 \n2\" \nfo fo \nId) \nh(t)\"'olt)xlt) \nIHlf).,\\(f).X(f)1 \n,. \ng \n\\ \nI \nI \n\\ \nI \n-1 \n1 \nIe) \nI, \nT, \n'Â·r\" \ng \nr'\" \n.. \n'''Ittflllff 111111111 .. : \n-To \nTo \nt \n-lU-.-\nf \nIf) \nTo \nglkT) \nGin/NT) \nFigure 6.S Discrete Fourier transform of a band-limited periodic waveform: the \ntruncation interval is not equal to one period. \nof the [sin(f)]/! function are not coincident with each sample value as was \nthe case in the previous example. \nThis discrepancy between the continuous and discrete Fourier trans-\nSec. 6.4 \nRelationship Between the Discrete and Continuous Fourier Transform \n103 \nFigure 6.6 Expanded illustration of the convolution of Fig. 6.5(e); To = 3.5T1 â¢ \nforms is probablY the one most often encountered and least understood by \nusers of the discrete Fourier transform. The effect of truncation at other \nthan a multiple of the period is to create a periodic function with sharp \ndiscontinuities, as illustrated in Fig. 6.5(g). Intuitively, we expect the intro-\nduction of these sharp changes in the time domain to result in additional \nfrequency components in the frequency domain. Viewed in the frequency \ndomain, time-domain truncation is equivalent to the convolution ofa [sin(f)]/ \nf function with the single impulse representing the original frequency func-\ntion H(f). Consequently, the frequency function is no longer a single im-\npulse, but rather a continuous function of frequency with a local maximum \ncentered at the original impulse and a series of other peaks that are termed \nside \nlobes . These sidelobes are responsible for the additional frequency com-\nponents that occur after frequency-domain sampling. This effect is termed \nleakage and is inherent in the discrete Fourier transform because of the \nrequired time-domain truncation. Techniques for reducing leakage are ex-\nplored in Sec. 9.2. \nFinite-Duration Waveforms \nThe preceding two examples have explored the relationship between \nthe discrete and continuous Fourier transforms for band-limited periodic \nfunctions. Another class of functions of interest is that of finite duration, \nsuch as the function h(t) illustrated in Fig. 6.7. If h(t) is time-limited, its \nFourier transform cannot be band-limited; sampling must result in aliasing. \nIt is necessary to choose the sample interval T such that aliasing is reduced \nto an acceptable range. As illustrated in Fig. 6.7(c), the sample interval T \nwas chosen too large and as a result there is significant aliasing. \nIf the finite-length waveform is sampled and if N is chosen equal to \nthe number of samples of the time-limited waveform, then it is not necessary \nto truncate in the time domain. Truncation is omitted and the Fourier trans-\nform of the time-sampled function [Fig. 6.7(c)] is multiplied by ~I(f), the \nfrequency-domain sampling function. The time-domain equivalent to this \n104 \nThe Discrete Fourier Transform \nChap. 6 \nh \ng ~ \n.. \n.. \nTo \nf \n(a) \n.. Â·11111111 i \nI \nii'i'll II II I \nL \n0 \n{\"\"' \nL \n~ \nt \nÂ·1 \n1 f \n(b) \nT \nT \n~ \n(e) \nÂ·1 \n1 \nf \n2T \n2T \n{\"\" \ng \n.. \n. \nTo \nTo \nt \n(d) \n(e) \nFigure 6.7 Discrete Fourier transform of a time-limited waveform. \nproduct is the convolution of the time functions shown in Figs. 6.7(c) and \n(d). The resulting waveform is periodic, where a period is defined by the N \nsamples of the original function, and thus is a replica of the original function. \nThe Fourier transform of this periodic function is the sampled function il-\nlustrated in Fig. 6.7(e). \nFor this class of \nfunctions, if N is chosen equal to the number of \nsamples \nof \nthe finite-length function, then the only error is that introduced by aliasing. \nErrors introduced by aliasing are reduced by choosing the sample interval \nT sufficiently small. For this case, the discrete Fourier transform sample \nvalues agree (within a constant) reasonably well with samples of the con-\nSec. 6.4 \nRelationship Between the Discrete and Continuous Fourier Transform \n105 \nhit) \n.. \n111111111 1'1 ii\"lllllll \nlal \nÂ·1 \nl' \nL \n1 f \nT \nÂ·T \n2\" \nglkT) \nI . \nIbl \nleI \nleI \nIf I \n~ \nÂ·1 \n1 \n1'0 To \nIH If).'\\,(f). \nx(f)1 \nGin/NT! \nf \n.. \nFigure 6.8 Discrete Fourier transform of a general waveform. \n106 \nThe Discrete Fourier Transform \nChap. 6 \ntinuous Fourier transform. Unfortunately, there exist few applications of \ndiscrete Fourier transform for this class of functions. \nGeneral Periodic Waveforms \nFigure 6.7 can also be used to illustrate the relationship between the \ndiscrete and continuous Fourier transform for periodic functions that are \nnot band-limited. Assume that h(t), as illustrated in Fig. 6.7(a), is only one \nperiod of a periodic waveform. If this periodic waveform is sampled and \ntruncated at exactly the period, then the resulting waveform is identical to \nthe time waveform of Fig. 6.7(c). Instead of the continuous frequency func-\ntion, as illustrated in Fig. 6.7(c), the frequency transform is an infinite series \nof equidistant impulses separated by liTo, whose areas are given exactly by \nthe continuous frequency function. Because the frequency-sampling func-\ntion AI(f), as illustrated in Fig. 6.7(d), is an infinite series of equidistant \nimpulses separated by liTo, then the result is identical to those of \nFig. 6.7(e). \nAs before, the only error source is that of aliasing if the truncation function \nis chosen exactly equal to an integer multiple of the period. If the time-\ndomain truncation is not equal to a period, then results as described pre-\nviously are to be expected. \nGeneral Waveforms \nThe most important class of functions are tho!le that are neither time-\nlimited nor band-limited. An example of this class of functions is illustrated \nin Fig. 6.8(a). Sampling results in the aliased frequency function illustrated \nin Fig. 6.8(c). Time-domain truncation introduces rippling in the frequency \ndomain of Fig. 6.8(e). Frequency sampling results in the Fourier transform \npair illustrated in Fig. 6.8(g). The time-domain function of this pair is a \nperiodic function, where the period is defined by the N points of the original \nfunction after sampling and truncation. The frequency-domain function of \nthe pair is also a periodic function, where a period is defined by N points, \nwhose values differ from the original frequency function by the errors in-\ntroduced in aliasing and time-domain truncation. The aliasing error can be \nreduced to an acceptable level by decreasing the sample interval T. Pro-\ncedures for reducing time-domain truncation errors are addressed in Sec. \n9.2. \nSummary \nWe have shown that if care is exercised, then there exist many appli-\ncations where the discrete Fourier transform can be employed to derive \nresults essentially equivalent to the continuous Fourier transform. The most \nimportant concept to keep in mind is that the discrete Fourier transform \nSec. 6.5 \nDiscrete Fourier Transform Properties \n107 \nimplies periodicity in both the time and frequency domains. If one remem-\nbers that the N sample values of the time-domain function represent one \nsample of a periodic function, then application of the discrete Fourier trans-\nform should result in few surprises. \n6.5 DISCRETE FOURIER TRANSFORM PROPERTIES \nProperties established for the Fourier transform in Chapter 3 can be extended \nto the discrete Fourier transform because we have shown that the discrete \ntransform is a special case of the continous transform. Although we often \nuse the continuous equivalents in our problem-solving thought process, it \nis the discrete properties that form the theoretical basis for applications of \nthe FFT. We replace kT by k and n/NT by n for convenience of notation. \nLinearity \nIf x(k) and y(k) have discrete Fourier transforms X(n) and Y(n) , re-\nspectively, then \nx(k) + y(k) ~ \nX(n) + Y(n) \n(6.25) \nThe discrete Fourier transform pair of Eq. (6.25) follows directly from the \ndiscrete Fourier transform pair of Eq. (6.21). \nSymmetry \nIf h(k) and H(n) are a discrete Fourier transform pair, then \n~ \nH(k) ~ \nh( -n) \n(6.26) \nThe discrete Fourier transform pair of Eq. (6.26) is established by rewriting \nEq. (6.17): \n1 N-J \nh( -k) = -\n~ \nH(n)ei2-rrn(-k)/N \nN k=O \nand by interchanging the parameters k and n: \n1 N-J \nh(-n) = -\n~ \nH(k)e-j2-rrnklN \nN n=O \nTime Shifting \nIf h(k) is shifted by the integer i, then \nh(k -\ni) ~ \nH(n)e -j2-rrnilN \n(6.27) \n(6.28) \n(6.29) \n108 \nThe Discrete Fourier Transform \nChap. 6 \nTo verify Eq. (6.29), substitute r = k -\ni into the inverse discrete Fourier \ntransform: \n1 N-I \nher) = -\nL H(n)ej2'TTnrIN \nN n~O \n1 N-J \nh(k -\ni) = -\nL H(n)ej 2\"TTn(k-i)IN \nN n~O \nFrequency Shifting \n1 N-J \n= - L [H(n)e -j21TniIN]ej21TnkIN \nN n~O \n(6.30) \nIf H(n) is shifted by the integer i, then its inverse discrete Fourier \ntransform is multiplied by ej2'TTiklN \nh(k)ej2'TTikIN ~ \nH(n -\ni) \n(6.31) \nThis discrete Fourier transform pair is established by substituting r = \nn -\ni into the discrete Fourier transform \nN-I \nH(r) = L h(k)e -j2\"TTrkIN \nk~O \nN-J \nH(n -\ni) = L h(k)e-j2'TT(n-OkIN \nk~O \nN-I \nL [h(k)ej2'TTikIN]e -j21TnkIN \nk~O \nAlternate Inversion Formula \n(6.32) \nThe discrete inversion formula, Eq. (6.17), can also be written as \nh(k) =! L H * (n)e -j2'TTnkIN \n[ N-J \nJ* \nN \nk~O \n(6.33) \nwhere * implies conjugation. To prove Eq. (6.33), we simply perform the \nindicated conjugation. Let H(n) = R(n) + jI(n); hence, H * (n) = R(n) -\njI(n) and Eq. (6.33) becomes \nh(k) = ! \nL [R(n) - jI(n)]e -j2'TTnkIN \n[ N-I \nJ* \nN \nn~O \n1 \n-\n. \n2-rrnk.. \n2-rrnk \n[\nN \nJ \n[ \n(JJ* \n= N \nn~o \n[R(n) - jI(n)] cos (N) -Jsm N) \nSec. 6.5 \nDiscrete Fourier Transform Properties \n1 [N \n- ] \n(2Tink) \n. (27'ink) \n= N \nn~o \nRCn) cos N \n- l(n)sm N \n. \nN \n- J \nâ¢ (27'ink) \n(27'ink) ] * \n- J n~o \nR(n) sm N \n+ len) cos N \n1 [N-J \n(27'ink) \n. (27'ink) \n= N \nn~o \nR(n) cos N \n- len) sm N \n. N-J \n. (27'ink) \n(27'ink) ] \n+ \nJ n~o \nRCn) sm N \n+ lCn) cos N \n1 N \n- ] \n. \n[ \n(27'ink) \n.. (27'ink) ] \n= N n~o [RCn) + \nJlCn)] cos N \n+ \nJ sm N \n1 N-] \n= - L H(n)eJ27rnkIN \nN n=O \n109 \n(6.34) \nThe significance of the alternate inversion formula is that the discrete \ntransform, Eq. (6.34), can be used to compute both the Fourier transform \nand its inversion. Hence, one only needs to develop a single FFT computer \nprogram. \nEven Functions \nIf heCk) is an even function, then heCk) = he( - k) and the discrete \nFourier transform of heCk) is an even function and is real: \nN-J \n(2 k) \nheCk) ~ \nReCn) = n~o heCk) cos \n;; \nC6.35) \nTo verify Eq. (6.35), we simply manipulate the defining relationships: \nN-J \nHeCn) = L he(k)e-J27rnkIN \nk=O \nN-J \n(2 k) \nk~O heCk) cos \n;; \nN-J \n(2 k) \nk~O heCk) cos \n;; \n= Re(n) \nN-J \n(2 k) \n+ j k~O heCk) sin \n;; \n(6.36) \nThe imaginary summation is zero because the summation is over an even \nnumber of cycles of an odd function. Because heCk) cos(27'inkIN) = heCk) \n{cos[27'iC - n)kIN]}, then He(n) = He( - n) and the frequency function is even. \nThe inversion formula is proven similarly. Hence, if H(n) is given as a real \n110 \nThe Discrete Fourier Transform \nChap. 6 \nand even function, then its inverse discrete Fourier transform is an even \nfunction. \nOdd Functions \nIf hoek) = - hoe - k), then hoek) is an odd function and its discrete \nFourier transform is an odd and imaginary function: \nN-) \nHo(n) = L ho(k)e -j2-rrnkIN \nk=O \nN-) \n(2 k) \nk~O hoek) cos \n;:; \nN-) \n(2 k) \n- j k~O hoek) sin \n;:; \nâ¢ N~) h (k) . (2-rrnk) \n= -j ~ 0 \nsm-\nN \nk=O \n= j1o(n) \n(6.37) \nThe real summation is zero because summation is over an even number of \ncycles of an odd function. For H(n) given as an odd and imaginary function, \nthe proof that hoek) is an odd function is established similarly; therefore, \nN-) \n(2 k) \nhoek) ~ \nj1o(n) = - j k~O hoek) sin \n;:; \n(6.38) \nWaveform Decomposition \nTo decompose an arbitrary function h(k) into an even and an odd func-\ntion, we simply add and subtract the common function h( - k)/2. \nh(k) = h(k) + h(k) \n2 \n2 \n= [h~k) + h(; \nk) ] + [h~k) _ h(; \nk) ] \n(6.39) \n= heCk) + hoek) \nThe terms in brackets satisfy the definition of an even and an odd function, \nrespectively. Because h(k) is periodic with period N, then \nand \nh(-k) = heN -\nk) \nheCk) = h~k) + heN \n2- k) \nh (k) = h(k) _ heN -\nk) \n0\n22 \n(6.40) \n(6.41) \nSec. 6.5 \nDiscrete Fourier Transform Properties \n111 \nFor discrete periodic functions, Eq. (6.41) is the desired relationship \nfor decomposition. From Eqs. (6.35) and (6.38), the discrete Fourier trans-\nform of Eq. (6.39) is \nH(n) = R(n) + jl(n) = He(n) + Ho(n) \n(6.42) \nwhere \nHAn) = R(n) \nand \nHo(n) = jl(n) \n(6.43) \nComplex Time Functions \nIf \nh(k) = hr(k) + jhi(k), where hAk) and hiCk) are, respectively, the \nreal and imaginary parts ofthe complex time function h(k), then the discrete \nFourier transform becomes \nN-) \nH(n) = L [hr(k) + jhi(k)]e -j2-rrnkIN \nN - 1 \n(2-rrnk) \n. \n(2-rrnk) \nk~O hr(k) cos N \n+ hiCk) SIn N \n(6.44) \n.[ \nN-) \nâ¢ \n(2-rrnk) \n(2'ITnk) ] \n-\n] \nk~O hr(k) SIn N \n- hiCk) cos N \nThe first expression of Eq. (6.44) is R(n), the real part of the discrete trans-\nform, and the latter expression is len), the imaginary part of the discrete \ntransform. If h(k) is real, then h(k) = hr(k), and from Eq. (6.44), \nN-) \n(2 k) \nRAn) = k~O hr(k) cos \n;; \n(6.45) \nlo(n) = - j ~~ \nhr(k) sin C;;k) \n(6.46) \nNote that cos(2-rrnkIN) = cos( - 2-rrnkl \nN); thus, Re(n) = Re( - n), and Re(n) \nis an even function. Similarly, lo(n) = - lo( - n) and lo(n) is an odd function. \nFor h(k) purely imaginary, h(k) = jhi(k) and from Eq. (6.44), \nN-) \n(2 k) \nRoCn) = k~O hiCk) sin \n;; \n(6.47) \nN-) \n(2 k) \nleCn) = k~O hiCk) cos \n;; \n(6.48) \nFor h(k) imaginary, the real part of its transform is odd and the imaginary \npart of its transform is even. \n112 \nThe Discrete Fourier Transform \nChap. 6 \nTime-Convolution Theorem \nDiscrete convolution is defined by the summation (see Chapter 7): \nN-\\ \ny(k) = \n~ \nx(i)h(k -\ni) \n(6.49) \n;=0 \nwhere x(k), h(k), and y(k) are periodic functions with period N. \nAnalogous to Fourier transform theory, one of the most important \nproperties of \nthe discrete Fourier transform is exhibited by the discrete Four-\nier transform of Eq. (6.49). Discrete Fourier transformation yields the dis-\ncrete convolution theorem that is expressed as \nN-\\ \n~ \nx(i)h(k -\ni) ~ \nX(n)H(n) \n(6.50) \n;=0 \nTo establish this result, substitute Eq. (6.17) into the left-hand side of Eq. \n(6.50): \nN-\\ \nN-\\ 1 N-\\ \n.. \n~ \nx(i)h(k -\ni) = \n~ \nN ~ \nX(n)eJ2-rrnlIN \n;=0 \n;=0 \nn=O \n1 N-\\ \nX -\n~ \nH(m)ej2-rrm(k-;)IN \nN m=O \n1 N-\\N-\\ \n= -\n~ ~ \nX(n)H(m)ej2-rrmkIN \nN n=O m=O \nx ! [N~\\ ej2-rr;nINe - j 2-rr;mIN] \nN \n;=0 \n(6.51) \nThe bracketed term of Eq. (6.51) is simply the orthogonality relationship of \nEq. (6.19) and is equal to N if m = n; therefore, \nN-\\ \n1 N-\\ \n. \n~ \nx(i)h(k -\ni) = -\n~ \nX(n)H(n)eJ 2-rrnkIN \n(6.52) \n;=0 \nN n=O \nThus, the discrete Fourier transform of the convolution of \ntwo periodic sam-\npled functions with period N is equal to the product of the discrete Fourier \ntransform of the periodic functions. \nFrequency-Convolution Theorem \nConsider the frequency convolution: \nN-\\ \nyen) = ~ \nX(i)H(n -\ni) \n(6.53) \n;=0 \nSec. 6.5 \nDiscrete Fourier Transform Properties \n113 \nWe can establish the frequency-convolution theorem by substitution into \nEq. (6.53): \n[ N-] \n] \nX \nk~O h( \nk)e - j2-rrk(n - OIN \nN-] N-] \n(6.54) \n= \n~ ~ \nx(m)h(k)e-j2-rrknIN \nm=O k=O \nX [ N,,~=-O] \n] \nÂ£oJ e -j2-rrmiiN ej2-rrkiiN \nThe bracketed term of Eq. (6.54) is the orthogonality relationship of Eq. \n(6.19) and is equal to N if m = k; therefore, \nN-] \nN-] \n~ \nX(i)H(n -\ni) = N ~ \nx(k)h(k)e -j2-rrnkIN \n(6.55) \n;=0 \nk=O \nand the discrete transform pair is established: \n1 N-] \nx(k)h(k) ~ \nN ~ \nX(i)H(n -\ni) \n;=0 \n(6.56) \nCorrelation Theorem \nDiscrete correlation is defined as \nN-] \nz(k) = \n~ \nx(i)h(k + i) \n(6.57) \n;=0 \nwhere x(k), h(k), and z(k) are periodic functions with period N. \nThe transform pair \nN-] \n~ \nx(i)h(k + i) ~ \nX * (n)H(n) \n(6.58) \ni=O \nis termed the discrete correlation theorem. By means of the correlation \ntheorem, correlation can be determined equivalently in the transform do-\nmain. To verify this relationship, substitute the discrete Fourier transform \nFourier Transform \nx(t) + y(t) 0> X(f) + Y(f) \nH(t) 0> h(-f) \nh(t -\nto) 0> H(f)e-j2-rrfto \nh(tkj2-rrtfo 0> H(f -\nfo) \n[1-\n00\n00 H*(f)e -j2-rrft df r \nhe(t) 0> Re(f) \nho(t) 0> j1o(f) \nh(t) = he(t) + ho(t) \n[h~) + h(~t)] \n+ [h~) _ h(~t)] \ny(t) = 1-\n00\n00 x(T)h(t -\nT)dT \n= x(t) * h(t) \ny(t) * h(t) 0> Y(f)H(f) \nz(t) = 1-\n00\n00 x(T)h(t + T) dT \ny(t)h(t) 0> Y(f) * H(f) \n1-\n00\n00 h 2(t) dt = roo 1 \nH(f) 12 df \nTABLE 6.1 \nContinuous and Discrete Fourier Transform Properties \nProperty \nDiscrete Fourier Transform \n..... \n.... \n(3.2) Linearity \n(6.25) x(k) + y(k) 0> X(n) + Y(n) \n(3.6) Symmetry \n(6.26) ~(k) 0> h( - n) \n(3.21) Time shifting \n(6.29) h(k -\ni) 0> H(n)e -j2-rrniiN \n(3.23) Frequency shifting \n(6.31) \nh(k)~2-rrkiIN 0> H(n -\ni) \n(3.25) Alternate inversion formula \n(6.33) [1. Nil H * (n)e-j2-rrknIN r \nN n~O \n(3.27) Even functions \n(6.35) he(k) 0> Re(n) \n(3.32) Odd functions \n(6.38) ho(k) 0> j1o(n) \n(3.33) Decomposition \n(6.39) h(k) = he(k) + ho(k) \n[h~k) + h(N \n2- k)] \n--I \n:::T \n+ [hik) _ h(N \n2- k)] \nCD \n0 \nCii\" \n0 \nN-I \n(il \n(4.1) Convolution \n(6.49) y(k) = ~ \nx(i)h(k -\ni) = x(k) * h(k) \nCD \n\"T1 \ni=O \n0 \nc: \n~ \n(4.14) Time convolution theorem \n(6.50) y(k) * h(k) 0> Y(n)H(n) \n~. \n--I \nN-I \niil \n:::l \n(4.21) Correlation \n(6.57) y(k) = ~ \nx(i)h(k + I) \nCJ> \n0' \ni=O \n3 \n(4.20) Frequency convolution theorem (6.56) y(k)h(k) 0> 1. Y(n) * H(n) \nN \n() \n:::T \nParseval's Theorem \nN-I \n1 N-I \nII> \n\"? \n~ \nh 2(k) = -\n~ \n1 H(n) 12 \nen \nk~O \nN n~O \nChap. 6 \nProblems \ninto the left-hand side of Eq. (6.58): \nN-J \n~ \nx(i)h( \nk + i) \ni=O \nN-J [1 N-J \n] \n~ _ \n~ \nX(n)ej2TrinIN \ni=O \nN n=O \n1 N-J \nX -\n~ \nH(m)ej2Trm(k+i)IN \nN m=O \nN-J [1 N-J \nJ* \n~ -\n~ \nX* (n)e-j2TrinIN \ni=O \nN n=O \n[ IN-I \n] \nX \n-\n~ \nH(m)ej2Trm(k+i)IN \nN m=O \n115 \n(6.59) \nwhere the alternate inversion formula, Eq. (6.33), has been utilized to in-\ntroduce the conjugate of X(n). Note that the second conjugation indicated \nin Eq. (6.33) can be omitted if only real functions are considered. For this \ncase, Eq. (6.59) can be rewritten as: \nN-I \n~ \nx(i)h(k + i) \ni=O \n(6.60) \nN-IN-I \n[IN-I \n] \n= ~ \nn~o m~o \nX * (n)H(m)ej2TrmkIN N i~O e -j2TrinlN ~2Trim1N \nFrom the orthogonality relationship of Eq. (6.19), the bracketed term is equal \nto N if n = m. Hence, Eq. (6.60) becomes \nN-I \n1 N-I \n~ \nx(i)h(k + i) = -\n~ \nX * (n)H(n)eJ2TrnkIN \n(6.61) \ni=O \nN n=O \nSummary Table of Discrete Fourier Transform \nProperties \nFor future reference, the discrete Fourier transform properties are sum-\nmarized in Table 6.1. The continuous Fourier transform properties are also \ntabled for purposes of comparison. Appropriate equation numbers are listed \nin order that one can easily locate the continuous or discrete development \nfor each property. \nPROBLEMS \n6.1. Repeat the graphical development of Fig. 6.1 for the following functions: \n(a) h(t) = I \ntie \n- alII \n(b) h(t) = 1 - I \nt I \n= 0 \n(c) h(t) = cos t \nI \nt I \n:5 1 \nI \nt I> 1 \n116 \nThe Discrete Fourier Transform \nChap. 6 \n6.2. Retrace the development of the discrete Fourier transform [Eqs. (6.1) through \n(6.16)]. Write all steps of the derivation in detail. \n6.3. Repeat the graphical derivation of Fig. 6.3 for h(t) = sin(2'lTfot). Show the \neffect of setting the truncation interval unequal to the period. What is the result \nof setting the truncation interval equal to two periods? \n6.4. Consider Fig. 6.7. Assume that h(t)Ao(t) is represented by N nonzero samples. \nWhat is the effect of truncating h(t)Ao(t) so that only 3NI4 nonzero samples \nare considered? What is the effect of truncating h(t)Ao(t) so that the N nonzero \nsamples and NI4 zero samples are considered? \n6.5. Repeat the graphical derivation of Fig. 6.7 for h(t) \nL \ne-alt-nTol. What \nn= -00 \nare the error sources? \n6.6. To establish the concept of rippling, perform the following graphical \nconvolutions: \n(a) An impulse with (sin I)lt. \n(b) A narrow pulse with (sin t)ft. \n(c) A wide pulse with (sin I)lt. \n(d) A single triangular waveform with (sin t)lt. \n6.7. Write several terms of Eq. (6.19) to establish the orthogonality relationship. \n6.8. The truncation interval is termed the record length. In terms of the record \nlength, write an equation defining the resolution or frequency spacing of the \nfrequency-domain samples of the discrete Fourier transform. \n6.9. Comment on the following: The discrete Fourier transform is analogous to a \nbank of band-pass filters. \nLet x(k) and y(k) be discrete periodic functions: \nx(k) ~ {l \nk = 0,4 \nk = 1,2,3 \nk = 5, 6, 7 \nx(k + 8r) = x(k) \nr = 0, Â± 1, Â±2, ... \ny(k) = x(k) \ny(k + 8r) = y(k) \nr = 0, Â± 1, Â±2, ... \n6.10. Compute X(n) and Y(n). Add these results to determine [X(n) + Y(n)]. De-\ntermine z(k) = x(k) + y(k). Compute Z(n). Discuss your results in terms of \nthe linearity property. \n6.11. Demonstrate the symmetry property of Eq. (6.26) for x(k). \n6.12. Compute the discrete Fourier transform of x(k -\n3). Compare results with \nthose obtained from the time-shifting relationship of Eq. (6.29). \n6.13. Compute the inverse discrete Fourier transform of \nX(n -\n1). Repeat this com-\nputation by applying the frequency-shifting theorem ofEq. (6.31) and compare \nresults. \n6.14. Compute the inverse discrete Fourier transform of X(n) using the alternate \ninversion formula of Eq. (6.33). \n6.15. Compute the discrete Fourier transform of x(k -\n2). Investigate the even-odd \nChap. 6 \nReferences \n117 \nrelationship of x(k -\n2) and the real-imaginary relationship of its discrete \ntransform. \n6.16. Let z(k) = x(k) -\ny(k - 4). Compute the discrete Fourier transform of z(k). \n6.17. Let z(k) = y(k) + y(k -\n2) -\nx(k -\n4). Decompose z(k) into even and odd \nfunctions both analytically and graphically. Demonstrate Eq. (6.42) with z(k). \n6.18. Demonstrate the frequency convolution theorem using x(k) and y(k). \n6.19. Demonstrate the discrete correlation theorem using x(k) and y(k). \nREFERENCES \n1. COOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH. \"The Finite Fourier Trans-\nform.\" IEEE Trans. on Audio and Electroacoustics (June 1969), Vol. AU-17, \nNo.2, pp. 77-85. \n2. BERGLAND, G. D. \"A Guided Tour of the Fast Fourier Transform.\" IEEE Spec-\ntrum (July 1969), Vol. 6, No.7, pp. 41-52. \n7 \nDISCRETE CONVOLUTION \nAND CORRELATION \nPossibly the most important discrete Fourier transform properties are those \nof convolution and correlation. This follows because the importance of the \nfast Fourier transform is primarily a result of its efficiency in computing \ndiscrete convolution or correlation. In this chapter, we examine, analytically \nand graphically, the discrete convolution and correlation equations. The \nrelationship between discrete and continuous convolution is also explored \nin detail. \n7.1 DISCRETE CONVOLUTION \nDiscrete convolution is defined by the summation: \nN-\\ \ny(kT) = L x(iT)h[(k - on \n;=0 \nwhere both x(kT) and h(kT) are periodic functions with period N, \nx(kT) = x[(k + rN)T] \nh(kT) = h[(k + rN)T] \nr = 0, Â± 1, Â± 2, .. . \nr = 0, Â± 1, Â± 2, .. . \n(7.1) \n(7.2) \nFor convenience of notation, discrete convolution is normally written as \ny(kT) = x(kT) * \nh(kT) \n(7.3) \nTo examine the discrete convolution equation, consider the illustra-\ntions of Fig. 7.1. Both functions x(kT) and h(kT) are periodic with period \n118 \nSec. 7.2 \nGraphical Interpretation of Discrete Convolution \nx(kT) \no T 2T 3T 4T \n(a) \nkT \nhlkT) \no T \n(b) \nFigure 7.1 \nExample sampled waveforms to be convolved discretely. \n119 \nkT \nN = 4. From Eq. (7.1), functions x(iT) and h[(k - i)T] are required. Function \nh( - iT) is the image of hUT) about the ordinate axis, as illustrated in Fig. \n7.2(a); function h[(k -\ni)T] is simply the function h( - iT) shifted by the \namount kT. Figure 7.2(b) illustrates h[(k - on for the shift 2T. Equation \n(7.1) is evaluated for each kT shift by performing the required multiplications \nand additions. \nhI-iT) \nh[(2-i)T) \nT 2T 3T 4T \niT \n(I) \nIb) \nFigure 7.2 Graphical description of discrete convolution shifting operation. \n7.2 GRAPHICAL INTERPRETATION OF DISCRETE \nCONVOLUTION \niT \nThe discrete convolution process is illustrated graphically in Fig. 7.3. Sample \nvalues of x(kT) and h(kT) are denoted by dots and crosses, respectively. \nFigure 7.3(a) illustrates the desired computation for k = O. The value of \neach dot is multiplied by the value of the cross that occurs at the same \nabscissa value; these products are summed over the N = 4 discrete values \nindicated. Computation of Eq. (7.1) is graphically evaluated for k = 1 in \nFig. 7.3(b); multiplication and summation is over the N points indicated. \nFigures 7.3(c) and (d) illustrate the convolution computation for k = 2 and \nk = 3, respectively. Note that for k = 4 [Fig. 7.3(e)], the terms multiplied \nand summed are identical to those of Fig. 7.3(a). This is expected because \n120 \nDiscrete Convolution and Correlation \nChap. 7 \nh(Â·iTI \niT \niT \niT \niT \niT \nÂ·3T Â·2T Â·T \nT 2T 3T 4T 5T 6T \nkT \n(hI \nFigure 7.3 Graphical illustration of discrete convolution. \nSec. 7.3 \nRelationship Between Discrete and Continuous Convolution \n121 \nboth x(kT) and h(kT) are periodic with a period of four terms. Therefore, \ny(kD = y[(k + rN)T] \nr = 0, Â± 1, Â± 2, ... \n(7.4) \nSteps for graphically computing the discrete convolution differ from \nthose of continuous convolution only in that integration is replaced by sum-\nmation. For discrete convolution, these steps are (1) folding, (2) displace-\nment or shifting, (3) multiplication, and (4) summation. As in the convolution \nof continuous functions, either the sequence x(kT) or h(kD can be selected \nfor displacement. Equation (7.1) can be written equivalently as \nN-] \ny(kT) = 2: x[(k -\ni)T]hUD \ni~O \n7.3 RELATIONSHIP BETWEEN DISCRETE \nAND CONTINUOUS CONVOLUTION \n(7.5) \nIf \nwe only consider periodic functions represented by equally spaced impulse \nfunctions, discrete convolution relates identically to its continuous equiv-\nalent. This follows because, as we show in Appendix A (Eq. A.14), contin-\nuous convolution is well-defined for impulse functions. \nThe most important application of discrete convolution is not to sam-\npled periodic functions but rather to approximate the continuous convolu-\ntions of general waveforms. For this reason, we will now explore in detail \nthe relationship between discrete and continuous convolution. \nDiscrete Convolution of Finite-Duration Waveforms \nConsider the functions x(t) and h(t), as illustrated in Fig. 7.4(a). We \nwish to convolve these two functions both continuously and discretely and \nto compare these results. Continuous convolution yet) of the two functions \nis also shown in Fig. 7.4(a). To evaluate the discrete convolution, we sample \nboth x(t) and h(t) with sample interval T and we assume that both sample \nfunctions are periodic with period N. As illustrated in Fig. 7.4(b), the period \nhas been chosen as N = 9 and both x(kD and h(kD are represented by P \n= Q = 6 samples; the remaining samples defining a period are set to zero. \nFigure 7.4(b) also illustrates the discrete convolution y(kD for the period N \n= 9; for this choice of N, the discrete convolution is a very poor approxi-\nmation of the continuous case because the periodicity constraint results in \nan overlap of the desired periodic output. That is, we did not choose the \nperiod sufficiently large so that the convolution result of one period would \nnot interfere or overlap the convolution result of the succeeding period. It \nis obvious that if we wish the discrete convolution to approximate continuous \n122 \nfL \n1 \nt \n. \nlr:~~) .... \n~ \n--l P=6 1-1 \nkT \n!-N=9-j \nx(kT) \n1 .... \n---l P=6 I--\nf--N=11 \nx(kT) \n1 .... \nkT \nkT \nL\n(kT) \n1 ............... \n.. \n............ \n.. \n. \n. \n. \n~N~ \nkT \nDiscrete Convolution and Correlation \nChap. 7 \nt \nh(l) \n~ \n1 \nI \n(a) \nh(kT) \n% â¢â¢â¢â¢ \n-l Q=6 t-\n!-N=9 \n(b) \nkT \nL \n--l 0=6 f--\n1 \nkT \n!--N= \n11-----l \n(e) \n,r\" \n-l \nQ=6 I-\nI r,: \n~N=15--l \n(d) \nh(kT) \n% --\nkT \ntV(I) \n~ \n2 \nI \n2 \nâ¢â¢ \nâ¢ â¢ \n. \n. . . . \n.. .. \n. \nrn \n. \n. \n.. \n. . \n.. \n!-N=9-! \nkT \nV(kT) \n2 \nâ¢â¢ \nN=II--j \nkT \nr,m \n2 \nâ¢â¢ \n. \n. \n.. \n!---N= \n15-----l \nkT \nV(kT) \n% \n.. \n. \n....................................................... \n. \nI- N \nkT \nFigure 7.4 Relationship between discrete and continuous convolution: finite-du-\nration waveforms. \nconvolution, then it is necessary that the period be chosen so that there is \nno overlap. \nChoose the period according to the relationship \nN=P+Q-l \nThis situation is illustrated in Fig. 7.4(c), where N \n(7.6) \nP + Q -\n1 = 11. \nSec. 7.3 \nRelationship Between Discrete and Continuous Convolution \n123 \nNote that for this choice of \nN there is no overlap in the resulting convolution. \nEquation (7.6) is based on the fact that the convolution of a function rep-\nresented by P samples and a function represented by Q \nsamples is a function \ndescribed by P + Q -\n1 samples. \nThere is no advantage in choosing N > P + Q -\n1; as shown in Fig. \n7.4(d), for N = 15, the nonzero values of the discrete convolution are iden-\ntical to those of Fig. 7.4(c). As long as N is chosen according to Eq. (7.6), \ndiscrete convolution results in a periodic function, where each period ap-\nproximates the continuous convolution results. \nFigure 7.4( \nc) illustrates the fact that discrete convolution results are \nscaled differently than that of continuous convolution. This scaling constant \nis T; modifying the discrete convolution Eq. (7.1), we obtain \nN-\\ \ny(kn = T L x(inh[(k -\ni)T] \n(7.7) \n;=0 \nThe relationship of Eq. (7.7) is simply the continuous convolution integral \nfor time-limited functions evaluated by rectangular integration. Thus, for \nfinite-length time functions, discrete convolution approximates continuous \nconvolution within the error introduced by rectangular integration. As il-\nlustrated in Fig. 7.4(e), if the sample interval T is made sufficiently small, \nthen the error introduced by the discrete convolution Eq. (7.7) is negligible. \nExample 7.1 Circular Convolution \nDiscrete convolution yields periodic results because of the periodicity of the func-\ntions being convolved. This periodicity gives rise to what is commonly called circular \nconvolution. Figure 7.5 illustrates this concept. \nIn Figure 7.5(a), we show two example discrete periodic waveforms to be \nconvolved. For the shift k = 2, Fig. 7.5(b) illustrates the appropriate folding and \nshifting operations. Multiplication and addition over the N = 8 points of the period \nyield the convolution results for k = 2. An alternate way of displaying the discrete \nconvolution of Fig. 7.5(b) for shift k = 2 is shown in Fig. 7.5(c). The rings represent \none period of the two periodic functions; the inner ring is h(in and is the function \nbeing shifted. As illustrated, the function is set for a shift of k = 2. The outer ring \ncorresponds to the function x(iT). Appropriate values to be multiplied are adjacent \nto each other. These mUltiplied results are then summed around the circle (i.e., over \none period). \nThe inner ring is turned for each shift of k. As the ring is turned, it returns to \nits original position every eight shifts. Hence, the same values will be computed. \nThis corresponds to the periodic convolution results discussed previously. Figure \n7.5(c) can also be used to illustrate the problem of overlap. As the inner ring turns, \nthere must be a sufficient number of \nzero values in the outer ring so that a convolution \nvalue is not computed, which is a function of both ends of the data used to form the \nouter ring. If sufficient zeros are appended to the nonzero sample values of each \nfunction to be convolved, then the finite-duration convolution result does not overlap \nwith the following period. \n124 \n:r~Â· Â· \n1. \n10 ~ 234567 \ni \n\"'iT) \nk~2 \nh((k-i)l1 \n4 \nâ¢ \n3 \n0 â¢ \n2 \nâ¢ \n0 \n0 \n0 \no 1 \n2 \n3 \n4 \n5 \n6 \n7 \n(b) \nDiscrete Convolution and Correlation \nChap. 7 \n(a) \n4 \n3 \n2 \nhliT) \no \no \n0 \no \no 1 234 5 \n6 \n7 \nleI \nFigure 7.5 Graphical illustration of circular convolution. \nDisr,rete Convolution of an Infinite- and a Finite-\nDuration Waveform \nThe previous example considered the case for which both x(kD and \nh(kT) were of finite duration. Another case of interest is that where only \none of \nthe time functions to be convolved is finite. To explore the relationship \nof the discrete and continuous convolution for this case, consider the illus-\ntrations of Fig. 7.6. As illustrated in Fig. 7.6(a), function h(t) is assumed to \nbe of finite duration and x(t) of infinite duration; convolution of these two \nfunctions is shown in Fig. 7.6(b). Because the discrete convolution requires \nthat both the sampled functions x(kD and h(kD be periodic, we obtain the \nillustrations of Fig. 7.6(c); period N has been chosen [Figs. 7.6(a) and (c)]. \nFor x(kT) infinite in duration, the imposed periodicity introduces what is \nknown as an end effect. \nCompare the discrete convolution of Fig. 7.6(d) and the continuous \nconvolution [Fig. 7.6(b)]. As illustrated, the two results agree reasonably \nwell, with the exception of the first Q -\n1 samples of the discrete convo-\nlution. To establish this fact more clearly, consider the illustrations of Fig. \nSec. 7.3 \nRelationship Between Discrete and Continuous Convolution \nhIt) \n1 \ni \n23456 \nt \nI---NT----l \n(a) \n-~-\n-~ \n(b) \n1.----N----t-I \nkT \nkT \n(el \nr'\" \n1 : ............................................................. \n/ ........................ \n.. \n,.. \n~~ kT \n(d) \nFigure 7.6 Relationship between discrete and continuous convolution: finite- and \ninfinite-duration wavefonns. \n125 \n7.7. We show only one period of x(in and h[(5 -\ni)T]. To compute the \ndiscrete convolution, Eq. (7.1), for this shift, we multiply those samples of \nx(in and h[(5 -\ni)T] that occur at the same time [Fig. 7.7(a)] and add. The \nconvolution result is a function of x(in at both ends of the period. Such a \ncondition obviously has no meaningful interpretation in terms of the desired \ncontinuous convolution. Similar results are obtained for each shift value until \nthe Q points of h(iT) are shifted by Q - 1, that is, the end effect exists until \nshift k = Q -\n1. \nNote that the end effect does not occur at the right end of the N sample \nvalues; functions hUn for the shift k = N -\n1 (therefore maximum shift) \n126 \nDiscrete Convolution and Correlation \nChap. 7 \nxliT, \nIÂ· \nN \niT \nr\"~\"l \n-\nIÂ· \nN \n-I \niT \n1-' \n.. \nI â¢ \nN -----I \na 1 \niT \nr[\"Â·'H~1 \nIÂ· \nN -----i-I \n.. \niT \nIb, \nFigure 7.7 Illustration of the end effect. \nand x(iT) are illustrated in Fig. 7.7(b). Multiplication ofthose values of \nx(iT) \nand h[(N -\n1 - ;)11 that occur at the same time and subsequent addition \nyield the desired convolution; the result is only a function of the correct \nvalues of x(iT). \nIf \nthe sample interval T is chosen sufficiently small, then the discrete \nconvolution for this example class of functions closely approximate the con-\ntinuous convolution except for the end effect. \nSummary \nWe have emphasized the point that discrete convolution is defined only \nfor periodic functions. However, as illustrated graphically, the implications \nof this requirement are negligible if at least one of the functions to be con-\nSec. 7.4 \nGraphical Interpretation of Discrete Correlation \n127 \nvolved is of finite duration. For this case, discrete convolution is approxi-\nmately equivalent to continuous convolution where the differences in the \ntwo methods are due to rectangular integration and to the end effect. \nIn general, it is impossible to discretely convolve two functions of \ninfinite duration. \nThe convolution waveform illustrated could have been computed \nequivalently by means of the convolution theorem. Recall that the discrete \nconvolution of Eq. (7.1) was defined in such a manner that those functions \nbeing convolved were assumed to be periodic. The underlying reason for \nthis assumption is to enable the discrete convolution theorem, Eq. (6.50), \nto hold. If \nwe compute the discrete Fourier transform of each of the periodic \nsequences x(kD and h(kD, mUltiply the resulting transforms, and then com-\npute the inverse discrete Fourier transform of this product, we obtain iden-\ntical results to those illustrated. As is discussed in Chapter 10, it is normally \nfaster computationally to use the discrete Fourier transform to compute the \ndiscrete convolution if the FFT is employed. \n7.4 GRAPHICAL INTERPRETATION OF DISCRETE \nCORRELATION \nDiscrete correlation is defined as \nN-J \nz(kD = L x(iDh[(k + i)T] \n;=0 \nwhere x(kT), h(kD, and z(kD are periodic functions. \nz(kT) = z[(k + rN)T] \nx(kD = x[(k + rN)T] \nh(kT) = h[(k + rN)T] \nr = 0, Â±1, Â±2, .. . \nr = 0, Â±1, Â±2, .. . \nr = 0, Â±1, Â±2, .. . \n(7.8) \n(7.9) \nAs in the continuous case, discrete correlation differs from convolution \nin that there is no folding operation. Hence, the remaining rules for dis-\nplacement, multiplication, and summation are performed exactly as for the \ncase of discrete convolution. \nTo illustrate the process of discrete correlation or lagged products, as \nit sometimes is referred, consider Fig. 7.8. The discrete functions to be \ncorrelated are shown in Fig. 7.8(a). According to the rules for correlation, \nwe shift, multiply, and sum, as illustrated in Figs. 7.8(b), (c), and (d), re-\nspectively. Compare with the results of Ex. 4.8. In Chapter to, we discuss \nthe application of the FFT for efficient computation of Eq. (7.8). \n128 \nDiscrete Convolution and Correlation \nChap. 7 \nx(kT) \nh(kTl \nk \nk \n(a) \nh[(i+k)T) \nDISPLACEMENT====~> \n-1 \ni 1--\nk \n(b) \nx(kTlh((i+k)T] \nMULTIPLICATION \n> \n(e) \nk \n--\n~ \nSummed Value \n'% \nSUMMATION====~> \n(d) \nFigure 7.8 Graphical illustration of discrete correlation. \nChap. 7 \nProblems \n7.1. Let \nand \nPROBLEMS \nx(kD = e- kT \n= 0 \n= x[(k + rN)l1 \nh(kT) = 1 \n=0 \n= h[(k + rN)l1 \nk = 0, 1,2,3 \nk = 4,5, ... ,N \nr = 0, Â± 1, Â± 2, ... \nk = 0, 1, 2 \nk = 3,4, ... ,N \nr = 0, Â± 1, Â± 2, ... \n129 \nWith T = 1, graphically and analytically determine x(kD * h(kD. Choose N \nless than, equal to, and greater than Eq. (7.6). \n7.2. Consider the continuous functions x(t) and h(t), as illustrated in Fig. 4.14(a). \nSample both functions with sample interval T = To/4 and assume both sample \nfunctions are periodic with period N. Choose N according to relationship of \nEq. (7.6). Determine x(kD * h(kD both analytically and graphically. Inves-\ntigate the results of an incorrect choice of N. Compare results with continuous \nconvolution results. \n7.3. Repeat Problem 7.2 for Figs. 4.14(b) and (c). \n7.4. Refer to Fig. 7.6. Let x(t) be defined as illustrated in Fig. 7.6(a). Function h(l) \nis given as \n(a) h(t) = 8(t) \n(b) h(t) = 8(1) + 8(1 - D \n(c) h(t) = 0 \n1 < 0 \nI \n= I \n0<1<2 \n= 0 \nI \n2<1<1 \n3 \n= I \n1<1<2 \n= 0 \n3 \nI> -\n2 \nFollowing Fig. 7.6, graphically determine the discrete convolution in each case. \nCompare the discrete and continuous convolution in each case. Investigate \nthe end effect in each case. \n7.5. It is desired to discretely convolve a finite-duration and an infinite-duration \nwaveform. Assume that a hardware device is to be used that is limited in \ncapacity to N sample values of each function. Describe a procedure that allows \none to perform successive N-point discrete convolutions and combine the two \nto eliminate the end effect. Demonstrate your concept by repeating the illus-\ntrations of Fig. 7.6 for the case NT = 1.5. Successively apply the developed \ntechnique to determine the discrete convolution y(kT) for 0 ~ \nkT ~ \n3. \n130 \nDiscrete Convolution and Correlation \n7.6. Derive the discrete convolution theorem for the following: \nN-l \n(a) ~ \nh(iDx[(k -\nOT] \ni=O \nN-l \n(b) ~ \nh(;nh[(k -\nOT] \n;=0 \nChap. 7 \n7.7. Let x(kT) and h(kn be defined by Problem 7.1. Determine the discrete cor-\nrelation of Eq. (7.11) both analytically and graphically. What are the contraints \non N? \n7.8. Repeat Problem 7.2 for discrete correlation. \n7.9. Repeat Problem 7.3 for discrete correlation. \n7.10. Repeat Problem 7.4 for discrete correlation. \nREFERENCES \n1. COOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH. \"The Finite Fourier Trans-\nform.\" IEEE Trans. on Audio and Electroacoustics (June 1969), Vol. AUI7, No. \n2, pp. 77-85. \n2. COOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH. \"The Fast Fourier Transform \nand its Applications.\" IEEE Trans. on Education (March 1969), Vol. 12, pp. \n27-34. \n3. BERGLAND, G. D. \"A Guided Tour of the Fast Fourier Transform.\" IEEE Spec-\ntrum (1969), Vol. 6, No.7, pp. 41-52. \n8 \nTHE FAST FOURIER \nTRANSFORM (FFT) \nInterpretation of fast Fourier transform results does not require a well-\ngrounded education in the algorithm itself, but rather a thorough under-\nstanding of the discrete Fourier transform. This follows from the fact that \nthe FFT is simply an algorithm (i.e., a particular method of performing a \nseries of computations) that can compute the discrete Fourier transform \nmuch more rapidly than other available algorithms. For this reason, our \ndiscussion of the FFT addresses only the computational aspect of the \nalgorithm. \nA simple matrix-factoring example is used to intuitively justify the FFT \nalgorithm. The factored matrices are alternatively represented by signal flow \ngraphs. From these graphs, we construct the logic of an FFT computer \nprogram. Theoretical developments of various forms of the FFT algorithm \nare then presented. \n8.1 MATRIX FORMULATION \nConsider the discrete Fourier transform, Eq. (6.16): \nN-1 \nX(n) = L xo(k)e -j27fnkIN \nk=O \nn = 0, 1, ... ,N -\n(8.1) \nwhere we have replaced kT by k and nl \nNT by n for convenience of notation. \nWe note that Eq. (8.1) describes the computation of N equations. For ex-\n131 \n132 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nample, if N = 4 and if we let \nW = e-j2-rrIN \n(8.2) \nthen Eq. (8.1) can be written as \nX(O) = xo(O)WO + xo(l)WO + xo(2)WO + xo(3)WO \nX(l) = xo(O)WO + xo(1)W I + xo(2)W2 + xo(3)W3 \n(8.3) \nX(2) = xo(O)WO + xo(l)W2 + xo(2)W4 + xo(3)W6 \nX(3) = xo(O)WO + xo(l)W3 + xo(2)W6 + xo(3)W9 \nEquations (8.3) can be more easily represented in matrix form: \n[~m] \n= [E: ~ \nX(3) \nWO \nW3 \n(8.4) \nor more compactly as \n(8.5) \nWe will denote a matrix by boldface italic type. \nExamination of Eq. (8.4) reveals that since Wand possibly xo(k) are \ncomplex, then N 2 complex multiplications and (N)(N -\n1) complex addi-\ntions are necessary to perform the required matrix computation. The FFT \nowes its success to the fact that the algorithm reduces the number of mul-\ntiplications and additions required in the computation of Eq. (8.4). We will \nnow discuss, on an intuitive level, how this reduction is accomplished. A \nproof of the FFT algorithm is delayed until Sec. 8.9. \n8.2 INTUITIVE DEVELOPMENT \nTo illustrate the FFT algorithm, it is convenient to choose the number of \nsample points of \nxo(k) according to the relation N = 2'Y, where 'Y is an integer. \nLater developments remove this restriction. Recall that Eq. (8.4) results from \nthe choice of N = 4 = 2'Y = 22; therefore, we can apply the FFT to the \ncomputation of Eq. (8.4). \nThe first step in developing the FFT algorithm for this example is to \nrewrite Eq. (8.4) as \n[\nX(O)] \n[1 \nX(1) \n_ \n1 \nX(2) \n-\n1 \nX(3) \n1 \n(8.6) \nSec. 8.2 \nIntuitive Development \n133 \nMatrix Eq. (8.6) was derived from Eq. (8.4) by using the relationship W nk \n= wnk mod(N). Recall that [nk mod(N)] is the remainder upon division of nk \nby N; hence if N = 4, n = 2, and k = 3, then \n(8.7) \nbecause \nw\nnk = W6 = exp [ ( - ~21T)(6) \n] = exp[ - j31T] \n(8.8) \n= exp[ - j1T] = exp [ ( - ~21T)(2) \n] = W 2 = W nk mod(N) \nThe second step in the development is to factor the square matrix in \nEq. (8.6) as follows: \n[\nX(O)] \n[1 \nX(2) \n_ \n1 \nX(1) \n-\n0 \nX(3) \n0 \nWO 0 \nW2 \n0 \no \n1 \no \n1 \no ] [1 0 W\nO 0] [XO(O)] \no \n0 1 \n0 \nWO \nxo(1) \nWI \n1 0 \nW2 \n0 \nxo(2) \nW 3 \n0 \n1 \n0 \nW 2 \nxo(3) \n(8.9) \nThe method of factorization is based on the theory of the FFT algorithm \ndeveloped in Sec. 8.9. For the present, it suffices to show that multiplication \nof the two square matrices of Eq. (8.9) yields the square matrix of Eq. (8.6) \nwith the exception that rows 1 and 2 have been interchanged (the rows are \nnumbered 0, 1, 2, and 3). Note that this interchange has been taken into \naccount in Eq. (8.9) by rewriting the column vector X(n); let the row-inter-\nchanged vector be denoted by \nX(n) = [~m] \nX(3) \n(8.10) \nRepeating, the reader should verify that Eq. (8.9) yields Eq. (8.6) with the \ninterchanged rows as noted. This factorization is the key to the efficiency \nof the FFT algorithm. \nHaving accepted the fact that Eq. (8.9) is correct, although the results \nare scrambled, one should then examine the number of multiplications re-\nquired to compute the equation. First, let \n(8.11) \nThat is, column vector xl(k) is equal to the product of the two matrices on \nthe right in Eq. (8.9). \n134 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nElement x I (0) is computed by one complex multiplication and one com-\nplex addition (WO is not reduced to unity in order to develop a generalized \nresult). \n(8.12) \nElement x I (1) is also determined by one complex multiplication and \naddition. Only one complex addition is required to compute x I (2). This fol-\nlows because WO = \n- W2; hence, \nX I (2) = xo(O) + W 2 xo(2) \n= xo(O) -\nWOxo(2) \n(8.13) \nwhere the complex multiplication WO xo(2) has already been computed in the \ndetermination of \nXI (0) [Eq. (8.12)]. By the same reasoning, XI (3) is computed \nby only one complex addition and no multiplications. The intermediate vec-\ntor x1(k) is then determined by four complex additions and two complex \nmultiplications. \nLet us continue by completing the computation of Eq. (8.9) \n[~~~~] -\n;~~~~ \nX(1) \n-\nx2(2) \nX(3) \nx2(3) \n1 \n1 \no \no \nWO 0 \nW 2 \n0 \no \n1 \no \n1 \no \no \nXI(O) \nXI(l) \nxl(2) \nxl(3) \n(8.14) \nTerm X2(0) is determined by one complex multiplication and addition: \n(8.15) \nElement X2(1) is computed by one addition because WO = - W2. By similar \nreasoning, x2(2) is determined by one complex multiplication and addition, \nand x2(3) by only one addition. \nComputation of X(n) by means of Eq. (8.9) requires a total of four \ncomplex multiplications and eight complex additions. Computation of X(n) \nby (8.4) requires 16 complex multiplications and 12 complex additions. Note \nthat the matrix-factorization process introduces zeros into the factored ma-\ntrices and, as a result, reduces the required number of multiplications. For \nthis example, the matrix-factorization process reduces the number of mul-\ntiplications by a factor oftwo. Because computation time is largely governed \nby the required number of multiplications, we see the reason for the effi-\nciency of the FFT algorithm. \nFor N = 2\"\"1, the FFT algorithm is then simply a procedure for factoring \nan N x N matrix into -y matrices (each N x N), such that each of the \nfactored matrices has the special property of minimizing the number of com-\nplex multiplications and additions. If \nwe extend the results of the previous \nexample, we note that the FFT requires N-y/2 = 4 complex multiplications \nand N-y = 8 complex additions, whereas the direct method [Eq. (8.4)] re-\nSec. 8.2 \nIntuitive Development \n135 \nquires N 2 complex multiplications and N(N -\n1) complex additions. If we \nassume that computing time is proportional to the number of multiplications, \nthen the approximate ratio of direct to FFT computing time is given by \nN 2 \n2N \nNy/2 = -:; \n(8.16) \nwhich for N = 1024 = 210 is a computational reduction of more than 200 \nto 1. Figure 8.1 illustrates the relationship between the number of mUltipli-\ncations required using the FFT algorithm compared with the number of \nmUltiplications using the direct method. \nThe matrix-factoring procedure does introduce one discrepancy. Recall \nthat the computation of Eq. (8.9) yields X(n) instead of X(n); that is, \n[\nX(O)] \n[X(O)] \n-\nX(2). \nX(1) \nX(n) = \nX(1) \nInstead of X(n) = \nX(2) \nX(3) \nX(3) \n(8.17) \nThis rearrangement is inherent in the matrix-factoring process and is a minor \nproblem because it is straightforward to generalize a technique for unscram-\nbling X(n) to obtain X(n). \n1024 \nÂ§ \n.!!. \nDirect Calculation \nfJ) \nz \n0 \ni= \n5 \n:::i \n<>. \ni= \n512 \n...J \n::J \n:::E \nLI. \n0 \nII: \nW \nCD \n/ \n:::E \n266 \n::J \nz \n128 \n64 \n/ \nFFT Algorithm \nV \n\\ \n/' \n, \n64 128 \n266 \n512 \n1024 \nN (number of sample points) \nFigure 8.1 Comparison of multiplications required by direct calculation and FFT algorithm. \n136 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nRewrite X(n) by replacing argument n with its binary equivalent: \n[~~~~] \nbecomes \n[~~~~~] \n(8.18) \nX(1) \nX(Ol) \nX(3) \nX(1l) \nObserve that if the binary arguments of Eq. (8.18) arefiipped or bit-reversed \n(i.e., 01 becomes 10, 10 becomes 01, etc.), then \nX(n) = [~1~l] \nflips to \n[~1~l] = X(n) \n(8.19) \nX(1l) \nX(11) \nIt is straightforward to develop a generalized result for unscrambling the \nFFT. \nFor N greater than 4, it is cumbersome to describe the matrix-facto-\nrization process analogous to Eq. (8.9). For this reason, we interpret (8.9) \nin a graphical manner. Using this graphical formulation, we can describe \nsufficient generalities to develop a flow graph for a computer program. \n8.3 SIGNAL FLOW GRAPH \nWe convert Eq. (8.9) into the signal flow granh illustrated in Fig. 8.2. As \nshown, we represent the data vector or array xo(k) by a vertical column of \nnodes on the left of the graph. The second vertical array of nodes is the \nvector xl(k) computed in Eq. (8.1l), and the next vertical array corresponds \nto the vector x2(k) = X(n) \n, Eq. (8.14). In general, there will be 'Y compu-\ntational arrays where N = 2\"'1. \nThe signal flow graph is interpreted as follows. Each node is entered \nCOMPUTATION ARRAYS \n. \n, \nl \nData Arrll'( \nArray 1 \nArrIIV 2 \nxO(k' \nx,(k' \n~(Jd \nxO(O, _-------:f\":::------:::; \n.. \n-~(O, \n--------\nEq. (1-23' \nEqs_ (1-30 to 1-33' \nEqs_ (1-35 to 1-38' \nFigure 8.2 FFT signal flow graph, N = 4. \nSec. 8.3 \nSignal Flow Graph \n137 \nby two solid lines representing transmission paths from previous nodes. A \npath transmits or brings a quantity from a node in one array, multiplies the \nquantity by WP, and inputs the result into the node in the next array. Factor \nWP appears near the arrowhead of the transmission path; absence of this \nfactor implies that WP = 1. Results entering a node from the two transmis-\nsion paths are combined additively. \nTo illustrate the interpretation of the signal flow graph, consider node \nDATA \nARRAY \nxolk) \n( I- 1 \nxllk). \nCOMPUTATION ARRAYS \n1 \n1-2 \n1-3 \nx2lk) \nx3M \n1-4 ' \nx4lk) \n'0(0) ~--------_\"\":----='-----\"\"'_--:\":\"\"---_\"\"'\"\"\"\":\"\":\"7---t_ \n\"4(0) \nFigure 8.3 Example of dual nodes. \nDUAL \nNODE \nPAIR \n138 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nX I (2) in Fig. S.2. According to the rules for interpreting the signal flow graph, \nxl(2) = xo(O) + W2xo(2) \n(S.20) \nwhich is simply Eq. (S.13). Each node of the signal flow graph is expressed \nsimilarly. \nThe signal flow graph is then a concise method for representing the \ncomputations required in the factored matrix FFT algorithm of Eq. (S.9). \nEach computational column of the graph corresponds to a factored matrix; \n'Y vertical arrays of N points each (N = 2'Y) are required. Utilization of this \ngraphical presentation allows us to easily describe the matrix-factoring pro-\ncess for large N. \nWe show in Fig. S.3 the signal flow graph for N = 16. With a flow \ngraph of this size, it is possible to develop general properties concerning the \nmatrix-factorization process and thus provide a framework for developing \na FFT computer program flowchart. \n8.4 DUAL NODES \nInspection of Fig. S.3 reveals that in every array we can always find two \nnodes whose input transmission paths stem from the same pair of nodes in \nthe previous array. For example, nodes x I (0) and x I (S) are computed in \nterms of nodes xo(O) and xo(S). Note that nodes xo(O) and xo(S) do not enter \ninto the computation of any other node. We define two such nodes as a dual-\nnode pair. \nBecause the computation of a dual-node pair is independent of other \nnodes, it is possible to perform in-place computation. To illustrate, note \nfrom Fig. S.3 that we can simultaneously compute XI(O) and XI(S) in terms \nof xo(O) and xo(S) and return the results to the storage locations previously \noccupied by xo(O) and xo(S). Storage requirements are then limited to the \ndata array xo(k) only. As each array is computed, the results are returned \nto this array. \nDual-Node Spacing \nLet us now investigate the spacing (measured vertically in terms of the \nindex k) between a dual-node pair. The following discussion will refer to \nFig. S.3. First, in array I = 1, a dual-node pair, say {XI (O),XI (S)}, is separated \nby k = S = N/2 1 = N/2 I. In array I = 2, a dual-node pair, say {X2(S),X2(12)}, \nis separated by k = 4 = N/21 = N/22. Similarly, a dual-node pair, \n{x3(4),x3(6)}, in array I = 3 is separated by k = 2 = N/2 1 = N123; and in \narray I = 4, a dual-node pair, {X4(S),X4(9)}, is separated by k = 1 = N/21 \n= N124. \nGeneralizing these results, we observe that the spacing between dual \nnodes in array I is given by N/2 1â¢ Thus, if we consider a particular node \nSec. 8.4 \nDual Nodes \n139 \nx/(k), then its dual node is x/(k + N/2/). This property allows us to easily \nidentify a dual-node pair. \nDual-Node Computation \nThe computation of a dual-node pair requires only one complex mul-\ntiplication. To clarify this point, consider node x2(8) and its dual x2(12), as \nCOMPUTATION ARRAYS \n1 \nDATA \nARRAY \nxO(k, \n1- 4 \\ \nx4(k' \n_-----------._-=,-----__ \n-:-::-,---__ \n..... \n-:::,--__. \n.. x4(0, \n/-2 \n1-3 \nx2(k, \nx3(k. \n-------~~~~---~-.~-~~-~~~--~~SKIP \n'Of \n141_-+--------------.......,-.!_~'I__----......... \n_ .... \n__:_:,.,.,..,~--__ \n__. \n.... \n----__. \n.. x4( 14) \nFigure 8.4 Example of nodes to be skipped when computing a signal flow graph. \n140 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nillustrated in Fig. 8.3. The transmission paths stemming from node XI (12) \nare multiplied by W 4 and WI2 prior to input at nodes x2(8) and x2(12), re-\nspectively. It is important to note that W4 = \n- W12 and that only one mul-\ntiplication is required because the same data X I (12) is to be multiplied by \nthese terms. In general, if the weighting factor at one node is WP, then the \nweighting factor at the dual node is Wp+NI2. Because WP = - Wp+NI2, only \none multiplication is required in the computation of a dual-node pair. The \ncomputation of any dual-node pair is given by the equation pair: \nxl(k) = Xl-I(k) + WPX1-I(k + N12/) \n(8.21) \nxl(k + N121) = Xl-I(k) -\nWPX1-1(k + N12/) \nIn computing an array, we normally begin with node k \n0 and se-\nquentially work down the array, computing the equation pair of Eq. (8.21). \nAs stated previously, the dual of any node in the Ith array is always down \nNI21 in the array. Because the spacing is N121\n, then it follows that we must \nskip after every NI21 node. To illustrate this point, consider array I = 2 in \nFig. 8.4. If we begin with node k = 0, then according to our previous dis-\ncussions, the dual node is located at k = NI22 = 4, which can be verified \nby inspection of Fig. 8.4. Proceeding down this array, we note that the dual \nnode is always located down by 4 in the array until we reach node 4. At this \npoint, we have entered a set of nodes previously encountered, that is, these \nnodes are the duals for nodes k = 0, 1, 2, and 3. It is necessary to skip over \nnodes k = 4,5,6, and 7. Nodes 8, 9, 10, and 11 follow the original convention \nof the dual node being located 4 down in the array. In general, if we work \nfrom the top down in array I, then we will compute Eq. (8.21) for the first \nN/21 nodes, skip the next N12/, etc. We know to stop skipping when we \nreach a node index greater than N -\n1. \n8.5 Wl'DETERMINATION \nBased on the preceding discussions, we have defined the properties of each \narray with the exception of the value p in Eq. (8.21). The value of p is \ndetermined by (a) writing the index k in binary form with 'Y bits, (b) scaling \nor sliding this binary number 'Y -\nI bits to the right and filling in the newly \nopened bit position on the left with zeros, and (c) reversing the order of the \nbits. This bit-reversed number is the term p. \nTo illustrate this procedure, refer to Fig. 8.4 and consider node x3(8). \nBecause 'Y = 4, k = 8, and I = 3, then k in binary is 1000. We scale this \nnumber 'Y -\nI = 4 - 3 = 1 places to the right and fill in zeros; the result \nis 0100. We then reverse the order of the bits to yield 0010 or integer 2. The \nvalue of p is then 2. \nLet us now consider a procedure for implementing this bit-reversing \noperation. We know that a binary number, say a4a3a2a I', can be written \nSec. 8.7 \nFFT Computation Flowchart \n141 \nin base 10 as a4 x 23 + a3 x 22 + a2 x 21 + al x 2Â°. The bit-reversed \nnumber that we are trying to describe is given by a 1 x 23 + a2 x 22 + a3 \nx 21 + a4 x 2Â°. If we describe a technique for determining the binary bits \na4, a3, a2, and aI, then we have defined a bit-reversing operation. \nNow assume that M is a binary number equal to a4a3a2alÂ·. Divide M \nby 2, truncate, and multiply the truncated results by 2. Then compute \na4a3a2a \nI' -\n2(a4a3a2Â·). If the bit a 1 is 0, then this difference is zero because \ndivision by 2, truncation, and subsequent multiplication by 2 does not alter \nM. However, if the bit al is 1, truncation changes the value of M and the \nabove difference expression is nonzero. We observe that by this technique, \nwe can determine if the bit a \n1 is 0 or 1. \nWe can identify the bit a2 in a similar manner. The appropriate dif-\nference expression is a4a3a2' -\n2(a4a3Â·). If this difference is zero, then a2 \nis zero. Bits a3 and a4 are determined similarly. This procedure forms the \nbasis for developing a bit-reversing computer routine in Sec. 8.7. \n8.6 UNSCRAMBLING THE FFT \nThe final step in computing the FFT is to unscramble the results analogous \nto Eq. (8.19). Recall that the procedure for unscrambling the vector X(n) is \nto write n in binary and reverse or flip the binary number. We show in Fig. \n8.5 the results of this bit-reversing operation: terms x4(k) and x4(i) have \nsimply been interchanged, where i is the integer obtained by bit-reversing \nthe integer k. \nNote that a situation similar to the dual-node concept exists when we \nunscramble the output array. If \nwe proceed down the array, interchanging \nx(k) with the appropriate xU), we eventually encounter a node that has pre-\nviously been interchanged. For example, in Fig. 8.5, node k = 0 remains \nin its location, nodes k = 1, 2, and 3 are interchanged with nodes 8, 4, and \n12, respectively. The next node to be considered is node 4, but this node \nwas previously interchanged with node 2. To eliminate the possibility of \nconsidering a node that has previously been interchanged, we simply check \nto see if i (the integer obtained by bit-reversing k) is less than k. If \nso, this \nimplies that the node has been interchanged by a previous operation. With \nthis check, we can ensure a straightforward unscrambling procedure. \n8.7 FFT COMPUTATION FLOWCHART \nUsing the discussed properties of the FFT signal flow graph, we can easily \ndevelop a flowchart for programming the algorithm on a digital computer. \nWe know from the previous discussions that we first compute array I = 1 \nby starting at node k = 0 and working down the array. At each node k, we \n142 \nThe Fast Fourier Transform (FFT) \nChap.S \nk \nx4(k.-X,,\"' \nXIn. \no \nx4(OOOO, ... \n-----------; \n___ \nâ¢ X(oooo. \nX \n(00011 \n2 \nX(0010' \n3 \nX(00111 \n4 \nX(Ol00' \n5 \nX(01011 \n6 \nX(0110' \n7 \nX(01111 \n8 \nX(l000' \n9 \nX(l00ll \n10 \nX(1010) \n11 \nX(1011) \n12 \nX(ll00' \n13 \nX(1101) \n14 \nX(1110) \n15 \nx4(1111) ... \n-----------; \n.. \n_. X(llll) \nFigure 8.5 Example of the bit-reversing operation for N = 16. \ncompute the equation pair of Eq. (8.21), where p is determined by the de-\nscribed procedure. We continue down the array computing the equation pair \nof Eq. (8.21) until we reach a region of nodes that must be skipped over. \nWe skip over the appropriate nodes and continue until we have computed \nthe entire array. We then proceed to compute the remaining arrays using \nthe same procedures. Finally, we unscramble the final array to obtain the \ndesired results. Figure li.6 illustrates a flowchart for computer programming \nthe FFT algorithm. \nSec. 8.7 \nFFT Computation Flowchart \nSTART \nINPUT DATA \nData: x(k). k = O. 1 \n..... \nN-~ 1 \nN = \n2\"1. \"1 an integer. \nNU =\"1 \nINITIALIZATION \n0 \n- - - -'';''1----\nN2 = N/2 \nNU1='YÂ·l \nk = \nM = Integer value of (k/2NU \n1) 0 \nP = IBR(M) \nJ2 = \nM/2 \nIBR = \n2Â·IBR + (M-2Â·J2) \nM=J2 \nFigure 8.6 FFT computer program flowchart. \n143 \nÂ® \n1 \n= \n1+ 1 \nN2 = N2/2 \nNUl = NU1Â·l \nk = 0 \nBox 1 describes the necessary input data. Data vector xo(k) is assumed \nto be complex and is indexed as k = 0, 1, ... , N -\n1. If xo(k) is real, then \nthe imaginary part should be set to zero. The num.ber of sample points N \nmust satisfy the relationship N = 2'Y, where \"y is integer valued. \n144 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nInitialization of \nthe various program parameters is accomplished in Box \n2. Parameter I is the array number being considered. We start with array I \n= 1. The spacing between dual nodes is given by the parameter N2; for \narray I = 1, N2 = NI2 and is initialized as such. Parameter NUl is the right \nshift required when determining the value ofp in Eq. (8.21); NUl is initialized \nto 'Y -\n1. The index k of the array is initialized to k = 0; thus, we will work \nfrom the top and progress down the array. \nBox 3 checks to see if the array I to be computed is greater than 'Y. If \nyes, then the program branches to Box 13 to unscramble the computed \nresults by bit inversion. If all arrays have not been computed, then we pro-\nceed to Box 4. \nBox 4 sets a counter I = 1. This counter monitors the number of dual-\nnode pairs that have been considered. Recall from Sec. 8.4 that it is necessary \nto skip certain nodes in order to ensure that previously considered nodes \nare not encountered a second time. Counter I is the control for determining \nwhen the program must skip. \nBoxes 5 and 6 perform the computation of Eq. (8.21). Because k and \nI have been initialized to 0 and 1, respectively, the initial node considered \nis the first node of the first array. To determine the factor p for this node, \nrecall that we must first scale the binary number k to the right 'Y -\nI bits. \nTo accomplish this, we compute the integer value of k/2-y-l = k/2NU1 and \nset the result to M as shown in Box 5. According to the procedure for \ndetermining p, we must bit reverse M, where M is represented by 'Y = NU \nbits. The function IBR(M) denoted in Box 5 is a special function routine for \nbit inversion; this routine is described later. \nBox 6 is the computation of Eq. (8.21). We compute the product WP \nx(k + N2) and assign the result to a temporary storage location. Next, we \nadd and subtract this term according to Eq. (8.21). The result is the dual-\nnode output. \nWe then proceed down the array to the next node. As shown in Box \n7, k is incremented by 1. \nTo avoid recomputing a dual node that has been considered previously, \nwe check Box 8 to determine if the counter I is equal to N2. For array 1, \nthe number of nodes that can be considered consecutively without skipping \nis equal to N 12 = N2. Box 8 determines this condition. If I is not equal to \nN2, then we proceed down the array and increment the counter I, as shown \nin Box 9. Recall that we have already incremented k in Box 7. Boxes 5 and \n6 are then repeated for the new value of k. \nIf \nI = N2 in Box 8, then we know that we have reached a node pre-\nviously considered. We then skip N2 nodes by setting k = k + N2. Because \nk has already been incremented by 1 in Box 7, it is sufficient to skip the \npreviously considered nodes by incrementing k by N2. \nBefore we perform the required computations indicated by Boxes 5 \nand 6 for the new node k = k + N2, we must first check to see that we \nSec. 8.8 \nFFT Basic and Pascal Computer Programs \n145 \nhave not exceeded the array size. As shown in Box 11, if k is less than N \n-\n1 (recall k is indexed from 0 to N -\n1), then we reset the counter I to 1 \nin Box 4 and repeat Boxes 5 and 6. \nIf k > N -\n1 in Box 11, we know that we must proceed to the next \narray. Hence, as shown in Box 12, I is indexed by 1. The new spacing N2 \nis simply N2/2 (recall the spacing is NI2 l ). NUl is decremented by 1 (NUl \nis equal to 'Y -\nI), and k is reset to zero. We then check Box 3 to see if all \narrays have been computed. If \nso, then we proceed to unscramble the final \nresults. This operation is performed by Boxes 13 through 17. \nBox 13 bit-reverses the integer k to obtain the integer i. Again we use \nthe bit-reversing function IBR(k), which is explained later. Recall that to \nunscramble the FFT, we simply interchange x(k) and xU). This manipulation \nis performed by the operations indicated in Box 15. However, before Box \n15 is entered, it is necessary to determine, as shown in Box 14, if i is less \nthan or equal to k. This step is necessary to prohibit the altering of \npreviously \nunscrambled nodes. \nBox 16 determines when all nodes have been unscrambled and Box 17 \nis simply an index for k. \nIn Box 18, we describe the logic of the bit-reversing function IBR(k). \nWe have implemented the bit-reversing procedure discussed in Sec. 8.5. \nWhen one proceeds to implement the flow graph of Fig. 8.6 into a \ncomputer program, it is necessary to consider the variables x(k) and WP as \ncomplex numbers and they must be handled accordingly. \nS.S FFT BASIC AND PASCAL COMPUTER PROGRAMS \nA listing of a BASIC program based on the FFT algorithm flowchart in Fig. \n8.6 is shown in Fig. 8.7. The program does not attempt to accomplish the \nutlimate in efficiency but rather is designed to acquaint the reader with the \ncomputer programming procedure of the FFT algorithm. Efficient program-\nming results in a slight increase in computing speed. \nThe inputs to the FFT program are XREAL(N%), the real part of the \nfunction to be discrete Fourier transformed; XIMAG(N%), the imaginary \npart; N%, the number of sample points; and NU%, where N% = 2NU%. \n10000 REM: \n10002 REM: \n10004 REM: \n10010 N2% = \n10020 NU1% = \n10030 K% = 0 \n10040 FOR L% \nFFT SUBROUTINE- THE CALLING PROGRAM SHOULD \nDIMENSION XREAL( 1%) AND XIMAG( 1%). \nN% AND NU% MUST BE INITIALIZED. \nNV2 \nNU% -\n1 \n= 1 TO NU% STEP 1 \nFigure 8.7 FFT BASIC computer subroutine. \n146 \n10050 \n10060 \n10070 \n10080 \n10090 \n10100 \n10110 \n10120 \n10130 \n10140 \n10150 \n10160 \n10170 \n10180 \n10190 \n10200 \n10210 \n10220 \n10230 \n10240 \n10250 \n10260 \n10270 \n10280 \n10290 \n10300 \n10310 \n10320 \n10330 \n10340 \n10350 \n10360 \n10370 \n10380 \n10390 \n10400 \n10410 \n10420 \n10430 \n10440 \n10450 \n10460 \n10470 \n10480 \n10490 \n10500 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nFOR \nI~ = 1 TO N2~ STEP \nJ~ = \nK~\\2ANU1~ \nGOSUB 10410 \nARG = 6.283185# * \nIBITR~/N~ \nC = COS(ARG) \nS = SIN(ARG) \nK1% \nK~ + 1 \nK1N2~ = K1~ + N2~ \nTREAL = \nXREAL(K1N2~) * C + XIMAG(K1N2~)*S \nTIMAG = XIMAG(K1N2~) * C -\nXREAL(K1N2~) * S \nXREAL(K1N2~) = XREAL(K1~) -\nTREAL \nXIMAG(K1N2~) = XIMAG(K1~) -\nTIMAG \nXREAL(K1~) = XREAL(K1~) + TREAL \nXIMAG(K1~) = \nXIMAG(K1~) + TIMAG \nK~ = K~ + 1 \nNEXT \nI~ \nK~ = K~ + N2~ \nIF K~<N~ GOTO 10050 \nK~ = 0 \nNU1% = NU1% -1 \nN2~ = N2~ / 2 \nNEXT \nL~ \nFOR K~ = 1 TO N~ STEP \nJ~ = K~ -\n1 \nGOSUB 10410 \nI~= IBITR~ + 1 \nIF( I~<=K~) GaTO 10380 \nTREAL = XREAL(K~) \nTIMAG = XIMAG(K~) \nNEXT K~ \nRETURN \nEND \nXREAL(K~) \nXREAL(I%) \nXIMAG(K~) \nXIMAG( I~) \nXREAL( I~) \nTREAL \nXIMAG( I~) \nTIMAG \nREM: \nBIT REVERSAL SUB-ROUTINE \nJ1% = J~ \nIBITR~ = 0 \nFOR \n11~ = 1 TO NU~ STEP 1 \nJ2~ = J1%\\2 \nIBITR~ = \nIBITR~*2 + \n(J1~ -\n2*J2~) \nJ1% = J2~ \nNEXT 11% \nRETURN \nEND \nFigure 8.7 (continued) \nSec. B.B \nFFT Basic and Pascal Computer Programs \n147 \nUpon completion, XREAL(N%) is the real part of the transform and \nXIMAG(N%) is the imaginary part ofthe transform. Input data is destroyed. \nNote that the backslash (\\) implies integer division in BASIC. Integer division \nis required for the subroutine to operate correctly. Integer variables are \nspecified by the percent symbol (%) and the pound symbol (#) denotes \ndouble precision. The program uses the convention that array indexing be-\ngins with 1. (Computing recipes throughout this book are more readily im-\nplemented if the complier permits the use of an array index of zero.) Because \nFFT computations are recursive, double-precision arithmetic may be \nrequired. \nIn Fig. 8.8, we show a PASCAL program based on the flowchart of \nFig. 8.6. Input and output variables are identical to those described for the \nBASIC program. \nTYPE REALARRAY=ARRAY[O .. 31] OF REAL; \nFUNCTION IBITR (J,NU: INTEGER): \nINTEGER; \nVAR \nI,J1,J2,K: \nINTEGER; \nBEGIN \nEND; \nJ 1 \n: = J; \nK \n0; \nFOR I \nBEGIN \nJ2 \nK \nJ1 \nEND; \nIBITR \n-\n( IB ITR) \n1 TO NU DO \nJ1 DIV 2' \n- K*2+(J1-2*J2) ; \n-\nJ2 \nK \nPROCEDURE FFT (VAR XREAL,XIMAG: REALARRAY; N,NU: \nINTEGER); \nVAR \nN2,NU1, I ,L,K,M: INTEGER; \nTREAL,TIMAG,P,ARG,C,S: REAL; \nLABEL LBL; \nBEGIN \nN2 := N DIV 2; \nNU1 \n: = NU-1; \nK := 0; \nFOR L := 1 TO NU DO \nBEGIN \nLBL: \nFOR \n:= 1 TO N2 DO \nBEGIN \nM \nK DIV ROUND(EXP (NU1 * LN (2Â»); \nP \nI \nB \nI \nTR ( \nM \n, NU) ; \nFigure 8.8 FFT PASCAL computer subroutine. \n148 \nEND; \nFOR K \nBEGIN \nEND \nEND; {FFT} \nEND; \nThe Fast Fourier Transform (FFT) \nChap. 8 \nARG := 6.283185*P/N; \nC : = COS (ARG); \nS : = SIN (ARG); \nTREAL := XREAL[K+N2]*C+XIMAG[K+N2]*S; \nTIMAG := XIMAG[K+N2]*C-XREAL[K+N2]*S; \nXREAL[K+N2] \n:= XREAL[K]-TREAL; \nXIMAG[K+N2] \n:= XIMAG[K]-TIMAG; \nXREAL[K] \nXREAL[K]+TREAL; \nXIMAG[K] \nXIMAG[K]+TIMAG; \nK := K+1 \nK := K+N2; \nIF K<N THEN GOTO LBL; \nK := 0; \nNU1 \n: = NU1-1; \nN2 := N2 DIV 2 \no TO N-1 DO \nI : = I \nB \nI \nTR (K, NU) ; \nIF I>K THEN \nBEGIN \nTREAL \nXREAL[K]; \nTIMAG \nXIMAG[K]; \nXREAL[K] \nXREAL[I]; \nXIMAG[K] \nXIMAG[I]; \nXREAL[ I] .= TREAL; \nXIMAG[I] \nTIMAG \nEND \nFigure 8.8 \n(continued) \nB.9 THEORETICAL DEVELOPMENT OF THE BASE-2 FFT \nALGORITHM \nIn Sec. 8.2, we used a matrix argument to develop an understanding of why \nthe FFT is an efficient algorithm. We then constructed a signal flow graph \nthat described the algorithm for any N = 2'1. In this section, we relate each \nof these developments to a theoretical basis. First, we will develop a theo-\nretical proof of the algorithm for the case N = 4. We then extend these \narguments to the case N = 8. The reason for these developments for specific \ncases is to establish the notation that we use in the final derivation of the \nalgorithm for the case N = 2'1, where 'Y is integer valued. \nSec. B.9 \nTheoretical Development of the 8ase-2 FFT Algorithm \n149 \nDefinition of Notation \nConsider the discrete Fourier transform relationship of Eq. (8.1) \nN-I \nX(n) = ~ \nxo(k)Wnk \nn = 0, 1, ... , N -\n1 \n(8.22) \nk=O \nwhere we have set W = e -)2\",1 \nN. It is desirable to represent the integers n \nand k as binary numbers; that is, if we assume N = 4, then 'Y = 2 and we \ncan represent k and n as two-bit binary numbers, \n'< = 0, 1, 2, 3 \nn = 0, 1,2,3 \nor \nor \nk = (k\\,ko) = 00,01,10,11 \nn = (n\\,no) = 00,01, to, 11 \nA compact method of writing k and n is \nk = 2k\\ + ko \nn = 2n\\ + no \n(8.23) \nwhere ko, k\\, no, and n\\ can take the values ofO and 1 only. Equation (8.23) \nis simply the method of writing a binary number as its base-to equivalent. \nUsing the representation of Eq. (8.23), we can rewrite Eq. (8.22) for \nthe case N = 4 as \nX(n \\ ,no) = \n~ ~ \nxo(k \\ ,ko) w(2m + no)(2kl + ko) \nko=O kl =0 \n(8.24) \nNote that the single summation in Eq. (8.22) must now be replaced by 'Y \nsummations in order to enumerate all the bits of the binary representation \nof k. \nFactorization of WP \nNow consider the W P term. Because W a+b = WaWb, then \nw(2nl +no)(2kl + ko) \nw(2m + no)2kl w(2m + no)ko \n[w4mkl] w2nokl w(2m + no)ko \nW2nok1 w(2m + no)ko \nNote that the term in brackets is equal to unity because \nW 4mk1 = [w4]mkl = [e -)2\",4/4]nlkl = [l]nlkl \nThus, Eq. (8.24) can be written in the form: \nX(n\\,no) = Â± \n[Â± xo(k\\ ,ko)W2nokl] w(2nl+no)ko \nko=O \nkl=O \n(8.25) \n(8.26) \n(8.27) \n150 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nThis equation represents the foundation of the FFT algorithm. To dem-\nonstrate this point, let us consider each of the summations of Eq. (8.27) \nindividually. First, rewrite the summation in brackets as \nI \nxI(no,ko) = \n~ \nXO(kl,ko)w2noJ<l \nkl~O \nEnumerating the equations represented by Eq. (8.28), we obtain \nXI(O,O) = xo(O,O) + xo(1,O)Wo \nXI(O,I) = xo(O,1) + xo(1,I)Wo \nx I \n(1,0) = xo(O,O) + xo(1 ,0) W 2 \nXI (1, I) = xo(O,1) + xo(1, 1)W2 \nIf \nwe rewrite Eq. (8.29) in matrix notation, we have \n[\nXI(O,O)] \n[1 Â° \nWO 0] [XO(O,O)] \nXI(O,1) \nÂ° 1 Â° WO \nxo(O,l) \n= \n2 \nXI(1,O) \nlOW Â° \nxo(1,O) \nXI(l,1) \nÂ° \n1 Â° W 2 \nxo(1,1) \n(8.28) \n(8.29) \n(8.30) \nNote that Eq. (8.30) is exactly the factored matrix equation of Eq. (8.11), \ndeveloped in Sec. 8.2, with the index k written in binary notation. Thus, the \ninner summation of Eq. (8.27) specifies the first of the factored matrices for \nthe example developed in Sec. 8.2 or, equivalently, the array I = 1 of the \nsignal flow graph illustrated in Fig. 8.2. \nSimilarly, if we write the outer summation of Eq. (8.27) as \nI \nx2(nO,nl) = ~ \nxI(no,ko)W(2n,+no)ko \nko~O \nand enumerate the results in matrix form, we obtain \nWO Â° \nW 2 Â° \nÂ° 1 \nÂ° 1 \n(8.31) \n(8.32) \nwhich is Eq. (8.14). Thus, the outer summation of Eq. (8.27) determines the \nsecond of the factored matrices of the example in Sec. 8.2. \nFrom Eqs. (8.27) and (8.31) we have \n(8.33) \nSec. 8.9 \nTheoretical Development of the Base-2 FFT Algorithm \n151 \nThat is, the final results x2(nO,nl) as obtained from the outer sum are in bit-\nreversed order with respect to the desired values X(nl ,no). This is simply \nthe scrambling that results from the FFT algorithm. \nIf we combine Eqs. (8.28), (8.31), and (8.33), \nI \nX I (no ,ko) = L xo(k \nI ,ko) \nW 2nokl \nkl=O \nI \nL x I (no ,ko) \nw(2m + nolko \nko=O \n(8.34) \nthen the set of Eq. (8.34) represents the original Cooley-Tukey [3] formu-\nlation of the FFT algorithm for N = 4. We term these equations recursive \nin that the second is computed in terms of the first. \nExample 8.1 Cooley-Tukey Algorithm: N = \n8 \nTo illustrate further the notation associated with the Cooley-Tukey formulation of \nthe FFT, consider Eq. (8.22) for the case N = 23 = 8. For this case, \nn = 4n2 + 2n, + no \nk = 4k2 + 2k, + ko \nand Eq. (8.22) becomes \n, , \nn; = 0 or 1 \nk; = 0 or 1 \nX(n2,nt,nO) = \n~ ~ ~ \nXO(k2,k\"ko)w(4n2+2nl+nol(4k2+2kl+kol \nko=Okl=Okz=O \nRewriting WP, we obtain \nW<4nz+2m +no)(4kz+2kl +ko) = w(4nz+2m + nol(4kzlW(4nz \n+ 2m +nol(2kl) \nX w(4nz+2m+ no)(ko) \nWe note that because W\nS = [ei2'Tr/s]s = 1, then \nw(4nz+2nl+nol(4k2l = [WS(2nzkz)][wS(nlkz)]w4nokz = W 4nokz \nw(4nz+2m+no)(2kll = [WS(nzkl)]w(2nl+no)(2kl) = w(2m+no)(2kl) \nHence, Eq. (8.36) can be rewritten as \nt \n, \nX(n2,n\"nO) = \n~ ~ ~ \nXO(k2,kt,ko)w4nok2 \nko=O kl =0 kz=O \nX w(2nl+no)(2kllw(4nz +2nl+no)(ko) \n(8.35) \n(8.36) \n(8.37) \n(8.38) \n(8.39) \n152 \nIf we let \nThe Fast Fourier Transform (FFT) \nxl(no,kl,ko) = L XO(k2,kl,ko)W4nOk2 \nk2~O \nI \nX (n n k) = ~ x (n k k )w(2n1 +no)(2kIl \n2 \n0, 1, 0 \n4.; \nI \n0, 1, 0 \nkl ~O \nI \nX3(nO,nl,n2) = L X2(nO,nl,ko)w(4m+2nl+no)(ko) \nko~O \nChap. 8 \n(8.40) \n(8.41) \n(8.42) \n(8.43) \nthen we have determined the required matrix factorization or, equivalently, the signal \nflow graph for N = 8. The signal flow graph is shown in Fig. 8.9. \nDerivation of the Cooley-Tukey Algorithm for N = 2'Y \nWhen N = 2'1, nand k can be represented in binary form as \nn -\n2-y-l n-y_t + 2-y-2n-y_2 + \nk \n2-y- 1k-y_1 + 2-y- 2k-y_2 + \n+ no \n+ ko \nUsing this representation, we can rewrite Eq. (8.22) as \nt \nX(n-y_1 ,n-y-2, ... \n,no) = 2: 2: ... \n. ,ko)WP \nk-y-l~O \nCOMPUTATION ARRAYS \n(8.44) \n(8.45) \nDATA \n\" \n. \n\\ UNSCRAMBLED \nARRAY \n'/-, \n1-2 \n1-3 \nARRAY \n\"o(k) \nI \ni1lkf \nx2(k) \nx3(k) \nXO(O)I-------___ \n....-----_....,._-----l-. \n...... - - - _X(O) \nwO x3(O) \nFigure 8.9 FFT signal flow graph for N = 8. \nSec. B.9 \nTheoretical Development of the 8ase-2 FFT Algorithm \nwhere \np = (2'Y-1n'Y_1 + 2'Y-2n'Y_2 + ... + no) \nX (2'Y- 1k'Y- 1 + 2'Y- 2k'Y_ 2 + ... + ko) \nBecause wa + b = wa Wb , we rewrite WP as \nNow consider the first term of Eq. (8.47): \nbecause \n[ W2'Y(2'Y-2n'Y-lk'Y-Il] \nX [W2'Y(2'Y-3n'Y-2k'Y-I)] \nX ... [W2'Y(nlk'Y- 1)] \nX w2'Y- 1(no \nk'Y-1l \nw2'Y-1(no \nk'Y-I) \nSimilarly, the second term of Eq. (8.47) yields \nX [W2'Y(2'Y-4n'Y-2k'Y-2)] \nX ... w 2'Y- 1(nl \nk'Y- 2) \nX \nw 2'Y- 2(no \nk'Y-2) \n153 \n(8.46) \n(8.47) \n(8.48) \n(8.49) \n(8.50) \nNote that as we progress through the terms of Eq. (8.47), we add an-\nother factor that does not cancel by the condition W2'Y = 1. This process \ncontinues until we reach the last term in which there is no cancellation. \nUsing these relationships, Eq. (8.45) can be rewritten as \nI \nI \nX(n'Y-1,n'Y-2, ... ,no) = ~ ~... ~ xo(k'Y-.,k'Y- 2, \n... ,ko) \nko=O kl =0 \nk'Y-1 =0 \n(8.51) \n154 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nPerforming each of the summations separately and labeling the intermediate \nresults, we obtain \nxl(no,k.y- 2, \n... \n,ko) = L \nXO(k\"Y_I,k\"Y_2, ... \n,ko)W(2.,,-I)(no k.,,-l) \nk.,,-l~O \nI \n( \nk \nk ) \n~ x (n k \nk )W(2nl+no)(2.,,-2k.,,-2) \nX2 nO,nl, '1-3, ... \n, 0 = \nÂ£.J \nI \n0, '1-2,Â·Â·Â·, 0 \nk.,,-2~0 \nx\"y(no,nl , \n... ,n\"y- d = L x\"y-I(no,nl, \n... ,ko) \nko~O \nX(n\"y-I ,n\"y-2, \n... ,no) = x\"y(no,nl , \n... \n,n\"Y-1) \n(8.52) \nThis set of recursive equations represents the original Cooley-Tukey \nformulation of the FFT, N = 2'1. Recall that the direct evaluation of an N-\npoint transform requires approximately N 2 complex multiplications. Now \nconsider the number of multiplications required to compute the relationships \nof Eq. (8.52). There are -y summation equations that each represent N equa-\ntions. Each of the latter equations contains two complex multiplications; \nhowever, the first multiplication of each equation is actually a multiplication \nby unity. This follows because the first multiplication is always of the form \nWak.,,-i, where k\"y- i = O. Thus, only N-y complex multiplications are required. \nIt can be shown that in the computation of an array, there occurs the re-\nlationship WP = - WP + N12; the number of multiplications can be reduced \nby another factor of 2. The number of complex multiplications for N = 2'1 \nis then N-y12. Similarly, one can reason that there are N-y complex additions. \nCanonic Forms of the FFT \nThere exist many variations of the FFT algorithm that are canonic. \nEach particular algorithm variation is formulated to exploit either a particular \nproperty of the data being transformed or the computer architecture. The \nCooley-Tukey algorithm is illustrated by the signal flow graph of Fig. 8.9. \nWe observe from the signal flow graph that this form of the algorithm can \nbe computed in place, that is, a dual-node pair can be computed and the \nresults stored in the original data storage locations. Further, we observe that \nwith this form of the algorithm, the input data is in natural order and the \noutput data is in scrambled order. In addition, the powers of Ware in bit-\nreversed order. \nIf \none desires, it is possible to rearrange the signal flow graph shown \nin Fig. (8.9) in order that the input data is in scrambled order and the output \nSec. B.9 \nTheoretical Development of the Base-2 FFT Algorithm \n155 \ndata is in natural order. The resulting signal flow can be computed in place \nand the powers of W necessary to perform the computation occur in natural \norder. \nThese two algorithms are often referred to in the literature by the term \ndecimation in time. This terminology arises because alternate derivations \nof the algorithm [8] are structured to appeal to the concept of sample-rate \nreduction or throwing away samples. \nAnother distinct form of the FFT is due to Sande [9]. To develop this \nform, let N = 4 and write \n\\ \n\\ \nX(nt.no) = \n~ ~ \nxo(k\\,ko)w(2nl+no)(2kl+ko) \nko=O kl =0 \n(8.53) \nIn contrast to the Cooley-Tukey approach, we separate the components \nof n instead of the components of k. \nW(2m + no) W(2kl + ko) = w(2m )(2kl + ko) Wno(2kl + ko) \nwhere W 4 = 1. \n[W4mkl] W2mko Wno(2kl + ko) \nw2mko wno(2kl + ko) \nThus, Eq. (8.53) can be written as \nX(n\\,no) = ko~O [k~O \nxo(kt.ko)W2noklwnoko]W2nlko \nIf we define the intermediate computational steps, then \n\\ \nx\\(no,ko) = \n~ \nxo(k t.ko) \nW 2nokl Wnoko \nkl=O \n\\ \nx2(nO,n\\) = \n~ \nx\\(no,ko)W2nlk\nO \nko=O \nX(nt.no) = x2(no,nd \n(8.54) \n(8.55) \n(8.56) \nThe signal flow graph describing the Sande-Tukey algorithm is shown \nin Fig. 8.10 for the case N = 8. We note that the input data is in natural \norder, the output data is in scrambled order, and the powers of W occur in \nnatural order. A signal flow graph that yields results in natural order can be \ndeveloped by proceeding as in the Cooley-Tukey case and interchanging \nnodes. The input data is now in bit-reversed order and the powers of W \noccur in bit-reversed order. \nThese two forms of the FFT algorithm are known by the term deci-\nmation infrequency, where the reasoning for the terminology is analogous \nto that for decimation in time. \n156 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nxO(k) \nx3(k) \nx(O) _----'\"'It--__ \n:----'l~ \nX(O) \nx(11 \nx(2) \nx(3) \nx(4) \nx(5) \nx(6) \nx(7) \nFigure 8.10 Sande-Tukey FFT algo-\nrithm signal flow graph. \n8.10 FFT ALGORITHMS FOR ARBITRARY FACTORS \nIn the discussions to this point, we have assumed that the number of points \nN to be Fourier transformed satisfies the relationship N = 2'Y, where \"y is \ninteger valued. As we saw, this base-2 algorithm resulted in a tremendous \nsavings in computation time; however, the constraint N = 2'Y can be rather \nrestrictive. In this section, we develop FFT algorithms that remove this \nassumption. We will show that significant time savings can be obtained as \nlong as N is highly composite, that is, N = r)r2 ... r m, where ri is an \ninteger. \nTo develop the FFT algorithm for arbitrary factors, we first consider \nthe case of N = r)r2. This approach allows us to develop the notation \nrequired in the proof for the general case. Examples for the base-4 and base-\n\"4 + 2\" algorithms are used to further develop the case N = r)r2. The \nCooley-Tukey algorithm for the case N = r)r2 ... \nrm is then developed as \nwell as twiddle factor algorithms. \nFFT Algorithm for N = f1 f2 \nAssume that the number of points N satisfies the relationship N \nr)r2, where r) and r2 are integer valued. To derive the FFT algorithm for \nthis case, we first express the nand k indices in Eq. (8.22) as \nno = 0, 1, ... , r) -\nn) = 0, 1, ... , r2 -\n1 \nko = 0, 1, ... , r2 -\nk) = 0, 1, ... , r) -\n1 \n(8.57) \nSec. 8.10 \nFFT Algorithms for Arbitrary Factors \n157 \nWe observe that this method of writing the indices allows us to give a unique \nrepresentation of each decimal integer. Using Eq. (8.57), we can rewrite Eq. \n(8.22) as \nX(n I ,no) = r~ \nI [ri: \nI xo(k \nI ,ko) \nwnklr2 ] wnko \nko=O \nkl =0 \nRewriting Wnk\"2, we obtain \nw(mn + no)kln \nwnnmkl wnokln \n[wnn]mkl wnokln \nwhere we have used the fact that wnn = W N = 1. \n(8.58) \n(8.59) \nFrom Eq. (8.59), we rewrite the inner sum of Eq. (8.58) as a new array: \n'I \n-\nI \nXI (no,ko) = ~ \nXO(kl,ko)wnokl'2 \nkl=O \nIf \nwe expand the terms W nko, the outer loop can be written as \n'2-1 \nx2(no,nd = ~ \nxI(no,ko)w(mn+no)ko \nko=O \nThe final result can be written as \nThus, as in the base-2 algorithm, the results are in reverse order. \n(8.60) \n(8.61) \n(8.62) \nEquations (8.60), (8.61), and (8.62) are the defining FFT algorithm re-\nlationships for the case N = rlr2. To further illustrate this particular al-\ngorithm, consider the following examples. \nExample 8.1 \nBase-4 Algorithm for N = 16 \nLet us consider the case N = '1'2 = 4 x 4 = 16, that is, we will develop the base-\n4 algorithm for the case N = 16. Using Eq. (8.57), we represent the variables nand \nkin Eq. (8.22) in a base-4 or quaternary number system: \nn = 4nl + no \nk = 4kl + ko \nEquation (8.58) then becomes \nnl,no = 0, 1,2,3 \n(8.63) \nk],ko = 0, 1,2,3 \n(8.64) \n158 \nRewriting W'nkt, we obtain \nThe Fast Fourier Transform (FFT) \nW4nkt \nw4(4nt + no)kt \nw.6ntkt W 4nOk t \n= [W.6]ntkt W 4nokt \n= W4nOk t \nThe term in brackets is equal to unity because WÂ· 6 = 1. \nChap.S \n(8.65) \nSubstitution of Eq. (8.65) into Eq. (8.60) yields the inner sum of the base-4 \nalgorithm: \nFrom Eq. (8.61) the outer sum is \n3 \nL xo(k.,ko)w4nokt \nkt=O \n3 \nx2(nO,n.) = L x.(no,ko)w(4nt +no)ko \nko=O \nand from Eq. (8.62), the base-4 algorithm results are given by \nX(n .,no) = x2(nO,n.) \n(8.66) \n(8.67) \n(8.68) \nEquations (8.66), (8.67), and (8.68) define the base-4 algorithm for the case N \n16. Based on these equations, we can develop a base-4 signal flow graph. \nExample 8.2 Base-4 Signal Flow Graph for N = \n16 \nFrom the defining relationships of Eqs. (8.66) and (8.67), we observe that there are \n\"I = 2 computational arrays and there are four inputs to each node. The inputs to \nnode x. \n(no,ko) are xo(O,ko), xo(1,ko), xo(2,ko), and xo(3,ko). That is, the four inputs \nto a node i in array I are those nodes in array I -\n1 whose subscripts differ from i \nonly in the (\"I -\nl)th quaternary digit. \nWe show in Fig. 8.11 an abbreviated signal flow graph for the base-4 N = 16 \nalgorithm. To alleviate confusion, only representative transmission paths are shown \nand all WP factors have been omitted. WP factors can be determined from Eqs. (8.66) \nand (8.67). Each pattern of transmission paths shown is applied sequentially to suc-\ncessive nodes until all nodes have been considered. Figure 8.11 also illustrates the \nunscrambling procedure for the base-4 algorithm. Enumeration of Eqs. (8.66) and \n(8.67) reveals that the base-4 algorithm requires approximately 30 percent fewer \nmultiplications than the base-2 algorithm. \nExample 8.3 Base-\"4 + 2\" Algorithm for N = 8 \nLet us now consider the case N = '.'2 = 4 x 2 = 8. This case represents the \nsimplest form ofthe base-\"4 + 2\" algorithm. Base \"4 + 2\" implies that we compute \nas many arrays as possible with a base-4 algorithm and then compute a base-2 array. \nTo develop the \"4 + 2\" algorithm, we first substitute,. = 4 and '2 = 2 into \nEq. (8.57): \nn = 4n. + no \nk = 2k. + ko \nno = 0, 1,2,3 \nko = 0, 1 \nn. = 0, 1 \n(8.69) \nk. = 0, 1, 2, 3 \nSec. 8.10 \nFFT Algorithms for Arbitrary Factors \nDATA \nARRAY \nxO(k,.\"o) \nCOMPUTATION ARRAYS \nA \n\"o(O.O)_----------,_--:II-=:::------_--c:= \n__ -\n-\n-\n-\nxO(2.0)tE----+---1!-+--~ \nxO(3.O)~---------'\" \nUNSCRAMBLED \nARRAY \nX(k) \n-_.X(O) \n159 \n\"2(3.3) -\n-\n-\n-\n- _. \nX(15) \nFigure 8.11 Abbreviated signal flow graph: base 4, N = 16. \nEquation (8.58) then becomes \nX(nl,no) \nExpanding w2nkl, we obtain \n[W8r1kl w2nokl \nW 2nO\nk l \n(8.70) \n(8.71) \n160 \nThe Fast Fourier Transform (FFT) \nWith Eq. (8.71), the inner sum of Eq. (8.70) becomes \n3 \nX \nI (no ,ko) = L xo(k \nI ,ko) \nw2no/q \nkt~O \nThe outer loop can be written as \nx2(nO,nl) = L xl(no,ko)w(4n t+no\n)ko \nko~O \nand the unscrambling is accomplished according to the relationship \nChap. 8 \n(8.72) \n(8.73) \n(8.74) \nEquations (8.72), (8.73), and (8.74) represent the base-\"4 + 2\" FFT algorithm \nfor N = 8. We observe that Eq. (8.72) is a base-4 iteration on the data array and \nEq. (8.73) is a base-2 iteration on array I = 1. The \"4 + 2\" algorithm is more efficient \nthan the base-2 algorithm and is equally restrictive in the choice of N. \nCooley-Tukey Algorithm for N = '1'2 ... 'm \nAssume that the number of points to be discretely transformed satisfies \nN = r 1 r2 ... \n, m, where rl , '2, ... \n, r \nm are integer valued. We first express \nthe indices nand k in a variable radix representation: \nwhere \nk = km-1(r2r3 ... 'm) + km-2(r3r4 . .. rm) \n+ ... + kl'm + ko \nni-I = 0, 1,2, ... ,ri -\n1 \nk i = 0, 1,2, ... , 'm-i -\nWe can now rewrite Eq. (8.22) as \nX(n m -l,n m -2, ... ,n, ,no) \n= ~ \n~ \n... ~ \nxo(km- t ,km- 2, ... ,ko)wnk \nko \nkt \nkm-t \n(8.75) \n(8.76) \nwhere ~ki \nrepresents a summation over all k i = 0, 1,2, ... , rm-i -\n1; 0 \n:S i:S m -\n1. Note that \nwnk = Wn[km-t(nr3 ... \nrm)+Â·Â·Â·+kol \n(8.77) \nSec. 8.10 \nFFT Algorithms for Arbitrary Factors \n161 \nand the first term of the summation expands to \nBecause Wnn. . \n.rm \nw\nN = 1, then Eq. (8.78) can be written as \n(8.79) \nand hence Eq. (8.77) becomes \n(8.80) \nEquation (8.76) can now be rewritten as \nX(n m -l,n m -2, ... ,nl,nO) = L L ... [ L xo(km - l ,km - 2, ... ,ko) \nko kl \nkm-l \nX WnOkm - l(n ... rm)] Wn[km-2(TJ .â¢. rm) \n+ ... + kol \n(8.81) \nNote that the inner sum is over km -I and is only a function of the variables \nno and k m - 2, ... , ko. Thus, we define a new array as \nxI(no,km - 2, ... ,ko) = L xo(km - I, ... ,ko)wnokm-l(n \n... rm) \nkTn -\nI \nEquation (8.81) can now be rewritten as \nX w n[km-2(TJ ... rm)+Â·Â·Â·+kO] \nBy arguments analogous to those leading to Eq. (8.79), we obtain \n(8.82) \n(8.83) \n(8.84) \nThe identity of Eq. (8.84) allows the inner sum of Eq. (8.83) to be \nwritten as \n= L xI(no,km - 2, ... ,ko)W(nIn+no)km-2(TJr\n4 ... \nrm) \nkm -2 \nWe can now rewrite Eq. (8.83) in the form \nX(n m -l,n m -2, ... ,nl,nO) = L L ... L X2(nO,nl,k m - 3 , â¢â¢â¢ ,ko) \nko kl \nkm-3 \nX Wn [km -3(r4rs ... rm )+Â·Â·Â·+kO) \n(8.85) \n(8.86) \n162 \nThe Fast Fourier Transform (FFT) \nChap. 8 \nIf we continue reducing Eq. (8.86) in this manner, we obtain a set of \nrecursive equations of the form \nXi(nO,nl, ... ,ni-I,km - i - I , ... ,ko) \n~ \nxi-l(nO,nl, ... ,ni-2,km - i, ... ,ko) \nkm-i \nX w[n;-I('1,., \n.. \n.rI- I) + ... + nolkm -1('1+ I ... \nrm) \ni = 1, 2, ... , m \n(8.87) \nThe expression of Eq. (8.87) is valid provided we define (ri+ I ... \nr m) \nfor i > m -\n1 and k _ I = o. \nThe final results are given by \n(8.88) \nThe expression of Eq. (8.87) is an extension due to Bergland [10] of \nthe original Cooley-Tukey algorithm. We note that there are N elements in \narray XI, each requiring rl operations (one complex multiplication and one \ncomplex addition), giving a total of Nrl operations to obtain Xl. Similarly, \nit takes Nr2 operations to calculate X2 from Xl. Thus, the computation of \nXm \nrequires N(rl + r2 + ... + r m) operations. This bound does not take into \naccount the symmetries of the complex exponential that can be exploited \nas developed in the following discussions. \nExample 8.4 Base-4, N = \n16, Twiddle Factor Algorithm \nRecall from Eqs. (8.66) and (8.67) that the recursive equations for the base-4 FFT \nalgorithm for N = 16 are given by \nxI(no,ko) = L xo(kl,ko)w4no/q \nkl ~o \n3 \nx2(no,nd = L xl(no,ko)w(4n l +no\n)ko \nko~O \nTo illustrate the twiddle factor concept, let us rewrite Eq. (8.89) as \n(8.89) \nNote that the term Wnoko has been arbitrarily grouped with the outer sum and could \nhave just as easily been grouped with the inner sum. By regrouping, Eq. (8.90) \nbecomes \nSec. 8.10 \nFFT Algorithms for Arbitrary Factors \nor in recursive form \nxl(no,ko) = [k~O \nXO(kl,ko)W4no,,,] Wnoko \nxz(no,n\\) = [k~O \nXI(no,ko)W4n,ko] \n163 \n(8.92) \n(8.93) \n(8.94) \nThe form of the algorithm given by Eq. (8.92) exploits the symmetries of the \nsine and cosine functions. To illustrate this point, consider the term W4no\nk, in brackets \nin Eq. (8.92). Because N = 16, then \n(8.95) \nThus, W4nok, only takes on the values Â± i and Â± 1, depending on the integer nok\\. \nAs a result, the four-point transform in brackets in Eq. (8.92) can be evaluated \nwithout multiplications. These results are then referenced or twiddled [9] by the \nfactor Wnoko, which is outside the brackets in Eq. (8.92). Note that by similar ar-\nguments, Eq. (8.93) can be evaluated without multiplications. The total computations \nrequired to evaluate the base-4 algorithm have been reduced by this regrouping. \nCooleyÂ· \nTukey Twiddle Factor Algorithm \nWe now develop a general formulation of the twiddle factor concept. \nThe original Cooley-Tukey formulation is given by the set of recursive Eqs. \n(S.S7). If \nwe regroup these equations, the first array takes the form \nkm-I \n(S.96) \nand the succeeding equations are given by \n= [ L xi-l(nO, . .. ,ni-2,km - i , . .. ,ko)Wni-,km-i(NIr;)] \n(S.97) \nkm-I \nWe have used the notation x to indicate that these results have been obtained \nby twiddling. Equation (S.92) is valid for i = 1, 2, ... , m if we interpret \nthe case i = 1 in the sense of Eq. (S.96) and if we define (ri+2 ... rm) = \n1 for i > m - 2 and k _ \nI = O. \nEach iteration of Eq. (S.97) requires the evaluation of an ri-point Four-\nier transform followed by a referencing or twiddling operation. The impor-\ntance of this formation is that the bracketed ri-point Fourier transforms can \n164 \nThe Fast Fourier Transform (FFT) \nChap.S \nbe computed with a minimum number of multiplications. For example, if ri \n= 8 (that is, a base-8 transform), then the WP factor in brackets only takes \non the values Â± 1, Â±j, Â± eJ-rr/4, and Â± e - frr/4. Because the first two factors \nrequire no multiplications and the product of a complex number and either \nof the last two factors requires only two real multiplications, a total of only \nfour real multiplications are required in evaluating each eight-point trans-\nform. As we see, the twiddle factor algorithms allow us to take advantage \nof the properties of the sine and cosine functions. \ncomputations Required by Base-2, Base-4, Base-S, \nand Base-16 Algorithms \nLet us consider the case N = 212 = 4096. The real number of multi-\nplications and additions required to evaluate the recursive Eq. (8.97) is given \nin Table 8.1. This summary of operations was first reported by Bergland \n[to]. In counting the number of multiplications and additions, it is assumed \nthat each of the twiddling operations requires one complex multiplication \nexcept when the multiplier is Woo \nTABLE 8.1 \nOperations Required in Computing \nBase-2, Base-4, Base-S, and Base-16 FFT \nAlgorithms for N = 4096 \nNumber of real \nNumber of real \nAlgorithm \nmultiplications \nadditions \nBase 2 \n81,924 \n139,266 \nBase 4 \n57,348 \n126,978 \nBase 8 \n49,156 \n126,978 \nBase 16 \n48,132 \n125,442 \nPROBLEMS \n8.1. Let xo(k) = k, where k = 0, 1, 2, and 3. Compute Eq. (8.1) and note the total \nnumber of multiplications and additions. Repeat the calculation following the \nprocedure outlined by Eqs. (8.6) through (8.14) and again note the total number \nof mUltiplications and additions. Compare your results. \n8.2. It has been shown that the matrix-factoring procedure introduces scrambled \nresults. For N = 8, 16, and 32, show the order of X(n) that results from the \nscrambling. \n8.3. It is desired to convert Eq. (8.9) into a signal flow graph for the case N = 8. \n(a) How many computation arrays are there? \n(b) Define dual nodes for this case. What is the dual-node spacing for each \narray? Give a general expression and then identify each node with its duals \nfor each array. \nChap. 8 \nReferences \n165 \n(c) Write the equation pair (8-21) for each node for array 1. Repeat for the \nother arrays. \n(d) Determine WP for each node and substitute these values into the equation \ndetermined in part c. \n(e) Draw the signal flow graph for this case. \n(f) Show how to unscramble the results of the last computational array. \n(g) Illustrate the concept of node skipping on the signal flow graph. \n8.4. Verify the computer program flowchart illustrated in Fig. 8.6 by mentally ob-\nserving that each of the arrays determined in Problem 8.3 is correctly \ncomputed. \n8.5. Relate each statement of the BASIC program illustrated in Fig. 8.7 with the \ncomputer program flowchart shown in Fig. 8.6. \n8.6. Write an FFT computer program based on the flowchart illustrated in Fig. 8.6. \nThe program should be capable of accepting complex time functions and per-\nforming the inverse transform using the alternate inversion formula. Call this \nprogram FFT. \n8.7. Leth(t) = e- t , where t > O. Sampleh(t) with T = 0.01 andN = 1024. Compute \nthe discrete Fourier transform of h(k) with both FFT and OFT. Compare com-\nputing times. \n8.8. Derive the FFT algorithm for N = r1r2 for the case where the components of \nn are separated, i.e., the Sande-Tukey algorithm. \n8.9. Develop the signal flow graph for the base-4 Sande-Tukey algorithm for N = \n16. \n8.10. Develop the Sande-Tukey base-\"4 + 2\" algorithm for the case N = 8. \n8.11. Develop fully the Sande-Tukey algorithm for the case N = r \n1, r2, ... , r \nm' \n8.12. Develop the Cooley-Tukey base-8 algorithm for the case N = 64. \n8.13. Let N = 16. Develop the Sande-Tukey twiddle factor algorithm. \n8.14. Let N = 8. Is there an advantage in using twiddle factors in computing the \nFFT by the Cooley-Tukey base-2 algorithm? Verify your conclusions by dem-\nonstrating the required number of mUltiplications in each case. \n8.15. Repeat Problem 8.14 for the base-\"4 + 2\" algorithm. \n8.16. Develop a FFT computer program for a base-\"4 + 2\" Cooley-Tukey algorithm \nand bit-reversed data. \n8.17. Develop a FFT computer program for a base-\"4 + 2\" Sande-Tukey algorithm \nand the data in natural order. \n8.18. Develop a FFT computer program for a base-\"8 + 4 + 2\" Sande-Tukey \nalgorithm with data in natural order. The program should maximize the number \nof base-8 computations, then maximize the number of base-4 computations. \nREFERENCES \n1. BERGLAND, G. D. \"A Guided Tour of the Fast Fourier Transform. \" IEEE Spec-\ntrum (July 1969), Vol. 6, No.7, pp. 41-52. \n166 \nThe Fast Fourier Transform (FFT) \nChap.S \n2. BRIGHAM, E. 0., AND R. E. MORROW. \"The Fast Fourier Transform.\" IEEE \nSpectrum (December 1967), Vol. 4, pp. 63-70. \n3. COOLEY, J. W., AND J. W. TUKEY. \"An Algorithm for Machine Calculation of \nComplex Fourier Series.\" Math. Computation (April 1965), Vol. 19, pp. 297-\n301. \n4. GENTLEMAN, W. M. \"Matrix Multiplication and Fast Fourier Transforms.\" Bell \nSyst. Tech. J. (July-August 1968), Vol. 47, pp. 1099-1103. \n5. OPPENHEIM, A. V., AND R. W. SCHAFER. Digital Signal Processing. Englewood \nCliffs, NJ: Prentice Hall, 1975. \n6. PELED, A., AND B. LIU. Digital Signal Processing. New York: Wiley, 1976. \n7. BURRIS, C. S., AND T. W. PARKS. DFT-FFT & Convolution Algorithms & Im-\nplementation. New York: Wiley, 1985. \n8. G-AE Subcommittee on Measurement Concepts. \"What is the Fast Fourier \nTransform?\" IEEE Trans. Audio and Electroacoustics. (June 1967), Vol. AU-\n15, pp. 45-55. Also Proc. IEEE (Oct. 1967), Vol. 55, pp. 1664-1674. \n9. GENTLEMAN, W. M., AND G. SANDE. \"Fast Fourier Transform for Fun and \nProfit.\" AFIPS Proc., 1966 Fall Joint Computer Conf., Vol. 29, pp. 563-678, \nWashington, DC: Spartan, 1966. \n10. BERGLAND, G. D. \"The Fast Fourier Transform Recursive Equations for Ar-\nbitrary Length Records.\" Math. Computation (April 1967), Vol. 21, pp. 236-\n238. \n11. DUHAMEL, P. \"Implementation of 'Split-Radix' FFT Algorithm for Complex, \nReal, and Real-Symmetric Data.\" IEEE Trans. on Audio, Speech and Signal \nProcessing (April 1986), Vol. ASSP-34, No.2, pp. 285-295. \n12. KUMARESAN, R., AND P. K. GUPTA. \"Prime Factor FFT Algorithm with Real \nValued Arithmetic.\" Proc. IEEE, (July 1985), Vol. 73, No.7, pp. 1241-1243. \n13. PREUSS, R. D. \"Very Fast Computation of the Radix 2 Discrete Fourier Trans-\nform.\" IEEE Trans. on Audio, Speech and Signal Processing (Aug. 1982), Vol. \nASSP-30, No.4, pp. 595-607. \n14. SKINNER, D. P. \"Prunning the Decimation-in-Time FFT Algorithm.\" IEEE \nTrans. on Audio, Speech and Signal Processing (April 1976), Vol. ASSP-24, No. \n2, pp. 193-194. \n9 \nFFT TRANSFORM \nAPPLICATIONS \nA fundamental application of the FFT is to the multifaceted areas of trans-\nform analysis. In Chapter 6, we developed the relationship between discrete \nand continuous Fourier transforms. Because the discrete Fourier transform \nyields a close approximation to the continuous Fourier transform, we expect \nsignificant usage of the FFT in computing Fourier and inverse Fourier trans-\nforms. In this chapter, we explore.the mechanics of applying the FFT to the \ncomputation of Fourier transforms, Fourier series, inverse Fourier trans-\nforms and Laplace transforms. As we show, differences between continuous \ntransforms and FFT results arise because of the discrete transform require-\nments for sampling and truncation. \n9.1 FOURIER TRANSFORM APPLICATIONS \nTo illustrate the application of the FFT to the computation of Fourier trans-\nforms, consider Fig. 9.1. We show in Fig. 9.1(a) the function e- t â¢ We wish \nto compute by means of the FFT an approximation to the Fourier transform \nof this function. \nThe first step in applying the discrete transform is to choose the number \nof samples N and the sample interval T. For N = 32 and T = 0.25, we show \nthe samples of e - t in Fig. 9.1 (a). Note that we have defined the sample value \nat t = 0 to be consistent with Eq. (2.47), which states that the value of the \nfunction at a discontinuity must be defined to be the mid \nvalue if \nthe inverse \nFourier transform is to hold. \n167 \n168 \nFFT Transform Applications \nChap. 9 \n1.0 \n0.9 \n0.8 \n0.7 \nw \n0.6 \n0 \n1\\ \nI \n{e-'t>o \n-\n\\ \n-- h(t) = \n'h toO -\no t<O \n\\ \n.- -\nâ¢ â¢ â¢ â¢ Samples of hIt) \nN = \n32 \n\\ \nT = \n0.25 \n:::l \nt-\n::i \n0.5 \n... \n:::E \nc( \n0.4 \n1\\ \\ \n0.3 \n\\ '\\ \n0.2 \n0.1 \n\"' \nr--... \nj'-.. \n~ \no \no \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nJ \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \n.... \no 0.5 \n1.0 1.5 2.0 2.5 \n3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 TIME (kT) \n(a) \n1.0 \nI \nI \nI \nI \nI \nI \nI \nI \nI \n0.9 \n__ R(f)= __ \nI_ \n--\n(21!f)2 + 1 \n0.8 \n. \ne._. Real Part of FFT \n--\nN = 32 \n0.7 \nT = 0.25 \n--\nw \n0 0.6 \n:::l \nâ¢ \nt-\n::i 0.5 \n... \n:::E \nc( \n0.4 \n\\ \n\\ \n0.3 \n0.2 \n0.1 \n\\ \n\\ \nâ¢ \nr----..... \nâ¢ \nâ¢ â¢ â¢ â¢ \no \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nn \nI \nI \nAv \nI \nI \n~ \no \n.25 .50 .75 \n1.0 1.25 1.50 1.75 \n-1.75 Â·1.5 -1.25 -1.0 -.75 -.50 -.25-.125 \nFREQUENCY (n/NT) \n(b) \nFigure 9.1 \nExample of Fourier transform computation via the FFT. \nSec. 9.1 \nFourier Transform Applications \n.5 \nâ¢ \nr--I- __ \nI(f'=~ \n121If,2 + 1 \nâ¢ \n.4 \n. \n3 '--t- â¢â¢â¢â¢ Imaginary Part of FFT \nN = 32 \n.2 -I-\nT= 0.25 \nâ¢ \nâ¢ \n~ .1 \nâ¢ \n::::l \nI-\n:::; \n.... \n~ \n-.1 \n-.2 \n-.3 \n-.4 \n-.5 \nâ¢ \n. \nâ¢ . \n-- -\n--- -- - --\nâ¢ \n- -- --\nV r.--\nl/' \nI \n'-'\" \no \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 Â·30 \nn \nI \nI \nI \nI \nI \nI \n'\\ \nI \nI \nI \nI \nI \n.. \no .25 .50 \n.75 1.00 1.25 1.50 1.75 \n-1.75-1.50-1.25-1.00-_75 -.50 -_25Â·.125 \nFREQUENCY InlNT' \nIe' \nFigure 9.1 \n(cont.) \nWe next compute the discrete Fourier transform using the FFT: \nH(.!!...-) = T \nN~ \nI [e - kT]e - j2'ITnklN \nNT \nk=O \nn = 0, I, ... , N -\n1 \n169 \n(9.1) \nNote that the scale factor T is introduced to produce equivalence between \nthe continuous and discrete transforms. These results are shown in Figs. \n9.I(b) and (c). In Fig. 9.I(b), we show the real part of Fourier transform as \ndetermined in Ex. 2.1 and as computed by Eq. (9.1). Note that the discrete \ntransform is symmetrical about n = N12. This follows because the real part \nof the transform is even [Eq. (6.35)] and the results for n > NI2 are simply \nnegative frequency results. This latter point is emphasized by plotting a true \nfrequency scale beneath the scale for parameter n. \nWe could have graphed the data of Fig. 9.1(b) in the manner conven-\ntionally used to display the continuous Fourier transform, that is, from - fo \nto + \nfo. However, conventional FFT computer programs provide results as \na function of the parameter n. As long as we remember that those results \nfor n > NI2 actually relate to negative frequency results, then we should \nencounter no interpretation problems. \nIn Fig. 9.I(c), we illustrate the imaginary part of the Fourier transform \n(Ex. 2.1) and the discrete transform. As shown, the FFT approximates rather \npoorly the continuous transform for the higher frequencies. To reduce this \nerror, it is necessary to decrease the sample interval T and increase N. \n170 \nFFT Transform Applications \nChap. 9 \nWe note that the imaginary function is odd with respect to n = N12. \nThis follows from Eq. (6.38). Repeating, those results for n > NI2 are to be \ninterpreted as negative frequency results. \nFFT Resolution \nThe FFT results of Figs. 9.l(b) and (c) are spaced in frequency by the \ninterval fo = liNT. As a result, frequency samples approximating the Four-\nier transform are computed for nonnegative frequencies OINT, liNT, 21 NT, \n... , (NI2)INT. The FFT frequency spacing fo = liNT is termed the res-\nolution of the FFT and each frequency result is called a resolution element \nor resolution cell. Intuitively, we can think of the term resolution in the \nsense that we can resolve or distinguish only frequency samples approxi-\nmating the Fourier transform for the frequencies OINT, liNT, 2INT, ... , \n(NI2)INT. Because resolution is given by liNT, then a decrease in the fre-\nquency spacing (increased resolution) can be achieved by increasing N, that \nis, by increasing the truncation interval of the function to be transformed. \n(An increase in T could result in aliasing.) If N is increased by a factor of \ntwo, then the frequency spacing is decreased by a factor of two. Beware \nthat the term increase in resolution is ambiguous in that one is not sure if \na larger or smaller resolving power is implied. \nRecall from Fig. 6.1 that the frequency spacing (resolution) in the dis-\ncrete Fourier transform is determined by the width of the rectangle that \nmUltiplies and truncates the function to be transformed. This truncation in \nthe time domain corresponds to convolution of the [sin(f)]lf function with \nthe Fourier transform of the original time waveform. Convolution with the \n[sin(f)]lf function produces a smearing or blurring of the Fourier transform. \nThe wider the time-domain truncation function, the narrower the [sin(f)]lf \nfunction and the less the frequency smear. The less the frequency smear, \nthe better the frequency-resolving power that is possible. Hence, increased \nfrequency-resolving power of the FFT is established by the degree of fre-\nquency smearing that is achieved by increasing the width of the rectangular \ntruncation function. \nA common mistake made by FFT users is to increase N by appending \nzeros to the sampled and truncated function and to interpret the results as \nhaving enhanced resolution. This is not the case, as can be seen from an \nexamination of Figs. 6.2 and 9.2. We show in Fig. 9.2(a) the discrete Fourier \ntransform resullts, Fig. 6.2(g), from the development of Fig. 6.2. We wish \nto observe the effect of appending zeros to the time function of Fig. 9.2(a). \nAssume that the number of zeros to be appended is N. This can be achieved \nby multiplication by the periodic time function illustrated in Fig. 9.2(b); the \ncorresponding frequency function is also shown. Multiplication yields a pe-\nriodic function of length 2N, where the nonzero values are defined by the \nN samples of Fig. 9.2(a). Multiplication in time implies convolution of the \n... \n..... \n... \nJ \nI \nH(;T) \n,.T, \n\\ \nN \nN \nn \n(a) \nwit) \nW(t) \nII \nI \nIÂ©> \nH' \nI \n-I \n2NT \nI \n(b) \nH(;T)*W(t) \n, \n, Â©> \n....... \n~ \nk \n----------\nn \n2N \n2N \n(e) \nFigure 9.2 Example illustrating false FFT resolution enhancement by appending zeros \n. \n172 \nFFT Transform Applications \nChap. 9 \nfrequency function of Figs. 9.2(a) and (b). Note that the frequency resolution \nhas already been set in Fig. 9.2(a) and the convolution operation merely \nprovided additional frequency samples by interpolating with a [sin(f)]/f func-\ntion the original frequency transform results. Hence, although the frequency \nspacing of the FFT results are more closely spaced as a result of adding \nzeros, the resolution has not been changed. FFT resolution cannot be en-\nhanced by appending zeros unless the function is in fact zero-valued over \nthe interval where the zeros are appended. \nRecall that this discussion simply reinforces the well-known fact that \nresolution is determined by the time duration of \na signal. In FFT applications, \nthe time duration of the signal is set by the truncation interval duration. \nExample 9.1 \nFFT Aliasing \nOne problem encountered when computing the Fourier transform with the FFT is \nthat of aliasing. As stated in Sec. 5.3, aliasing occurs if \nsamples of the time function \nare not taken sufficiently close together. The result is that the frequency function \nfolds, or overlaps, on itself. Figure 9.3 graphically illustrates this point. \nIn Figs. 9.3(a) to (c), we have sampled the function h(t) = e-', t > 0, with \nsample intervals T = 1.0,0.5, and 0.25 s, respectively. N is set to 32 for each case. \nThe magnitude of the Fourier transforms as computed by the FFT are also shown \nin Figs. 9.3(a) to (c). Note that the FFT results are severely aliased for sample interval \nT = 1.0 s. (The magnitude of the continuous Fourier transform is shown in Fig. \n9.3(d).) As shown, aliasing is reduced for T = 0.5 s. A further reduction in sample \ninterval to T = 0.25 s yields results that compare favorably with the theoretical \nfrequency function. Figure 9.3 illustrates the principle that aliasing is reduced as the \nsample interval is reduced. There is no truncation effect as T is reduced because NT \nis always significantly greater than the nonzero width of h(t). \nExperimentally, one can choose an appropriate sample interval by performing \na series of computations similar to those of Fig. 9.3. By successively reducing the \nsample interval, one notices negligible change in the FFT results when an approx-\nimately small sample interval T has been chosen. However, care must be exercised \nto ensure that the effect of decreasing the truncation interval NT does not enter int( \nthe results. If \nrequired, N can be increased as T is decreased. \nExample 9.2 FFT Time-Domain Truncation \nAnother error commonly encountered in applying the FFT to computation of the \nFourier transform arises from time-domain truncation. This error occurs when the \ntotal number of samples chosen to characterize the time function truncates the orig-\ninal time waveform. To illustrate this point, consider Figs. 9.4(a) to (c), where we \nhave truncated h(t) at NT = 1,2, and 5 s, respectively. The magnitude of the Fourier \ntransforms as computed by the FFT are also illustrated. \nTruncation at 1.0 s produces considerable rippling in the FFT results. For a \ntruncation interval of 2.0 s, the FFT results display a decrease in rippling. A further \nincrease in the truncation interval to 5.0 s yields FFT results that have no apparent \ntime-domain truncation rippling effect, as evidenced by Fig. 9.4(d). \nFigure 9.4 illustrates an experimental approach for determining a suitable trun-\n1.0 \n2.0 \n3.0 \n4.0 \n5.0 \nt (sec) \n0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 I(Hz) \n(d) \nFigure 9.3 Illustration of frequency-domain aliasing as a function of sampling rate. \n174 \n~ hit) \n\" \nt. \n05-'Â· â¢ \nN = 32 \nT = 0.25 \nTRUNCATION @ 1.0 \n1 \n... , \n.. \"\"\"\"\"\",+\"Â·1Â·Â·_ \no 4 \n8 \n12 16 20 24 2831 \nk \nI \nI \nI \nI \nI \nI \nI \nI \n~ \n0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 \nt (sec) \nt \nhit) \n10 '. \nN = 32 \nT = 0.25 \n0.5Â·Â·- â¢ \nTRUNCATION @ 2 0 \nL \n... , \n... \n''''I'''''' \n., \n.. -\no 4 \n8 \n12 16 20 24 28 31 \nk \nI \nI \nI \nI \nI \nI \nI \nI \n~ \n0.0 1.0 2.0 3.04.0 5.0 6.0 7.0 \nt (sec) \nt \nhit) \n1.0 \nâ¢ \nT = 0.25 \nN=32 \n\"\"( \nâ¢â¢â¢ '\" \n\"\"\",e.,noN @ \" \n~ \nâ¢â¢ \nt \nâ¢â¢â¢â¢â¢â¢â¢ \n+ \nâ¢â¢ ~ \no 4 \n8 \n12 16 20 24 28 31 \nk \n+ I \nI \nI \nI \nI \nI \n1 \n.. \n0.0 1.0 2.0 3.04.0 5.0 6.0 7.0 \nt (sec) \nN = 32 \nT=0.25 \nr \n\"'J', \\\"'''''' , \nI \no 4 \n8 \n12 16 20 24 2831 \nI \nI \nI \nI \n1 1 \n1 \n1 \n0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 \n~ \nk \n-\nt (sec) \nla) \n(b) \nIe) \n(d) \nFFT Transform Applications \nlIH(I)1 \n1.0 Â± \n0.5 \nâ¢ . \nChap. 9 \nt\n'\" \n. \n..... \n. \n..... \n. \no J ~ lli';~'2b i4 2b ~1 \"n \nI \nI \n1 \nI \n\"v,.-+I-+-I \n--+1--...., \n.. \n-\n0.00.51.0 1.5 \n-1.5 -1.0-0.5 \n11Hz) \n, \nIHII)I \n1.0-t-\n0.5 \n'. \nâ¢â¢ \ne â¢â¢ \n. \n. \n..... \n4 \n8 \n12 16 20 24 2831 \nn \nI \n1 I \n1 \"V-+1 \n-+1 \n-+I--\"~-\n0.00.51.01.5 \nÂ·1.5Â·1.0-0.5 \nI(Hz) \nâ¢ \nIHllli \n1.0-'-\n0.5 \n. \n.' \ne. \n._ \n--+-t--t'-\"-'P\"\"t ,Â·,,\"1 \nI \nI -\nn \no 4 \n12 16 20 24 2831 \nI \nI \nI I \"v~I-~I--~\"-\n0.00.51.01.5 \n-1.5-1.0-0.5 \n11Hz) \nt \nIH(I)I \n1.Â°1~~ \n0.5 \nâ¢ . \n. \n. \n. \n. \n. \n-. \n.. \n. \n... \n. \n... \nI \nI \"\"\"\"1 \nI \nI \nI \n.. \n9 4 \n8 \n12 W \n20 24 28 31 \nn \nt \nI I \nI . \nV,....-fI--lII--+-1 --,. \n.. \n-\n0.0 0.5 1.0 1.5 \n-1.5-1.0 -0.5 \nf(Hz) \nFigure 9.4 Illustration of time-domain truncation. \nSec. 9.1 \n-12 -8 -4 \nFourier Transform Applications \n11(1) \nN=32 \nT=I.0 \n0.6 \n175 \nR(n\"f) \n1'0!.~~T) \n101 \n1 I \n1,:'\" \n.. \n4 \n8 12 \nI \n14 8 12 16 20 24 28 31 \nk \n1 I \nI \nI \nI \nI I-\n4 \n8 12 16 20 24 28 31 n \n(a) \n'. . \n,' \n-0.5 \n-1.0 \n.. . \n'. \n.. \nn \niJ .~ \n~:tÂ·Â· . \n-8 \n-+1---11-1-1 \n-+1 \n---I-I-I \n-+1 \n-+1 \n-\n4 \n8 12 16 20 24 2831 \n(d) \nt \nR(n\"f) \n8Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â· \n6 \n4 \n2 \n4 \n8 12 16 20 24 2831 \nn \n(f) \n(b) \n(e) \n~ h(kT) \n0.6 \n10L:~\"\" \n~:~~ \n+--+--\no 4 \n8 12 16 20 24 2831 \nk \n-.0.5+ \n-10+ \nIe) \nloA\" \n.. \n.. \n.,' \n.. \nn \n::tÂ·Â· \n-8 \n-+1---1-1-1 \n-+1 \n---11-+-1 \n-+1-+1 \n-\n4 \n8 12 16 20 24 2831 \n(g) \nFigure 9.5 Illustration of the FFT of noncausal time functions. \ncation interval. By successively increasing the truncation interval, we can see the \ngradual reduction in the rippling effect. \nExample 9.3 FFT of Noncausal Time Functions \nBecause the discrete Fourier transform requires periodicity in the time domain, care \nmust be exercised when computing the FFT of a time function defined for both \n176 \n1. o~ \n8 \n\\ \n6 \n\\ \n4 \nw \nQ \n~ \n::::; \n.2 \n\"-\n:IE \n<-. \n2 \n4 \n.6 \n.8 \n-1 .0 \n.5 \n.4 \nw .3 \nQ \n:::l \n!:: \nz \n~ .2 \n:IE \n.1 \n0 \n\\ \n0 \nI \nFFT Transform Applications \nChap. 9 \n-- cos (211\" fo t) \nâ¢â¢â¢â¢ \nSamples of cos (21rfot) \nT= 1.0 \nfo = 1/8 \n( \nr\\ \n( 1\\ \n( \\ \nI \\ \nI \n, \nI \\ \nI \\ \nI \\ \nf \n\\ \nI \n\\ \nI \nII \n\\ \nII \n\\ \nII \n\\ \nI \n1\\ \nI \n1\\ \nI \n1\\ \nI \n1\\ 7 \n\\ I \n\\ I \nI \n7 \n\\ ~ \n\\ f \n~ f \n1 -, \n\\ \nlJ \n\\ \nU \n\\ \nU \n\\ \nU \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nt & k \nTIME \n1 I I I I I I I I \nt Continuous Fourier Transform Results \nâ¢ â¢ â¢â¢ Discrete Fourier Transform Results \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nn \nI \nI \nI \nI \nI \nI \n.. \n'AI \n'A \n'Va \n.'Va \n.'h \n-'AI \nf \nFREQUENCY \nFigure 9.6 FFT of a periodic waveform: the truncation interval is equal to a \nmUltiple of the period. \nSec. 9.1 \nFourier Transform Applications \n177 \npositive and negative time (a noncausal function). To develop this point, consider \nthe time function illustrated in Fig. 9.5(a). The appropriate technique for sampling \nsuch a function to maintain the time origin and to observe the periodicity constraint \nis shown in the sampled time function of Fig. 9.5(b). Although the one period shown \ndoes not closely resemble the original function, the periodic function does accurately \nreproduce the original time function. Because the time function is real and odd, from \nEq. (6.38), we expect the frequency function to be purely imaginary and odd, as \nillustrated in Figs. 9.5(c) and (d). \nFigures 9.5(e) to (g) illustrate a common mistake when applying the FFT to \ntime functions of this type. Note that the FFT results for this example are both real \nand imaginary. The real frequency component results from the time sample at t = \no \nbeing defined as 1 rather than 0, the midpoint of the discontinuity. As a result, the \nsampled time function of Fig. 9.5(e) is equal to the time function of Fig. 9.5(b) plus \nan impulse function of unity amplitude at the origin. The Fourier transform of an \nimpulse is a constant real function of frequency, as illustrated in Fig. 9.5(0. \nExample 9.4 FFT of Periodic Functions \nTo compute the FFT of a periodic function, we again must concern ourselves with \nchoosing the sample interval T and the truncation interval. As before, T must be \nchosen to reduce aliasing to an acceptable level. The truncation interval for a periodic \nfunction presents a new problem in that the function does not decay as in the previous \nexamples. However, recall that the N samples of the discrete Fourier transform \nresults represent one period of a periodic time function. Hence, we choose the trun-\ncation interval to be one period (or multiple periods) of the time waveform. In this \nway, our sampled function accurately represents the original periodic waveform. \nTo illustrate this point, we have computed the FFT of the cosine function \nillustrated in Fig. 9.6(a). For sample interval T = 1.0 s and the number of samples \nN = 32, we also show samples of the cosine waveform in Fig. 9.6(a). Note that the \n32 samples define exactly an integer mUltiple of the period of the waveform. In Fig. \n9.6(b), we illustrate the magnitude ofthe FFT of \nthese samples. As shown, the results \nare zero except at the desired frequency. Section 9.2 discusses FFT results for the \ncase where the truncation interval is not chosen equal to an integer multiple of the \nperiod. \nSummary \nIn applying the FFT to the computation of Fourier transforms, keep \nin mind the most important concept that the discrete Fourier transform im-\nplies is periodicity in both the time and frequency domains. If one always \nremembers that the N sample values of the time function represent one \nperiod of a periodic function, the application of the FFT should result in \nfew surprises. \nThe previous discussion and examples have shown that application of \nthe FFT to the computation of the Fourier transform requires that we ex-\nercise care in the choice of parameters T and N. Parameter T controls the \nlevel of aliasing, and parameters Nand T control the width of the truncation \nfunction. If the bandwidth is roughly known, then T can be chosen readily. \n178 \nFFT Transform Aoplications \nChap. 9 \nOtherwise, an experimental procedure as outlined in Exs. 9.1 and 9.2 should \nbe pursued. For an appropriate T and for N chosen sufficiently large so that \ntruncation of the function being transformed does not occur, the FFT will \nyield an accurate approximation to the Fourier transform. For periodic func-\ntions with known periods, we choose NT equal to a period (or integer mul-\ntiple of a period). For those cases where it is impossible to choose N suf-\nficiently large or where the period of a periodic function is not known, the \nconcept of a data-weighting function or data window must be employed. \n9.2 FFT DATA-WEIGHTING FUNCTIONS \nAs discussed previously, time-domain truncation can lead to an unacceptable \napproximation to the Fourier transform. For those cases where data-pro-\ncessing constraints limit the value of N or for those cases where periodic \nfunctions of unknown period are considered, it is necessary to utilize data \nwindows or data-weighting functions. In this section, we will investigate \ndata-weighting functions, a technique for minimizing the undesired effects \nof time-domain truncation. \nRectangular Weighting Function \nFor review, reconsider the graphical development technique shown in \nFig. 6.5. Recall that we first sample the sinusoid by mUltiplication with the \ninfinite sequence of impulse functions, as illustrated in Fig. 6.5(b). This \nresult, Fig. 6.5(c), must then be multiplied by the rectangular truncation \nfunction shown in Fig. 6.5(d) in order to limit the number of sample values \nto N. We refer to time-domain truncation as weighting the data by a rec-\ntangular weighting function. The result of time-domain truncation is evi-\ndenced clearly in Fig. 6.5(e). Note that the original frequency-domain im-\npulse functions have been replaced by [sin(f)]/f functions because of the \nconvolution that results from time-domain truncation. This convolution in-\ntroduces additional frequency-domain components because of the side-lobe \ncharacteristics of the [sin(f)]/f function. These additional components are \ntermed leakage. This terminology arises because the original frequency im-\npulse function has leaked through the side lobes of the [sin(f)]/f function. \nNote that even though our original time waveform is sinusoidal, the \nsampled time waveform is not. This is because the truncation interval is not \nequal to a period (or integer multiple of a period) and hence the convolution \nofthe time functions in Figs. 6.5(e) and (f) does not yield the original periodic \nfunction. Rather, this convolution yields a periodic function with an envelope \nthat is discontinuous. With this discontinuity, one expects the rippling effect \nin the frequency domain, which is illustrated in Fig. 6.5(g). \nTo further demonstrate the effect of a rectangular weighting function, \nSec. 9.2 \nFFT Data-Weighting Functions \n179 \nwe have computed the FFT of the cosine function illustrated in Fig. 9.7(a) \nfor T = 1.0 sand N = 32. In Fig. 9.7(b), we show the magnitude of the \ndiscrete Fourier transform of the samples of Fig. 9.7(a). Note that the FFT \nproduces nonzero frequency components at all discrete frequencies of the \ndiscrete transform. As stated previously, the additional frequency compo-\nnents are termed leakage and are a result of the side-lobe characteristics of \nthe [sin(f)]/f function. \n_-cos \n(21r f tl \n0 \n= \nN 32 \nâ¢â¢â¢â¢ Samples of cos (2Ir fo tl \nT= 1.0 \nfo = 119.143 \n1.0 \nV\\ \n/'\\ \nr \" \n8\\ \n6 , \n1\\ \nJ \\ \nII \\ \n\\ \n\\ \nI \n1\\ \nI \n\\ \n~ \nw \ncÂ· \n::::l \n... \n::J \n~ \n0( -. \n-. \n-1. \n4 \n2 \n2 \n4 \n6 \n8 \n0 \n.5 \n.4 \n~ .3 \n~ \nZ \n~ \n~ .2 \n.1 \no \n\\ \nI \n~ \nI \nI \n\\ \nt \n\\ \n1\\ \nI \n\\ \n1\\ \nI \n\\ \n\\ \nf \nk \n\\ \nI \nI \n\\ \nI \n\\ \n, \n1 \n\\ \nII \n\\ \n\\ I \ni\\ I \n\\ \n~ \n) \n\\' \nrJ \n, \n\\ \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nTIME \n(I \na \nI \nI .I I I I I I I \nI I \nt Continuous Fourier Transform \nI I \nI \nI \nI \nâ¢ â¢ â¢â¢ Discrete Fourier Trensform \n1\\ \nI \nI \nI \nI \n' I \nr \nI \nI \nI I \nI \nI \nI I \nI \nI \nI \nI \nI I \nI I \nI \nI \nrJ \nI~ \n1\\' \nIf \nr, \nI \n~ I \nI li \n\" \nl~ \n~ I{ \n~I\\ \n'{'f fi H \nI , \nII r \nI; Y \n'vY \ni1''i 'tV ,tv 1'1..; -tV \n,.. \n\" \n\" I' \nI \no \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nk \nFREQUENCY \n(b) \nFigure 9.7 FFf of a periodic waveform: the truncation interval is not equal to \na multiple of the period. \n180 \nFFT Transform Applications \nChap. 9 \nTo reduce leakage, it is necessary to employ a time-domain truncation \nor weighting function that has frequency-domain side-lobe characteristics \nthat are of smaller magnitude than those of the [sin(f)]!f function. The \nsmaller the side lobes, the less leakage affects the results of the FFT. To \nclarify this point, consider Fig. 6.5(d) again. The Fourier transform of the \nrectangular weighting function is the [sin(f)]/f function shown. We know \nthat we could choose, without change to the graphical development of Fig. \n6.5, an alternate truncation function in Fig. 6.5(d) that would have lower \nside-lobe characteristics. This is the approach one takes to improve the FFT \napproximation to the Fourier transform. Data-weighting functions that trun-\ncate and weight the data are applied to the N-point sampled function before \nthe FFT is computed. \nWeighting Function Characteristics \nSeveral popular truncation or weighting functions that have been em-\nployed with the FFT are shown in Fig. 9.8(a). The corresponding frequency-\nresponse functions are illustrated in Fig. 9.8(b). Table 9.1 lists the defining \nrelationships for each of these weighting functions in both the time domain \n(centered at the origin for convenience of notation) and the frequency \ndomain. \nAs shown in Fig. 9.8(b), all weighting functions have side lobes in the \nfrequency domain of lower amplitude than those of the rectangular function \nand hence produce less leakage. However, all of the weighting functions \nalso have a broader main lobe. Recall from Figs. 6.5(d) and (e) that the effect \nof time-domain truncation (weighting) is a frequency-domain convolution, \nwith the respective frequency function shown in Fig. 9.8(b). Hence, the \nbroader the main lobe, the more smeared the results of the FFT, that is, the \nbroader the main lobe of the weighting function, the less the capability of \nthe FFT to distinguish or resolve frequencies. In general, the more one \nreduces side lobes or leakage, the broader or more smeared the results of \nthe FFT appear. \nThe trade-off between leakage (side-lobe level) and resolution (main-\nlobe bandwidth) is well-known in many scientific fields. Table 9.1 defines \nthe highest side-lobe level and the 3-dB bandwidth for each weighting func-\ntion. For general experimental work, we prefer to use the Hanning function \nbecause of its implementation simplicity. One should choose that weighting \nfunction whose characteristics are best suited to the problem being \naddressed. \nIt should be noted that irrespective of the weighting function chosen, \nthe FFT yields results at frequency intervals fo = 1INT. But the actual \nfrequency resolution of FFT results is a function of the data-weighting func-\ntion's bandwidth (see Fig. 9.8(b)). Hence, the commonly used FFT reso-\nlution definition 11 NT should be used with care as it describes only the \nSec. 9.2 \nFFT Data-Weighting Functions \nwIt) \n1.0 \n--- - -- Rectangular \nHanning \n---- Parzen \n-\n-\n-\nBartlett \nr----------------\n--------------..., \nÂ·0.5 \nTo \no \nÂ·10 \nÂ·20 \nÂ·30 \n-\n'\" \ni \nÂ·40 \n..J \n~ \n..J \nÂ·50 \nw \n'\" \n9 \nl!: \nÂ·60 \niii \nÂ·70 \nÂ·80 \nÂ·90 \nÂ·100 \nÂ·0.4 \nÂ·0.3 \nTo \nTo \n1 \nNT \n'/ \n'i/O.B \n7 \n0.7 \n'//~ \n0.6 \n0.5 \nÂ·0.2 \nTo \nÂ·0.1 \nTo \n0.4 \n0.3 \n0.2 \n0.1 \n0 \n0.1 \nTo \nla) \nFREOUENCY \n2 \n3 \n4 \n5 \nNT \nNT \nNT \nNT \n0.2 \n~ \n0.4 \nTo \nTo \nTo \n6 \n7 \n8 \n9 \nNT \nNT \nNT \nNT \n~\\ \nI \nI \nl\" \n- - - - -- Rectangular \nHanning \n\\ \n>~:~ \n, \n---- Parzen \n\\ \n1\\.-, \n-\n-\n-\nBartlett \n'.\\ Ii' , 1-'\\ ' ....... \n\\ â¢ \n..l \nI \n\\ \n, ..... \n, \n\\ \n,,\\ ~\\ \n\\ \n\" \n\\ \" \n\\ 1-\" i,...... \n,-, \n\\ f\\, \\ \n: \n\\ I \n\\ \n\\ \n' \nI \n, \n,/ '. \nI \n\\ I 'V\\ I \n\\ \n/ \n\\ \n\\ I \nI~ \n\\ \nI \n\\ \n. \n\\ \\If \\/(\\1 \n~L \n\\ \n\\ \n\\ \n7 1, \n~\\ \nr\\, \n/ \nf\\ \n\\ \nI \n, \n~II \\ \nI \n\\ ! \n\\ / \n\\ \ni ! \n\\ Ii \nIb) \nFigure 9.8 FFT weighting or window functions. \n0.5 \nTo \n181 \nf \n.... \nCD \nN \nWeighting \nFunction \nNomenclature \nRectangular \nBartlett \n(triangle) \nHanning (cosine) \nPanen \nTABLE 9.1 \nData Weighting Functions (To = NT) \nTime Domain \nI \nTo \nwRIt) = I \nt I \n,,; -\n2 \nI \nTo \n=0 \ntl>-\n2 \nWB(t) = \nI - -\nI \nt I \n< -\n[ \n21tlJ \nTo \nTo \n2 \nTo \n=0 \nItl>-\n2 \nWHet) = cos2 (To) \n= H \nI +cos e;:) J \nItl,,;.!'.!! \n2 \n=0 \nItl>.!'.!! \n2 \nWp(t) = 1-24 -\n+48 -\nItl<.J! \n( t )2 \n1 t 11 \nT \nTo \nTo \n4 \n=2[1_~]1 \n.!'.!!<Itl<.!'.!! \nTo \n4 \n2 \nTo \n=0 \nIt 1\"\"\"'2 \nFrequency Domain \nWR(f) = To sinhrfTo) \n\"'fTo \nWB(f) = ~ \n[Sin ~Â¥ \nfTo )]2 \n\"2fTo \nTo \nsin(\",fTo) \nWH(f) = \"'2 \",fTo [I - (fTo)2] \nW p(f) = 3To [Sin(\"'fTo/4)]4 \n8 \n\"'fTo/4 \nHighest \nSide-Lobe \nLevel (db) \n-13 \n-26 \n-32 \n-52 \nAsymptotic \n3-dB \nRolloff \nBandwidth \n(dB/Octave) \n0.85 \nTo \n\\.25 \nTo \n1.4 \nTo \n1.82 \nTo \n6 \n12 \n18 \n24 \nSec. 9.2 \nFFT Data-Weighting Functions \n183 \nfrequency spacing of the FFT results and is independent of the window used. \nIn Chapter 13, where the FFT in the context of \nfilters is discussed, we return \nto this point. \nBecause of the low side-lobe characteristics of the illustrated data-\nweighting functions, we expect that their utilization significantly reduces the \nleakage that results from time-domain truncation. In Fig. 9.9(a), we show \nthe cosine waveform illustrated in Fig. 9.7(a) multiplied by the Hanning \nweighting function illustrated in Fig. 9.8(a). \nFigure 9.9(b) illustrates the FFT of the samples of Fig. 9.9(a). As ex-\npected, leakage is significantly reduced. Note that the majority of the fre-\nquency components are considerably broadened or smeared with respect to \nthe desired impulse function. Recall that this is expected because the effect \nof time-domain truncation or weighting is to convolve the frequency-impulse \nfunction with the Fourier transform of the weighting function. \nExample 9.5 FFT Signal Detection \nA practical application of the utilization of the Hanning weighting function (or any \nother good truncation function) is in signal detection. To illustrate, consider the \nfrequency function in Fig. 9.1O(a), which has been computed by the FFT using the \nstandard rectangular weighting function. A cursory comparison of this illustration \nwith that of Fig. 9.7(b) leads to the conclusion that the time function consists of a \nsingle sinusoid. \nNow consider Fig. 9. lO(b) , where we have recomputed the FFT of the original \ntime function but have used the Hanning weighting function. Note that a second \nsinusoidal component is clearly visible. We now conclude that the time function is \ncomposed of two sinusoids. Leakage resulting from the rectangular truncation func-\ntion almost completely obscures the second, smaller frequency component in Fig. \n9.1O(a). The detection of signals buried in noise is addressed in Sec. 14.3. \nExample 9.6 Dolph-Chebyshev Weighting Functions \nAs discussed previously, low side lobes are achieved at the expense of main-lobe \nbandwidth or broadening. Even though the Hanning function is a weighting function \nthat has low side lobes and is convenient to use, a weighting function that has better \nside-lobe characteristics is the Dolph-Chebyshev function. This window, Refs. [7] \nand [8], minimizes the bandwidth or smearing while forcing all side lobes to be below \na specified level. For some FFT applications, the additional complexity of this \nweighting function may be warranted. \nThe Dolph-Chebyshev weighting function can be calculated [8] from the \nrelationship \nwhere \n( .) = N \n- 1 \n~ (i - 2) (N - i) \nf.I. k+ I \nWN I \nN _ .,.,;.. \nk \nk + 1 I-' \nI k~O \nM = i -\n2 \n= N - i-\ni ~ \n(N + 1)/2 \ni ~ \n(N + 1)/2 \ni ~ 1 or N \n(9.2) \n184 \nFFT Transform Applications \nChap. 9 \nand \nand 13 is the desired side-lobe level in dB. Figure 9.11 shows a BASIC program for \ncomputing WN(i). The required inputs are the number of weights N, (i.e., the number \nof data samples) and the desired side-lobe level (SSL) in decibels. SSL is input as \na positive number. The logarithm function used in the program is the naturalloga-\n.0 \n.8 \n.6 \n.4 \n. \n2 \n.2 \n-.4 \n-.6 \n-.8 \n-1 \nw \no \n~ \n!:: \nz \n~ \n~ \n.0 \n/\\ \nI \nI \nI \nI \nI \nI \n-- cos (2'R' fo t) ['I. - 'I. cos (21T tIT \nc) \n'- \\ â¢â¢â¢â¢ \nSampled Function \nto; 1/9.14 \n3 \nV 1\\ \nI 1 \nN = 32 \nTc = \n32.0 \nT; 1.0 \n/ \n~ \nII \n..... \n...... \nI \n\\ \n1\\ \n/ \nr-x \n\"\\ V \n\\ \nI \n\\ \n/ \nk \nI \n\\ \nI \n1\\ \nI \n\\ J \n\\ 1 \n\\/ \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nTIME \n.5 \nI I I I I I I \n-- Continuous Fourier Transform \n4 \nâ¢â¢â¢â¢ \nFFT \n3 \nâ¢ \nâ¢ \n2 \n1 \nâ¢ \nâ¢ \nn. \nâ¢ \nâ¢ \no \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nk \nFREQUENCY \nFigure 9.9 Example of applying the Hanning function to reduce leakage in com-\nputation of the FFf. \nSec. 9.2 \n.5 \n.4 \nw .3 \nc \n:::l \nI-\nZ \nCI \nÂ« \n::ii .2 \n. 1 \n. \n05 \no \n.5 \n.4 \n~ .3 \n:::l \n!: \nz \nCI \n~ .2 \nâ¢ \n1 \n.05 \no \nFFT Data-Weighting Functions \nI I I I I I \nâ¢ \nContinuous Fourier Transform \nâ¢â¢â¢ \nFFT \nâ¢ \nIÂ· \nâ¢ \nâ¢ \nâ¢ \nâ¢ \nâ¢ \nâ¢ \nâ¢ It \nâ¢ \nâ¢ â¢ \nâ¢ â¢ \nâ¢ \nâ¢ â¢ \no \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nn \nFREQUENCY \n(a) \nI I I I I I \nâ¢ \nContinous Fourier Transform \nâ¢â¢â¢ FFT \n\" \nâ¢ \nâ¢ \nâ¢ Â·tÂ· \n~. â¢ \n.. \n-\n-\no \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nn \nFREQUENCY \n(b) \nFigure 9.10 (a) Example signal obscured by side-lobe leakage, and (b) signal \ndetection by application of the Hanning weighting function. \n185 \nrithm. Exercise caution when computing Dolph-Chebyshev weights because of the \nprecision required if \nN is large [13, 14]. \nWhen the number of data samples is to or larger, the 3-dB bandwidth of the \nmain lobe is essentially independent of N and depends only on the side-lobe level. \nFigure 9.12(a) shows a plot of the increase in bandwidth as the side-lobe level pa-\nrameter varies. Hence, side lobes can be reduced to any desired value but with a \n186 \nFFT Transform Applications \nChap. 9 \n8500 \nREM: \n8510 \nREM: \n8520 \nREM: \n8530 \nREM: \n8540 \nREM: \nDOLPH-CHEBYSHEV WEIGHTING FUNCTION SUBROUTINE \nTHE CALLING PROGRAM SHOULD DIMENSION \n8550 \nAN=N% \nTHE WEIGHTING FUNCTION ARRAY W( 1%), THE NUMBER \nOF WEIGHTING FUNCTION VALUES N%, AND THE DESIRED \nSIDE-LOBE LEVEL \nIN DB SHOULD BE INITIALIZED. \n8560 N1%=(N%+1 )\\2 \n8570 S=10!A(SSL/20!) \n8580 A=2!*LOG(S+SQR(S*S-1 !))/AN-1!) \n8590 \nB=(EXP(A)-1! )*(EXP(A)-1!) \n8600 \nC=(EXP(A)+1! )*(EXP(Al+1!) \n8620 \nD=B/C \n8630 \nFOR 1%=2 TO N1% \n8640 \nAI=I% \n8650 \n11=1%-1 \n8660 \nE=O! \n8670 \nFOR K%=1 \nTO 11 \n8680 K1%=K%-1 \n8690 AK=K% \n8700 \nG=1! \n8710 \nH=1! \n8720 IF (K%-1 )=0 THEN 8770 ELSE 8730 \n8730 \nFOR J%=1 TO K1% \n8740 \nAJ=J% \n8750 G=G*(AI-1 !-AJ)/AJ \n8760 \nNEXT J% \n8770 FOR L%=1 TO K% \n8780 \nAL=L% \n8790 H=H*(AN-AI+1 !-AL)/AL \n8795 NEXT L% \n8800 \nE=E+G*H*(DAAK) \n8810 \nNEXT K% \n8820 \nW( 1%)=(AN-1! )*E/(AN-AI) \n8830 \nW(N%-I%+1 )=W( 1%) \n8840 \nNEXT \n1% \n8850 W(N%)=1 ! \n8860 \nW(1 )=1! \n8870 \nRETURN \n8880 END \nFigure 9.11 \nComputer subroutine in BASIC for computing the Dolph-Chebyshev \nweighting function. \ncorresponding increase in bandwidth. The normalized data plotted in Fig. 9. 12(a) is \nadequate to evalute the degree of increased bandwidth as a function of side-lobe \nlevel. Figure 9.12(b) illustrates Dolph-Chebyshev weighting functions for several \nchoices of side-lobe levels. The rectangular and Hanning functions are also shown \nfor comparison. \nSec. 9.2 \nFFT Data-Weighting Functions \n2 \nNT \n:J: \n0 \n~ \n3 \nc 2NT \nz \nÂ« \n\"\" \nw \n\"\" \n1 \n0 \n...J \nNT \nz \nÂ« \n::lE \n0.5 \nNT \n{\nRECTANGULAR -\n0.85 \n3 d8 8ANDWIDTH \nNT \nHANNING \n-\n1.4 \nNT \n-30 \n-40 \n-50 \n-60 \n-70 \n-80 \n-90 \nÂ·dB \nSIDE \nLOBE LEVEL \n(a) \n1 \n2 \n3 \n4 \n5 \nNT \nNT \nNT \nNT \nNT \n- - - - - - RECTANGULAR \n-10 \nHANNING \n-- -\n40 dB DOLPH-CHEBYSHEV \n\"\" \n-\n-\n-60 \ndB DOLPH-CHEBYSHEV \n'0 -20 \nw \nc \n::::> \nI-\n::; \n-30 \n\"-\n:::!; \nÂ« \nz \n-40 \n0 \n;::: \nu \nz \n::::> \n-50 \n\"-\nCI \nZ \n;::: -60 \n:J: \nCI \n~ -70 \n-80 \n(b) \nFigure 9.12 (a) Bandwidth vs. side-lobe level for the Dolph-Chebyshev weighting \nfunction, and (b) Dolph-Chebyshev frequency-domain functions for 40- and 60-\ndB side-lobe levels. \nSummary \n187 \nThe reader should not infer from the previous discussion that the FFT \nis of little utility for computing the Fourier transform of periodic functions. \nIf you know the period, then you should take advantage of this knowledge \nby selecting the truncation interval equal to an integer multiple ofthe period. \n188 \nFFT Transform Applications \nChap. 9 \nIf the period is unknown, then the FFT results computed with a Hanning \nweighting function are as good an estimate of the frequency function as any \nother estimate. The only reason one questions the results is if \ntruth is known. \nIf the experimenter believes that the time function is periodic, then a succes-\nsion of FFTs with sequentially longer truncation intervals should identify \nthe period. A catalog and comparison of FFT weighting functions is given \nin Refs. [9], [10], and [1 I] \n. \n9.3 FFT ALGORITHMS FOR REAL DATA \nIn applying the FFT, we often consider only real functions of time, whereas \nthe frequency functions are, in general, complex. Thus, a single computer \nprogram written to determine both the discrete transform and its inverse is \nwritten such that a complex time waveform is assumed: \n1 N-I \nH(n) = -\n~ \n[hr(k) + jh;(k)]e-j2-rrnkIN \nN k=O \n(9.3) \nThis follows because the alternate inversion formula of Eq. (6.33) is given \nby \n1 [N-I \nJ* \nh(k) = -\n~ \n[HAn) + jH;(n)]*e -j2-rrnkIN \nN \nn=O \n(9.4) \nand because both Eqs. (9.3) and (9.4) contain the common factor e -j2-rrnkIN, \nthen a single computer program can be used to compute both the discrete \ntransform and its inverse. \nIf the time function being considered is real, we must set to zero the \nimaginary part of the complex time function in Eq. (9.3). This approach is \ninefficient in that the computer program still performs the multiplications \ninvolvingjh;(k) in Eq. (9.4) even thoughjh;(k) is zero. \nIn this section, we develop two techniques for using this imaginary \npart of the complex time function to more efficiently compute the FFT of \nreal functions. \nFFT of Two Real Functions Simultaneously \nIt is desired to compute the discrete Fourier transform of the real time \nfunctions h(k) and g(k) from the complex function \ny(k) = h(k) + jg(k) \n(9.5) \nSec. 9.3 \nFFT Algorithms for Real Data \n189 \nThat is, y(k) is constructed to be the sum of two real functions, where one \nof these real functions is taken to be imaginary. From the linearity property \nof Eq. (6.25), the discrete Fourier transform of y(k) is given by \nY(n) = H(n) + jG(n) \n= [Hr(n + jH;(n)] + j[GAn) + jG;(n)] \n= [Hr(n) -\nG;(n)] + j[H;(n) + Gr(n)] \n= R(n) + jl(n) \n(9.6) \nBy means of the frequency-domain equivalent of Eq. (6.39), we decompose \nboth R(n), the real part of Y(n) , and I(n), the imaginary part of Y(n), into \neven and odd components: \nY(n) = (R;n) + R(N2-\nn)) + (R;n) _ R(N2-\nn)) \n. \n(/(n) \nI(N -\nn)) \n. \n(/(n) \nI(N -\nn)) \n+j 2 \n+ \n2 \n+j 2 -\n2 \nFrom Eqs. (6.45) and (6.46), \nH(n) = Re(n) + j1o(n) \n= (R(n) \nR(N -\nn)) \n2 + \n2 \nSimilarly, from Eqs. (6.47) and (6.48), \njG(n) = Ro(n) + j1e(n) \nor \nG(n) = 1 \ne(n) - jRo(n) \n. \n(/(n) \nI(N - n)) \n+ j \n-\n-\n--'-----'-\n2 \n2 \n= (/~) + I(N 2\n-\nn)) _j(R;n) _ R(N2-\nn)) \n(9.7) \n(9.8) \n(9.9) \nThus, if the real and imaginary parts of the discrete transform of a complex \ntime function are decomposed according to Eqs. (9.8) and (9.9), then the \nsimultaneous discrete transform of two real time functions can be accom-\nplished. This procedure results in a two-series capability with only the re-\nquirement for sorting the results. For ease of reference, the necessary steps \nto simultaneously compute the FFT of two real functions are listed in Fig. \n9.13. Note that Step 4 is written in terms of \nR(N) and I(N). By periodicity, \nwe know that R(N) = R(O) and I(N) = 1(0). \nA BASIC computer program according to the procedure defined in \n190 \nFFT Transform Applications \nChap. 9 \nFig. 9.13 is listed in Fig. 9.14. The two-input real data arrays are \nXIREAL(I%) and X2REAL(I%), where each is N points. Parameters N% \nand NU% should be initialized in the main program; N% = N. Transform \nresults for the real data array XIREAL(I%) are returned from the subroutine \nwith the real part of the transform stored in XIREAL(I%) and the imaginary \npart of the transform stored in XlIMAG(I%). Similarly, the real part of the \ntransform of the X2REAL(I%) data array is returned in array X2REAL(I%) \nand the imaginary part in X2IMAG(I%). The subroutine completely sorts \nthe results so that the output is in exactly the same form as two independent \nFFTs. The program uses the FFT subroutine listed in Fig. 8.7 and hence \nXREAL(I%) and XIMAG(I%) must be dimensioned. For clarity of presen-\ntation, we have utilized additional arrays that are not required if memory is \n1. Functions h(k) and g(k) are real \nk = 0, 1, ... , N -\n1 \n2. Form the complex function: \ny(k) = h(k) + jg(k) \n3. Compute \nk = 0, 1, ... , N -\n1 \nN-1 \nY(n) = L y(k)e-j2TmklN \nk=O \n= R(n) + jl(n) \nn = 0, 1, ... , N - 1 \nwhere R(n) and I(n) are the real and imaginary parts of Y(n), \nrespectively. \n4. \nCompute \nH( ) = [R(n) \nR(N -\nn)] \n. [/(n) _ I(N -\nn)] \nn \n2 \n+ \n2 \n+} \n2 \n2 \nG(n) = [/(;) + I(N; n)] _ j [R~n) _ R(N \n2- n)] \nn = 0, 1, ... , N - 1 \nwhere R(N) = R(O) , I(N) = 1(0), and H(n) and G(n) are the \ndiscrete transforms of h(k) and g(k), respectively. \n5. Scale the results by the sample interval T. \nFigure 9.13 Computation procedure for simultaneous FFT of two real functions. \nSec. 9.3 \nFFT Algorithms for Real Data \n191 \n11000 REM: \nSUBROUTINE FOR SIMULTANEOUS FFT OF TWO REAL FUNCTIONS \n11002 REM: \nSTORED IN ARRAYS X1REAL( 1%) AND X2REAL( 1%). RESULTS ARE \n11004 REM: \nRETURNED IN ARRAYS X1REAL( 1%). Xl IMAG( 1%), X2REAL( 1%), AND \n11006 REM: \nX2IMAG( 1%). THESE ARRAYS AND XREAL( 1%), XIMAG( 1%) MUST BE \n11008 REM: \nDIMENSIONED IN THE MAIN PROGRAM. \nN% AND NU% MUST BE \n11010 REM: \nINITIALIZED. THIS PROGRAM CALLS THE FFT SUBROUTINE \n11012 REM: \nBEGINNING AT LINE 10000 (FIG. 8-7). \n11014 REM: \n11020 FOR 1%=1 TO N% \n11030 \nXREAL( 1%)=X1REAL( 1%) \n11040 \nXIMAG( 1%)=X2REAL( 1%) \n11050 NEXT 1% \n11060 \nGOSUB 10000 \n11070 \nN2%=N%/2 \n11080 \n11090 \nXl REAL ( 1 ) =XREAL ( 1 ) \nXl IMAG( 1) =0 \n11100 \nX2REAL<1 )=XIMAG(1) \n11110 \nX21 MAG ( 1 ) =0 \n11120 FOR 1%=2 TO N% \n11130 \n11140 \nX1REAL( I%)=(XREAL( 1%)+XREAL(N%+2-1%))/2 \nXl IMAG( I%)=(XIMAG( 1%)-XIMAG(N%+2-1%))/2 \n11150 \nX2REAL( I%)=(XIMAG( 1%)+XIMAG(N%+2-1%))/2 \n11160 \nX2IMAG( I%)=-(XREAL( 1%)-XREAL(N%+2-1%))/2 \n11170 NEXT 1% \n11180 RETURN \n11190 END \nFigure 9.14 Computer subroutine in BASIC for simultaneous FFT of two real functions. \na constraint. Output results must be scaled by the sample interval T to obtain \nequivalence to the continuous Fourier transform. \nTransform of 2 N Samples with an N-Sample \nTransform \nThe imaginary part of the complex time function can also be used to \ncompute more efficiently the discrete transform of \na single real time function. \nConsider a function x(k) that is described by 2N samples. It is desired to \ncompute the discrete transform of this function using Eq. (9.3). That is, we \nwish to break the 2N-point function x(k) into two N-sample functions. Func-\ntion x(k) cannot simply be divided in half; instead, we divide x(k) as follows: \nh(k) = x(2k) \n}k = 0, 1, ... , N \n- 1 \n(9.10) \ng(k) = x(2k + 1) \nThat is, function h(k) is equal to the even-numbered samples of x(k), and \n192 \nFFT Transform Applications \nChap. 9 \ng(k) is equal to the odd-numbered samples. Eq. (9.3) can then be written as \n2N-J \nX(n) = \n~ x(k)e - J2-rrnk12N \nk=O \nN-J \nN-J \n= \n~ \nx(2k)e -J2-rrn(2k)/2N + ~ \nx(2k + l)e -J2-rrn(2k+ 1)/2N \nk=O \nk=O \nN-J \nN-J \n= ~ \nx(2k)e - J2-rrnkl \nN + e - J-rrnl \nN ~ \nx(2k + l)e - J2-rrnkl \nN \n(9. 11) \nk=O \nk=O \nN-J \nN-J \n= \n~ \nh(k)e -J2-rrnkIN + e -J-rrnIN ~ \ng(k)e -J2-rrnkIN \nk=O \nk=O \n= H(n) + e -J-rrnIN G(n) \nTo efficiently compute H(n) and G(n), use the previously discussed tech-\nnique. Let \ny(k) = h(k) + jg(k) \nthen \nY(n) = R(n) + jI(n) \nFrom Eqs. (9.8) and (9.9), \nH(n) = Re(n) + jIo(n) \nG(n) = IAn) - jRo(n) \nSubstitution of Eq. (9.13) into Eq. (9.11) yields \nX(n) = Re(n) + jIo(n) + e-J-rrnIN[IAn) - jRo(n)] \n[Re(n) + cos (;)IAn) -\nsin (;)Ro(n)] \n+ j[Io(n) -\nsin (;)Ie(n) - cos (;)Ro(n)] \n= Xr(n) + jXi(n) \nHence, the real part of the 2N-sample function x(k) is \nX ( ) = [R(n) \nR(N - n)] \n(1Tn) [I(n) \nI(N - n)] \nr n \n2 + \n2 \n+ cos \nN \n2 + \n2 \n- sin (;) \n[R;n) - R(N2- n)] \n(9.12) \n(9.13) \n(9.14) \n(9.15) \nSec. 9.3 \nFFT Algorithms for Real Data \n1. Function x( k) is real \nk = D. 1 \nâ¢.... 2N -\n1 \n2. Divide x(k) into two functions: \nh(k) = \nx(2k) \n} \ng(k) = \nx(2k + 1) \nk = D. 1 \nâ¢...â¢ N -\n1 \n3. Form the complex function: \ny(k) = h(k) + jg(k) \n4. Compute \nk = D. 1 \nâ¢...â¢ N -\n1 \nN-l \nYen) = L y(k)e- J2'1fnklN \nk=O \n= R(n) + jl(n) \nn = D. 1 \nâ¢...â¢ N -\n1 \nwhere R(n) and I(n) are the real and imaginary parts of Y(n). \nrespectively. \n5. Compute \nX( ) = [R(n) \nR(N - n)] \n(TI'n) [/(n) \nI(N - n)] \nr n \n2 + \n2 \n+ cos \nN \n2 + \n2 \n_ sin (TI'~) [R~n) _ R(N2\n- n)] \nn = D.1 â¢. ..â¢ \nN - 1 \nX;(n) = [/(;) _/(N; n)] _ sin (TI'~) [/(;) + I(N; n)] \n_ cos (~) \n[R~n) _ R(N2\n- n)] \nn = D. 1 \nâ¢...â¢ N -\n1 \nwhere R(N) = R(D). I(N) = I(D). X,{n) and X,{n) are the real \nand imaginary parts of the 2N = point discrete transform \nof x(k). respectively. \n6. Scale the results by the sample interval T. \nFigure 9.15 Computation procedure for the FFT of a 2N-point function by means \nof aN-point FFT. \n193 \n194 \nFFT Transform Applications \nChap. 9 \n12000 REM: \n12002 REM: \n12004 REM: \n12006 REM: \n12008 REM: \n12010 REM: \nSUBROUTINE FOR EFFICIENT COMPUTATION OF THE FFT OF REAL \nFUNCTIONS. THE CALLING PROGRAM SHOULD DIMENSION XREAL( 1%) \nAND XIMAG( 1%) AND THE COMPUTATION ARRAYS X1REAL( 1%) AND \nXl IMAG( 1%). N%=2N AND NU% MUST BE INITIALIZED. \nTHIS PROGRAM CALLS THE FFT SUBROUTINE BEGINNING AT \nLINE 10000 (FIG. 8-7). \n12012 REM: \n12020 \nN%=N%l2 \n12030 \nNU%=NU%-l \n12040 REM: \nPLACE ODD NUMBERED SAMPLES IN XREAL( \n), EVEN IN XIMAG( \n). \n12050 FOR 1%=1 TO N% \n12060 \nX1REAL( 1%)=XREAL(2*1%-1) \n12070 \nXl IMAG( 1%)=XREAL(2*1%) \n12080 NEXT 1% \n12090 FOR 1%=1 TO N% \n12100 \nXREAL( 1%)=X1REAL( 1%) \n12110 \nXIMAG( 1%)=Xl IMAG( 1%) \n12120 \n12130 \n12140 \n12150 \n12160 \nNEXT \n1% \nREM: \nCOMPUTE THE \nGOSUB 10000 \nARG=3.145926#/N% \nINIT=ARG \nFFT. \n12170 FOR 1%=2 TO N% \n12180 \nS=SIN(ARG) \n12190 \nC=COS(ARG) \n12200 \n12210 \n12220 \n12230 \n12240 \n12250 \n12260 \n12270 \n12280 \n12290 \n12300 \n12310 \n12320 \n12330 \n12340 \n12350 \n12360 \n12370 \n12380 \n12390 \n12400 \n12410 \n12420 \n12430 \nAl=(XREAL( 1%)+XREAL(N%+2-1%))/2 \nA2=(XREAL( 1%)-XREAL(N%+2-1%))/2 \nBl=(XIMAG(1%)+XIMAG(N%+2-1%))/2 \nB2=(XIMAG( 1%)-XIMAG(N%+2-1%))/2 \nX1REAL( 1%)=Al+C*Bl-S*A2 \nXl IMAG( 1%)=B2-S*Bl-C*A2 \nARG=ARG+INIT \nNEXT \n1% \nXREAL(N%+l )=XREAL(l )-XIMAG(l) \nXIMAG(N%+l )=o! \nXREAL(l )=XREAL(l )+XIMAG(l) \nXIMAG(l )=0 \nXIMAG(N%+l )=0 \nFOR 1%=2 TO N% \nXREAL( 1%)=X1REAL( 1%) \nXIMAG( 1%)=XlIMAG( 1%) \nK%=2*N% \nXREAL(K%+2-1%)=XREAL( 1%) \nXIMAG(K%+2-1%)=-XIMAG( 1%) \nNEXT 1% \nN%=2*+% \nNU%=NU%+l \nRETURN \nEND \nFigure 9.16 Computer subroutine in BASIC for the FFT of 2N data samples with aN-point \nFFT. \nSec. 9.4 \nInverse Fourier Transform Applications \n195 \nand, similarly, the imaginary part is \nx.( ) = [/(n) _ I(N -\nn)] _ \n. (1Tn) [/(n) \nI(N -\nn)] \nIn \n2 \n2 \nSin \nN \n2. + \n2 \n(9.16) \n- cos (;) \n[R;n) - R(N2-\nn)] \nThus, the imaginary part of the complex time function can be used \nadvantageously to compute the transform of a function defined by 2N sam-\nples by using a discrete transform that sums only over N values. We normally \nspeak of this computation as performing a 2N-point transform by means of \na N-point transform. For reference, an outline of the computation approach \nis given in Fig. 9.15. Note that Step 5 is written in terms of R(N) and I(N). \nBy periodicity, R(N) = R(O) and I(N) = 1(0). \nA BASIC computer program that implements the outline of Fig. 9.15 \nis listed in Fig. 9.16. The 2N-point real input function is stored in \nXREAL(I%). The main program should initialize N% and NU%, where N% \n= \n2N. For clarity of presentation, dummy array DREAL(I%) and \nDIMAG(I%) are used. All arrays should be of dimension 2N. Because this \nprogram cuts FFT computation time approximately in half, readers with a \npersonal computer should find it very useful. Output results must be scaled \nby the sample interval T to obtain equivalence to the continuous Fourier \ntransform. \n9.4 INVERSE FOURIER TRANSFORM APPLICATIONS \nAssume that we are given the continuous real and imaginary frequency func-\ntions considered in Figs. 9.1(b) and (c) and that we wish to determine the \ncorresponding time function by means of the inverse FFT: \nN-\\ \nh(kT) = Ilf L [R(nllf)] + j/(nllf)ej2-rrnkIN \nn=O \nk = 0,1, ... ,N -\n(9.17) \nwhere Ilf is the frequency sample interval frequency. Assume N = 32 and \nIlf = 1/8. \nBecause we know that R(f), the real part of the complex frequency \nfunction, must be an even function, then we fold R(f) about the frequency \nf = 2.0, which corresponds to the sample point n = N12. As shown in Fig. \n9.17(a), we simply sample the frequency function up to the point n = NI2 \nand then fold these values about n = N12 to obtain the remaining samples. \nIn Figure 9. 17(b), we illustrate the method for determining the N sam-\nples of the imaginary part of the frequency function. Because the imaginary \n196 \nFFT Transform Applications \nChap. 9 \n1.0 \n.9 \n.8 \n~ \nI \nI \nI \nI \nI \nI \nI \nI \nI \n, \n-- R(I) = __ \n1_ (Iolded about n/2) \n, \nI \n(2111)2 + 1 \nI \nâ¢ â¢ â¢â¢ Sampled Real Frequency Function \nI \n.7 \nw \nC \n.6 \n:::l \nSample period = .0.1 \n, \nN = 32 \n; \nIlt= 1/8 \nI-\n:::i \n\"-\n.5 \n::!i \nÂ« \n.4 \n.3 \n\\ \nI \n\\ \nI \n.2 \n.1 \n1\\ \nFold About \n\\ \nI n =IN/2 I \n1I \nI'---~ \n......... / \no \no \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nn \n.5 \n.4 \n. \n3 \n.2 \n~ .1 \n:::l \nj 0 \n\"-\n::!i \nÂ« Â·.1 \n-.2 \n-.3 \n-.4 \n-.5 \nI \nI \ntv \nI \nI \nI \nI \nI \nI \nI \nI \n... \no .25 .50 .75 1.00 1.251.50 1.75 :t.2.0Â·1.75.1.50.1.25-1.00 -.75 -.50 -.25Â·.125 \nnllt \nFREQUENCY \n(a) \nI \nI \nI \n_, \nI \nI \nI \nI \nI \n---I \n-\n1--- 1(1) = ~ \n(Iolded and flipped \n/ , \n(2111)2 + 1 about NI2) \nII \nI \n-\nf- â¢â¢â¢â¢ Sampled Imaginary Frequency Function \nV \nN = 32 \nI \n-f-\n.0.1 = 1/8 \n...... \n......-' ...-' \n.. \n--- ~ \" \n......... ,...... \nFold and flip about n = NI2 \n\"..... \n/' \nI \n........ \n0 \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 3031 \nn \nI \nI \nI \nI \nI \nI \nI \n.. \n0 \n.25 .50 .75 1.00 1.25 \nI \nI \n~ \nI \nI \nI \n1.50 1.75 \nÂ·1.75 Â·1.50 -1.25 Â·1.0 Â·.75 Â·.50 Â·.25-.125 \nnllt \nFREQUENCY \n(b) \nFigure 9.17 Example of the inverse Fourier transform computation via the FFT. \nfrequency function is odd, we must not only fold about the sample value NI \n2, but also flip the results. To preserve symmetry, we set the sample at n \nNI2 to zero. \nComputation of Eq. (9.17) with the sampled function illustrated in Figs. \nSec. 9.4 \nInverse Fourier Transform Applications \n1.0 \n0.9 \n0.8 \n0.7 \n0.6 \nw \no \n~ \n0.5 \n::::; \n... 0.4 \n~ \n0.3 \n0.2 \no. 1 \n0.0 \nf-----\n1\\Â· \n\\ \n\\ \n1 \n-0. 0 \n1\\ \n\\ \n~ \n\"\\ \n2 \n4 \n__ \ne-t \n-f---\nâ¢ â¢ â¢â¢ Real Part of Inverse FFT \nN =32 \n-f--\nT= 0.25 \n-f--\n\" \n,..,... \nF:;::; k \n~ \nâ¢ â¢ â¢ \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 \n3031 \nk \nI \nI \nI \nI \nI \nI \nI \nJ \nI \nI \nI \nI \nI \nI \nI \nJ \n,.. \n0.0 \n0.5 1.0 1.5 2.0Â· 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 TIME (kINAf) \n(e) \nFigure 9.17 (continued) \n197 \n9. 17(a) and (b) yields the inverse discrete Fourier transform. The result is \na complex function whose imaginary part is approx!mately zero and whose \nreal part is as shown in Fig. 9.17(c). We note that at k = 0, the result is \napproximately equal to the correct mid \nvalue and reasonable agreement is \nobtained for all except the results for k large. Improvement can be obtained \nby reducing AI and increasing N. \nNote that there can exist a requirement for a frequency-domain weight-\ning function analogous to the previously discussed time-domain weighting \nfunction. The slightly oscillating results of Fig. 9.17(c) occur because the \nimaginary frequency function has been truncated. This truncation effect can \nbe reduced by increasing N, the number of data points, or by using a weight-\ning function. To use the Hanning weighting function in this example, apply \nit so that it is unity at I = 0 Hz, zero at I = 2 Hz, and returns to unity for \nthe negative frequencies. \nThe key to using the inverse FFT for obtaining an approximation to \ncontinuous results is to specify the sampled frequency functions correctly. \nFigures 9. 17(a) and (b) illustrate this correct method. One should observe \nthe scale factor AI, which is required to give a correct approximation to \ncontinuous inverse Fourier transform results. \nIt is not necessary to write a special FFT program to compute the \ninverse transform relation of Eq. (9.17). Rather, we use a direct transform \nFFT algorithm and employ the alternate inversion formula of Eq. (6.33). To \n198 \nx(kT) \n3 \n2 \nYo \nFFT Transform Applications \nT= 1.0 see. \nN=32 \nL \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n.. \n1 2 4 \nx'(k'T') \n3 \nâ¢ \n2 \n6 \n8 10 12 '4,'6 18 20 22 24 26 28 30: \nT' \n.-; \nn' \n(el \nT' = 0.25 \nO+---~--~----+---~--~----+-~-r--+---~ \n-1 \n4 \nÂ·8 \n12 \n16 \n20 \n24Â· \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n.... \n28 \n32 \nk' \n-2 \n(d) \nFigure 9.18 Example illustrating time-domain interpolation using the FFT. \nChap. 9 \nSec. 9.5 \nLaplace Transform Applications \n199 \nuse this relationship, we first conjugate the complex frequency function, that \nis, the imaginary sampled function illustrated in Fig. 9 \n.17(b) is multiplied by \n-I. Because the resulting time function is real, \nN-J \nh(kD = At L [R(nAf) + j( \n-l)/(nAf)]e \n-j2TrnklN \n(9.18) \nn=O \nwhich yields the time function illustrated in Fig. 9.17(c). \nExample 9.7 FFT Interpolation \nThe FFT is a convenient tool for interpolating a time function. Consider the sampled \ntime function illustrated in Fig. 9.18(a). Because the small number of samples does \nnot give a good indication of the shape of the curve, interpolation is desired. \nFirst, we compute the FFT of the sampled time function shown in Fig. 9. 18(a). \nThe frequency function illustrated in Fig. 9.18(b) is purely real because the time \nfunction is even. We then add zeros to the frequency function. We do this by sep-\narating the frequency function at n = NI2 and adding zeros, as illustrated in Fig. \n9.18(c). We next compute the inverse Fourier transform of this stretched frequency \nfunction. The resulting time function is illustrated in Fig. 9.18(d). As shown, the \nresult of adding zeros in the frequency domain is to interpolate the sampled time \nfunction. \n9.5 LAPLACE TRANSFORM APPLICATIONS \nThe analytic methods for performing the inverse Laplace transform of an \nirrational transfer function are complicated and incomplete. Many numerical \nmethods are available, but the simplest to implement is that using the FFT. \nIn this section, we address the fundamentals of FFT Laplace transform \ninversion. \nThe Laplace transform of a real function of time is given by \nG(s) = Loo g(t)e -sf dt \n(9.19) \nThe Fourier and Laplace transforms are very closely related. In general, \nthe Fourier transform is a function of the real variable t and the Laplace \ntransform is a function of the complex variable s. If \nwe let s = c + j27ft, \nthen Eq. (9.19) becomes \nG(c + j27ft) = Loo [g(t)e -Cf]e -j2Trff dt \n(9.20) \nIf g(t) = 0 for t < 0, then the lower limit of Eq. (9.19) can be replaced by \n-\n00 and Eq. (9.20) becomes the Fourier transform relationship. Hence, the \n200 \nFFT Transform Applications \nChap. 9 \nLaplace transform can be cast in terms of the Fourier transform: \ng(t)e -ct ~ \nG(e + j2'ITf) \n(9.21) \nFrom Eq. (9.21), we can formulate a procedure for inversion of the \nLaplace transform. First, rewrite the transform function G(s) by replacing \nthe variable s by e + j2'ITJ. This inversion yields the time function g(t)e -ct. \nMultiplication of this function by e ct yields the desired function get). Recall \nfrom Laplace transform theory that the parameter e must be chosen larger \nthan the real part of the poles for the transform function G(s). \nAlthough e can be chosen to be any value larger than the real part of \nthe poles of G(x), it should be noted that the effect of a large value of e is \nto attenuate get) and thereby reduce the effects of time-domain aliasing. \nHowever, if e is chosen too large, then the product [g(t)e-ct]ect for large t \nresults in errors due to the rounding errors that occur in the computation of \ng(t)e -ct. Cooley [3] has determined a procedure for optimally choosing e \nto balance aliasing and rounding errors. Unless one is overly concerned with \naccuracy (1 part in 104), then an optimum choice for e is not warranted. \nExample 9.8 Inverse Laplace Transform: C = \n0 \nTo illustrate the computation of the inverse Laplace transform, consider the transfer \nfunction G(s) = l/(s + 1). Replace parameter s by c + j2'ITf to obtain G(c + j2'ITf) \n= 1/(j2'ITf + 1 + c). Because the pole is located at s = - I, then c can be chosen \nas any value greater than - I, say O. Thus, \nG(c + j2:rrf) = G(j2'ITf) = 1/(j2:rrf + 1) \n= 1/[(2'ITf)2 + I] - /l7rf/[(27rf)2 + I] \n(9.22) \nwhich is exactly the example considered in Sec. 9.4. The procedure for computing \nthe inverse Fourier transform of Eq. (9.20) using the FFT is then identical to those \ndescribed in the previous section and Figs. 9.17(a) to (c) apply. The time function \nof Fig. 9.17( \nc) is g(t)e - ct, but because c = 0, then e - ct = 1 and it is not necessary \nto mUltiply by ect. The desired time function is then given by Fig. 9.17(c). \nExample 9.9 Inverse Laplace Transform: C = \n1 \nConsider G(s) = I/s. The pole is located at s = 0 and, hence, c must be chosen \ngreater than zero. Let c = l. Replacing s by 1 + j2'ITf yields G(l + j2mf) = 1/(j2'ITf \n+ I), which is the frequency function considered in Eq. (9.22). Therefore, the inverse \nFourier transform yields the time function illustrated in Fig. 9.17(c). To obtain the \ndesired function g(t), we multiply Fig. 9.17(c) by ect = et ; the result is shown in \nFig. 9.19. The result approximates the theoretically correct step function. Recall \nfrom Sec. 9.4 that the error in this computation is due to frequency-domain trun-\ncation. Note that Fig. 9.19 is periodic, as required by the FFT. One must take this \ninto account in interpreting inverse Laplace transform results. \nChap. 9 \nw \no \n:J \nI-\n:::; \n0-\n:;; \nÂ« \n, \n1. \n1 \n. \n6 \n.3 \n.2 \n.1 \n0 \nÂ·.1 \nProblems \n1/'-\nI', II~ V~ Ira ib I,)' I.'h \n~ \n\\ : \\ \n11 \\ ,/ \\, \n\\. \n~ \n~ \n.\" \nv \nTh_etical Step Function \n-\nâ¢â¢â¢â¢â¢â¢ Real Part of Inverse FFT: g(t).-t -\n0-0-0-0 Computed g(l) \n-\nN = 32, T = 0.25 \nâ¢ . \nâ¢ \nâ¢ . \nâ¢ â¢ \n~ \no \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n22 \n24 \n26 \n28 3031 \nk \n1 \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \n., \no \n.5 \n1.0 \n1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 \n7.5 TIME (k/N~f) \nFigure 9.19 Example inverse Laplace transform computation using the FFT. \nPROBLEMS \n201 \n9.1 By means of the FFT, compute the amplitUde spectrum I \nH(f) I \nand phase aU) \nfor the functions illustrated in Fig. 2.12. \n9.2 Let h(t) = e- t. Sample h(t) with T = 0.25. Compute the FFT of h(kT) for N \n= 8, 16,32, and 64. Compare these results and explain the differences. Repeat \nfor T = 0.1 and T = 1.0 and discuss the results. \n9.3 Let h(t) = cos(2'ITt). Sample h(t) with T = 'IT/8. Compute the FFT for N = \n16. Repeat for N = 'IT/9. Compare these results with those of Figs. 9.6 and 9.7. \n9.4 Consider h(t) illustrated in Fig. 6.7(a). Let To = 1.0. Sample h(t) with T = 0.1 \nand N = 16. Compute the FFT. Repeat for T = 0.2 and N = 4, and for T = \n0.01 and N = 128. Compare and explain these results. \n9.5 Let h(t) = te-t, t > O. Compute the FFT. Give reasons for your choice of T \nandN. \n9.6 Let h(k) be defined according to Problem 9.5. Let \ng(k) = cos(2'ITk/1024) \nk = 0, ... , 1023 \nSimultaneously, compute the discrete Fourier transform of h(k) and g(k) using \nthe procedure described in Fig. 9.13. \n202 \nFFT Transform Applications \nChap. 9 \n9.7 Demonstrate the procedure illustrated in Fig. 9.15 on the function defined in \nProblem 9.5. Let 2N = 1024. \n9.8 Find the inverse FFf of the following functions: \n( ) [sin(21r \n)f) \nf \na \n2Trf \nI \n(b) (l + j21rf)2 \n9.9 By means of the FFT, compute the inverse Laplace transform of the following \nfunctions. The theoretical inverse is given as a means for checking your answer. \n(a) G(s) = e- s \ng(t) = 0; 0 < t < I \nS2 \n= t; t > I \n(b) G(s) = \ne--rrs \ng(t) = 0; ~ < t < Tr \nS2 + I \n= - sm(t); t > Tr \n(c) G(s) = l/[(s + 4)2 + l] \ng(t) = e -1 sin(t); t > 0 \nREFERENCES \n1. COOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH. \"The Finite Fourier Trans-\nform.\" IEEE Trans. Audio and Electroacoust. (June 1969), Vol. AU-17, No.2, \npp.77-85. \n2. COOLEy,J. W.,P.A. W.LEWIs,ANDP.D. WELCH. \"The Fast Fourier Transform \nand its Applications.\" IEEE Trans. Education (March 1969), Vol. 12, No. I, pp. \n27-34. \n3. COOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH. \"The Fast Fourier Transform \nAlgorithm: Programming Considerations in the Calculation of Sine, Cosine and \nLaplace Transforms.\" J. Sound Vih. (July 1970), Vol. 12, No.3, pp. 315-337. \n4. DUBNER, H., AND J. ABATE. \"Numerical Inversion of Laplace Transforms by \nRelating Them to the Finite Fourier Cosine Transform.\" J. Assoc. Comput. \nMach. (January 1968), Vol. 15, No. I, pp. 115-123. \n5. PRASAD, K. P. \"Fast Interpolation Algorithm Using FFf.\" Elec. Lett. (February \n1986), Vol. 22, No.4, pp. 185-187. \n6. SINGHAL, K. \"Interpolation Using the Fast Fourier Transform.\" Proc. IEEE \n(December 1972), Vol. 60, No. 12, p. 1558. \n7. DOLPH, C. L. \"A Current Distribution For Broadside Arrays Which Optimize \nthe Relationship Between Beam Width and Sidelobe Level,\" Proc. IRE (June \n1946), Vol. 34, pp. 335-348. \n8. WARD, H. R. \"Properties of Dolph-Chebyshev Weighting Functions.\" IEEE \nTrans. Aerospace and Elec. Syst. (September 1973), Vol. AE5-9, No.5, pp. \n785-786. \n9. HARRIS, F. J. \"On the Use of \nWindows for Harmonics Analysis with the Discrete \nFourier Transform.\" Proc.IEEE (January 1978), Vol. 66, No. I, pp. 51-83. \n10. CHILDERS, D., AND A. DURLING. Digital Filtering and Signal Processing. St Paul, \nMN: West Publishing, 1975. \nChap. 9 \nReferences \n203 \n11. GECKINLI, N. C., AND D. YARRIS. \"Some Novel Windows and a Concise Tutorial \nComparison of Window Families.\" IEEE Trans. Acoust. Speech Sig. Proc. (De-\ncember 1978), ASSP-26, pp. 501-507. \n12. RAMIREZ, R. W. The FFT Fundamentals and Concepts. Englewood Cliffs, NJ: \nPrentice-Hall, 1985. \n13. DIDERICH, R. \"Calculating Chebyshev Shading Coefficients via the Discrete \nFourier Transform.\" IEEEProc. Lett. (October 1974), Vol. 62, No. 10, pp. 1395-\n1396. \n14. NUTIAL, A. H. \"Generation of Dolph-Chebyshev Weights via a Fast Fourier \nTransform.\" IEEE Proc. Lett. (October 1974), Vol. 62, No. 10, p. 1936. \n10 \nFFT CONVOLUTION \nAND CORRELATION \nFFT applications such as matched filtering, digital signal processing, simu-\nlation, systems analysis, and time-interval measurements are based on an \nimplementation of the discrete convolution or correlation integral. In gen-\neral, a straightforward computation of the discrete integral relationships is \nnot practical because of the excessive number of required mUltiplications. \nHowever, as discussed in Chapter 6, both integrals can be computed by \nmeans of the discrete Fourier transform. With the tremendous increase in \ncomputational speed that can be achieved using the FFT, it is more efficient \nto compute the convolution and correlation integrals by means of \nthe discrete \nFourier transform. \nIn this chapter, we develop the techniques for applying the FFT to \nhigh-speed convolution and correlation. \n10.1 FFT CONVOLUTION OF FINITEÂ·DURATION \nWAVEFORMS \nThe discrete convolution relationship is given by Eq. (7.1) as \nN-] \ny(k) = L x(i)h(k -\ni) \n;=0 \n( \n10.1) \nwhere both x(k) and h(k) are periodic functions with period N. As discussed \nin Chapter 7, discrete convolution, if correctly performed, produces a replica \nof the continuous convolution, provided that both the functions x(t) and h(t) \n204 \nSec. 10.1 \nFFT Convolution of Finite-Duration Waveforms \n205 \nare of finite duration. We now extend that discussion to include efficient \ncomputation by means of the FFT. \nConsider the finite-duration, or aperiodic, waveforms x(t) and h(t) il-\nlustrated in Fig. 1O.1(a). Continuous convolution of these functions is also \nshown. By means of discrete convolution, it is desired to produce a replica \nof the continuous convolution. Recall from Chapter 7 that discrete convo-\nlution requires that we sample both x(t) and h(t) and form periodic functions \nwith period N, as illustrated in Fig. 1O.1(b). The resulting discrete convo-\nlution [Fig. 1O.1(c)] is periodic; however, each period is a replica of the \ndesired finite duration, or aperiodic waveform. Scaling constant T (sample \n1~Â·' \n. \n2 \n2 \n~--------~~-\n~~--------~~--~~ \nb \nt \nvitI a xltl.hltl \nxltl \na+b \nlal \nxlkl \nhlkl \n1 \n2 \na \nk \nb \nk \nN \nN \nIbl \nVlkl \n1 \n.............................. 2' \n.............................. \na+b \nk \n~-------NI--------~ \nleI \nFigure 10.1 Example illustrating inefficient discrete convolution. \n206 \nFFT Convolution and Correlation \nChap. 10 \nperiod) has been introduced to obtain results comparable with continuous \nconvolution. Note that because both x(t) and h(t) are shifted from the origin, \na large N is required to produce a period sufficiently large to eliminate the \noverlap or end effect described in Chapter 7. Computationally, the discrete \nconvolution illustrated in Fig. 1O.1(c) is very inefficient because of the large \nnumber of zeros produced in the interval [0, a + b]. To perform the discrete \nconvolution more efficiently, we simply restructure the data. \nRestructuring the Data \nAs illustrated in Fig. 10.2, we shift each sampled function [Fig. 1O.1(b)] \nto the origin; from Eq. (7.6), we choose the period N > P + Q -\n1 to \neliminate overlap effects. Because we ultimately desire to use the FFT to \nperform the convolution, we also require that N = 2\"'1, where 'Y is integer \nvalued; we assume that a radix-2 algorithm is used. Our results are easily \nextended for the case of other algorithms. \nFunctions x(k) and h(k) are required to have a period N satisfying \nN>P+Q-l \n(10.2) \nN = 2\"'1 \n'Y integer valued \nDiscrete convolution for this choice of N is illustrated in Fig. 1O.2(b); the \nresults differ from that of Fig. 10.1(c) only in a shift of origin. But this shift \nis known a priori. From Fig. 1O.1(a), the shift of \nthe convolutiony(t) is simply \nthe sum of the shifts of the functions being convolved. Consequently, no \ninformation is lost if we shift each function to the origin prior to convolution. \nTo compute the identical waveform of Fig. 1O.2(b) by means of the \nx(k) \nh(k) \n1 ............ \n. \n1 \n2\" ............ \n. \nk \nk \n(a) \ny(k) \n.......................... \nk \n(b) \nFigure 10.2 Discrete convolution of restructured data. \nSec. 10.1 \nFFT Convolution of Finite-Duration Waveforms \n207 \nFFT, we first shift both x(t) and h(t) to the origin. Let the shifts of \nx(t) and \nh(t) be a and b, respectively. Both functions are then sampled. Next, N is \nchosen to satisfy Eq. (10.2). The resulting sampled periodic functions are \ndefined by the relationships \nx(k) = x(kT + a) \nx(k) = 0 \nh(k) = h(kT + b) \nh(k) = 0 \nk = 0, 1, ... , P -\n1 \nk = P, P + 1, ... , N -\n1 \nk = 0, 1, ... , Q -\n1 \nk = Q, Q + 1, ... , N -\n1 \n(10.3) \nThe same notation is used to emphasize that our discussions should assume \nonly sampled periodic functions shifted to the origin. We now compute the \ndiscrete convolution by means of the discrete convolution theorem of Eq. \n(6.50). The discrete Fourier transforms of x(k) and h(k) are computed: \nNext, the product \nN-\\ \nX(n) = L x(k)e -j2-rrnkIN \nk=O \nN-\\ \nH(n) = L h(k)e -j2-rrnkIN \nk=O \nY(n) = X(n)H(n) \n(10.4) \n(10.5) \n(10.6) \nis formed, and finally we compute the inverse discrete transform of Y(n) and \nobtain the discrete convolution y(k): \n1 N-\\ \ny(k) = - L Y(n)ej2TrnkIN \nN n=O \n(10.7) \nNote that the single discrete convolution of Eq. (10.1) has now been replaced \nby Eqs. (10.4) to (10.7). This gives rise to the term the long way around. \nHowever, because of the computing efficiency of the FFT algorithm, these \nfour equations define a shortcut by the long way around. \nA step-by-step computation procedure for applying the FFT to con-\nvolution of discrete functions is given in Fig. 10.3. Note that we have used \nthe alternate inversion formula of Eq. (6.33) in Step 7 and scaled by liN. \nIn Step 8, we scale by the sample interval Tfor comparison with continuous \nresults. \nA BASIC computer program following the procedure of Fig. 10.3 is \nshown in Fig. 10.4. The two real functions to be convolved are stored in \narrays XIREAL(I%) and X2REAL(I%). These arrays, X lIMAG(I%) \n, \nX2IMAG(I%), XREAL(I%), and XIMAG(I%), should be dimensioned by \nthe number of samples N%. N% and NU% must be initialized. We use the \nFFT subroutine starting at line 10000, which is listed in Fig. 8.7. The reader \nis responsible for implementing Steps 2 to 4 to eliminate overlap effects. \n208 \nFFT Convolution and Correlation \nChap. 10 \n1. Let x( f) and h( f) be finite-length functions shifted from the \norigin by a and b, respectively. \n2. Shift x(f) and h(t) to the origin and sample \nx(k) = x(kT + a) \nh(k) = h(kT + b) \nk = 0,1, ... , P -\n1 \nk = 0,1, ... , Q - 1 \n3. Choose N to satisfy the relationships \nN~P+Q-1 \nN= 2'1 \n'Y integer valued \nwhere P is the number of samples defining x(f), and Q is \nthe number of samples defining h( f). \n4. Augment with zeros the sampled functions of Step 2: \nx(k) = 0 \nh(k) = 0 \nk = P, P + 1 \nâ¢...â¢ N - 1 \nk = Q, Q + 1 \nâ¢...â¢ N -\n1 \n5. Compute the FFT of x(k) and h(k): \nN-l \nX(n) = L x(k)e- j2\",nklN \nk=O \nN-l \nH(n) = L h(k)e- j2\",nklN \nk=O \n6. Compute the product \nYen) = X(n)H(n) \n7. Compute the inverse FFT using the forward FFT (note scal-\ning by lIN): \ny( k) = L \n.!. r( \nn) e - j2\",nklN \nN-l ( \n) \nn=O N \n8. Scale the results by the sample interval T. \nFigure 10.3 Computation procedure for FFf convolution of finite-length functions. \nConvolution results are returned stored in XREAL(I%) and must be scaled \nby the sample interval T to obtain results equivalent to continuous convo-\nlution. XIMAG(I%) results should be approximately zero. Note that the \nfactor liN, shown in Step 7, is incorporated in the program. \nSec. 10.1 \nFFT Convolution of Finite-Duration Waveforms \n209 \n13000 REM: \n13002 REM: \nSUBROUTINE FOR CONVOLVING TWO REAL FUNCTIONS STORED \n13004 \n13006 \n13008 \n13010 \n13012 \n13020 \n13030 \n13040 \n13050 \n13060 \n13070 \n13080 \n13090 \n13100 \nREM: \nREM: \nREM: \nREM: \nREM: \nIN ARRAYS X1REAL( I~) AND X2REAL( I~). N~ AND \nNU~ MUST BE \nINITIALIZED. DIMENSION X1REAL( I~) ,Xl IMAG( I~) ,X2REAL( I~), \nX2IMAG( I~),XREAL( \nI~) AND XIMAG( I~). USER IS RESPONSIBLE FOR \nPREVENTING END EFFECTS. CONVOLUTION RESULTS ARE \nRETURNED IN ARRAY XREAL( I~). THIS PROGRAM CALL THE FFT \nSUBROUTINE STARTING AT LINE 10000 (FIG. 8-7). \nFOR \n1~=1 TO N~ \nXREAL( 1~)mX1REAL( \nI~) \nXIMAG( I~)-O \nNEXT \nI~ \nGOSUB 10000 \nFOR \n1~=1 TO N~ \nX1REAL( I~)=XREAL( \nI~) \nXlIMAG( I~)=XIMAG( \nI~) \nXREAL( 1~)=X2REAL( \nI~) \n13110 \nXIMAG(I~)=O \n13120 NEXT \nI~ \n13130 \nGOSUB 10000 \n13140 FOR \n1~=1 TO N~ \n13150 \nX2REAL( I~)=XREAL(I~) \n13160 \nX2IMAG( I~)=XIMAG(I~) \n13170 \nXREAL( 1~)-(X1REAL( \n1~)*X2REAL( I~)-Xl IMAG( 1~)*X2IMAG( \nI~) )/N~ \n13180 \nXIMAG( I )--(X1REAL( 1~)*X2IMAG( \n1~)+Xl IMAG( 1~)*X2REAL( \nI~) )/N~ \n13190 NEXT \nI~ \n13200 \nGOSUB 10000 \n13210 RETURN \n13220 END \nFigure 10.4 BASIC subroutine for FFf convolution. \nExample 10.1 FFT Convolution \nThe application of the FFT to convolution computation is illustrated in Fig. 10.5. \nThe sampled function x(kT), with N = 32, is shown in Fig. lO.5(a). Results of ap-\nplying Eq. (10.4) using the FFT is also shown in Fig. 1O.5(a). Note that FFT results \nare complex and we show a magnitude function. The sampled function h(kn and \nits FFT as computed from Eq. (10.5) are shown in Fig. 1O.5(b). Because P = 16 and \nQ = 16, then N = 32 > P + Q -\nI and there is no overlap. \nWe next form the product frequency function of Eq. (10.6). This result is shown \nin Fig, 1O.5(c) in magnitude form. This complex frequency function is input to the \ninverse FFT, Eq. (10.7), or is conjugated and input to the forward FFT (Step 7, Fig. \n(10.3Â». All results have been scaled to approximate continuous results. \nComputational Efficiency of FFT Convolution \nEvaluation of the N samples of the convolution result y(k) by means \nof Eq. (10.1) requires a computation time proportional to N 2 , the number \n210 \nx(kT) \n2 .............. \n. \nN = 32 \nT = 0.25 \n, .... \n1 \n.... \n1 \n.... \n1.--\n5 \n10 15 20 25 30 \nk \nh(kT) \n2 \n............. \n. \n--+--+---1---j \n.... \nI \n.... \n1 \n.... \n1Â· ____ \n5 \n10 15 20 25 30 \n16 \n12 \nx(kT)'h(kT) \n. \n. \n8\" \n'. \n4 â¢â¢â¢â¢â¢ \n5 \n10 15 20 25 2') \nFFT Convolution and Correlation \n(a) \n(b) \n(e) \n8.0 \n6.0 \n4.0 : \nÂ· \nÂ· \n2.0 .,', \n'\" \n.. '''''If, \n\" \n.\" \n\",\" \n., ' \n8.0 \n6.0 \n4.0 : \nÂ· \n, \n2.0 '.', \n'\" \n5 \n10 15 20 25 30 \n\\I II' \n... \" \n1-': \n1\\,'. \n\" ' \n64.0 \n48.0 : \n32.0 1 \nj \n16.0 \n5 \n10 15 20 25 30 \n, \n, \n, \n, \nI \n. \n, \n. \n, \n, \n, \n1,-, .. \n,,' \n5 \n10 15 20 25 30 \nFigure 10.5 Example convolution using the FFT. \nChap. 10 \nn \nn \nn \nof multiplications. From Sec. 8.2, the computation time of the FFT is pro-\nportional to N log2 N; computation time of Eqs. (10.4) to (10.6) is then \nproportional to 3N log2 N and the computation time of Eq. (10.7) is pro-\nportional to N. It is generally faster to use the FFT and Eqs. (10.4) through \n(10.7) to compute the discrete convolution rather than computing Eq. (10.1) \ndirectly. \nExactly how much faster the FFT approach is than the conventional \napproach depends not only on the number of points but also on the details \nof the FFT and convolution programs being employed. To indicate the point \nat which FFT convolution is faster and the time savings that can be obtained \nSec. 10.2 \nFFT Convolution of Infinite- and Finite-Duration Waveforms \n211 \nby means of FFT convolution, we have observed as a function of \nN the time \nrequired to compute Eq. (10.1) by both the direct and FFT approaches. \nResults of this simulation are given in Table 10.1. As shown, with our com-\nputer programs, it is faster to employ the FFT for convolution if N exceeds \n64. In Sec. 10.3, we describe a technique for reducing the FFT computing \ntime by an additional factor of two; as a result, the breakeven point is for \nN = 32. \nTABLE 10.1 \nComputing Times (Seconds) \nN \nDirect Method \nFFT Method \nSpeed Factor \n16 \n0.0008 \n0.003 \n0.27 \n32 \n0.003 \n0.007 \n0.43 \n64 \n0.012 \n0.015 \n0.8 \n128 \n0.047 \n0.033 \n1.4 \n256 \n0.19 \n0.073 \n2.6 \n512 \n0.76 \n0.16 \n4.7 \n1024 \n2.7 \n0.36 \n7.5 \n2048 \n11.0 \n0.78 \n14.1 \n4096 \n43.7 \n1.68 \n26.0 \n10.2 FFT CONVOLUTION OF INFINITE- AND FINITE-\nDURATION WAVEFORMS \nWe have discussed to this point only the class of functions for which both \nx(t) and h(t) are of finite duration. Further, we have assumed that N = 2'Y \nwas sufficiently small so that the number of samples did not exceed our \ncomputer memory. When either of these two assumptions is false, it is nec-\nessary to use the concept of sectioning. \nConsider the waveforms x(t), h(t), and their convolution y(t), as il-\nlustrated in Fig. 10.6. We assume that x(t) is of infinite duration or that the \nnumber of samples representing x(t) exceeds the memory of the computer. \nAs a result, it is necessary to decompose x(t) into sections and compute the \ndiscrete convolution as many smaller convolutions. Let NT be the time \nduration of \neach section of \nx(t) to be considered; these sections are illustrated \nin Fig. 1O.6(a). As shown in Fig. 1O.7(a), we form the periodic sampled \nfunction x(k), where a period is defined by the first section of x(t); h(t) is \nsampled and zeros are added to obtain the same period. Convolution y(k) \nof \nthese functions is also illustrated in Fig. 1O.7(a). Note that we do not show \nthe first Q - 1 \npoints of \nthe discrete convolution; these samples are incorrect \nbecause of the end effect. Recall from Sec. 7.3 for h(k) defined by Q samples \nthat the first Q -\n1 samples of y(k) have no relationship to the desired \ncontinuous convolution and should be discarded. \nIn Fig. 1O.7(b), we illustrate the discrete convolution of the second \n212 \nFFT Convolution and Correlation \nChap. 10 \nx(t) \nNT \nNT \nNT \nCa) \n'p \n. \nt \nyCt) \nCb) \n, \n, \nCe) \nFigure 10.6 Example convolution of infinite- and a finite-duration waveforms. \nsection of duration NT illustrated in Fig. 1O.6(a). As described in Sec. 10.1, \nwe have shifted this section to the origin for purposes of efficient convo-\nlution. The section is then sampled and forced to be periodic; functions h(k) \nand the resulting convolution y(k) are also shown. Again, the first Q -\n1 \nsamples of the convolution function are deleted because of the end effect. \nThe final section of x(t) is shifted to the origin and sampled, as illus-\ntrated in Fig. 1O.7(c); discrete convolution results with the first Q - 1 sam-\nples deleted are also shown. \nEach of the discrete convolution sections of Figs. 1O.7(a) to (c) is re-\nconstructed in Figs. 1O.8(a) to (c), respectively. We have replaced the shift \nfrom the origin, which was removed for efficient convolution. Note that with \nthe exception of the holes created by the addition of these sectioned results, \nFig. 1O.8(d) approximates closely the desired continuous convolution of Fig. \n10.8(e). By simply overlapping the sections of xCt) by a duration (Q -\nl)T, \nwe can eliminate these holes entirely. \nOverlap-Save Sectioning \nIn Fig. 1O.9(a), we show the identical waveform x(t) of Fig. 10.6(a). \nHowever, note that the sections of x(t) are now overlapped by (Q -\nl)T, \nthe duration of the function h(t) minus T. \nWe shift each section of \nx(t) to the origin, sample the section, and form \nSec. 10.2 \nFFT Convolution of Infinite- and Finite-Duration Waveforms \n213 \nh(kl \nN \nI k \na-j N~ \nk \n(al \nx2(kl \nh(kl \n........................... \nN \nI k \na-j N~ \nk \n(bl \nx3(kl \nh(kl \nI---N---li k \n(el \nFigure 10.7 Discrete convolution of each section of Fig. 10.6(a). \na periodic function. Figures 10. 9(b) to (e) illustrate the discrete convolution \nresulting from each section. Note that as a result of the overlap, additional \nsections are necessary. The first Q -\n1 samples of each section are again \neliminated because of the end effect. \nAs illustrated in Fig. 10.10, we add each section of the discrete con-\nvolution. The appropriate shift is added to each section. We do not have \nholes as before because the end effect occurs for a duration of the convo-\nlution that was computed by the previous section. Combination of each of \nthe sections yields over the entire range the desired continuous convolution \n[Fig. 1O.6(c)]. The only end effect that cannot be compensated is the first \none, as illustrated. All illustrations have been scaled by the factor T for \nconvenience of comparison with continuous results. It remains to specify \nmathematically the relationships that have been developed graphically. \nRefer to Fig. 1O.9(a). Note that we choose the first section to be of \nduration NT. To use the FFT, we require that \nN = 2'1 \n'Y integer valued \n(10.8) \nand obviously, we require N > Q (the optimum choice of N is discussed \n214 \nFFT Convolution and Correlation \nChap. 10 \nk \n(a) \n(b) \nk \n(e) \n...â¢........ \n..................... \n............... \n. \n........................ \n. \n(d) \n(e) \nFigure 10.8 Reconstructed results of the discrete convolution of Fig. 10.7. \nlater). We form the sampled periodic function \nxJ(k) = x(kT) \nk = 0, 1, ... , N -\nand by means of the FFT compute \nN-J \nXJ(n) = .L x.(k)e-j2-rrnkIN \nk=O \n(10.9) \nNext, we take the Q sample values defining h(t) and assign zero to the \nSec. 10.2 \nFFT Convolution of Infinite- and Finite-Duration Waveforms \n215 \nD \nD I I \nD \nD D \n[ ~ \n(al \nh(kl \nf----N---li k \n(bl \nh(kl \nN \nI k \no-j N~ \nk \n(el \nx3(kl \nh(kl \n...................... \nN \nI k \n0--1 N~ \nk \n(dl \nx4(kl \nh(kl \nI------N---II k \nk \n(el \nFigure 10.9 Discrete convolution of overlapped sections of data. \n216 \nFFT Convolution and Correlation \nChap. 10 \n(a) \nk \n(b) \ni----N---l \n(e) \nk \n(d) \n..... \n. \n...\n..... . \n......................... \n(e) \nFigure 10.10 Reconstructed results of the discrete convolution of Fig. 10.9. \nremaining samples to form a periodic function with period N: \nh(k) = { ~(kT) \nk = 0, 1, ... , Q -\n1 \nk = Q, Q + 1, ... , N -\n1 \n(10.10) \nIf h(t) is not shifted to the origin, as illustrated in Fig. 10.6(b), then h(t) is \nfirst shifted to the origin and Eq. (10.10) is applied. Using the FFT, we \nSec. 10.2 \nFFT Convolution of Infinite- and Finite-Duration Waveforms \n217 \ncompute \nand then the product \nN-. \nH(n) = L h(k)e -j2-rrnkIN \nk=O \nY. (n) = X. (n)H(n) \nFinally, we compute the inverse discrete transform of Y. (n): \n1 N-. \ny.(k) = -\nL Y.(n)ej21TnkIN \nN n=O \n(10.1I) \n(10.12) \n(10.13) \nand because of the end effect, delete the first Q -\n1 samples of y(k): yeO), \ny(1), ... ,y(Q - 2). The remaining samples are identical to those illustrated \nin Fig. to.to(a) and should be saved for future combination. \nThe second section of xU), illustrated in Fig. to.9(a), is shifted to the \norigin and sampled: \nx2(k) = x[(k + [N - Q + I])T] \nk = 0, 1, ... , N -\n1 \n(10.14) \nEquations (I0.1I) through (10.13) are then repeated. From Eq. (10.11), the \nfrequency function H(n) has previously been determined and need not be \nrecomputed. Multiplication, as indicated in Eq. (10.12), and subsequent in-\nverse transformation, as indicated in Eq. (10.13), yield the waveform Y2(k), \nillustrated in Fig. to.to(b). Again, the first Q - 1 samples ofY2(k) are deleted \nbecause of the end effect. All remaining sectioned convolution results are \ndetermined similarly. \nThe method of combining the sectioned results is as illustrated in Fig. \nto.to(e): \ny(k) undefined \ny(k) = y.(k) \ny(k + N) = Y2(k + Q -\nI) \ny(k + 2N) = Y3(k + Q -\nI) \ny(k + 3N) = Y4(k + Q -\n1) \nk = 0, 1, ... , Q -\n2 \nk = Q -\n1, \nQ, ... , N -\n1 \nk = 0, 1, ... , N - Q \nk = 0, 1, ... , N - Q \nk = 0, 1, ... , N - Q \n(10.15) \nThe terms select-saving and overlap-save are given in the literature [2, 3] \nfor this technique of sectioning. \nOverlap-Add Sectioning \nAn alternate technique for sectioning has been termed the overlap-add \n[2, 3] method. Consider the illustrations of Fig. to.ll. We assume that the \n218 \nFFT Convolution and Correlation \nChap. 10 \nfinite-length function x(t) is of a duration such that the samples representing \nx(t) exceed the memory of our computer. As a result, we show the sections \nof length (N -\nQ)T, as illustrated in Fig. IO.ll(a). The desired convolution \nis illustrated in Fig. IO.ll(c). To implement this technique, we first sample \nthe first section of \nFig. IO.ll(a); these samples are illustrated in Fig. IO.12(a). \nThe samples are augmented with zeros to form one period of a periodic \nfunction. In particular, we choose N = 2'Y, N - Q samples of the function \nx(t): \nx.(k) = x(kT) \nand Q -\n1 zero values: \nk = 0, 1, ... , N - Q \nx.(k) = 0 \nk = N - Q + 1, ... , N -\n1 \n(10.16) \n(10.17) \nNote that the addition of Q -\n1 zeros ensures that there will be no end \neffect. Function h(t) is sampled to form a function h(k) with period N, as \nillustrated; the resulting convolution is also shown. \nThe second section of \nx(t), illustrated in Fig. IO.ll(a), is shifted to zero \nand then sampled: \nx2(k) = x[(k + N - Q + 1)11 \n=0 \nxltl \n'0 \nlal \nIbl \nk = 0, ... ,N - Q \nk = N - Q + 1, ... , N -\n(10.18) \nh \n~ \n\" \n. \nt \nleI \nFigure 10.11 \nExample illustrating proper sectioning for overlap-add discrete \nconvolution. \nSec. 10.2 \nFFT Convolution of Infinite- and Finite-Duration Waveforms \n219 \nxl lkl \nhlkl \nYl lkl \n.................................... \n................â¢.... \nN \nI k \no-j N~ \nk \nN~ \nk \n(al \nx21kl \nhlkl \nY21kl \n............................................ \nN \nI k \no-j N~ \nk \nN~ \nk \nIbl \nx31kl \nhlkl \nY31kl \n....................................... \n..... \n...... \nN \nI k \n0-j N~ \nk \nN~ \nk \n(el \nx41kl \nh(kl \nY4(kl \n........................................... \nN \nI k \no-j N~ \nk \n1-0.1 \nk \nN \nIdl \nFigure 10.12 Discrete convolution of each section of Fig. 10.11. \nAs before, we add Q -\nI zeros to the sampled function xU). Convolution \nwith h(k) yields the functionY2(k), as illustrated in Fig. 10.12(b). Convolution \nof each of the additional sequences is obtained similarly; the results are \nillustrated in Figs. 10. 12(c) and (d). \nWe now combine these sectioned convolution results, as illustrated in \nFig. 10.13. Each section has been shifted to the appropriate value. Note that \nthe resulting addition yields a replica of the desired convolution. The trick \nof \nthis technique is to add sufficient zeros to eliminate any end effects. These \nconvolution results are then overlapped and added at identically those sam-\nples where zeros were added. This gives rise to the term overlap-add. \n220 \nFFT Convolution and Correlation \nI\" \"~\":---l\"\"\"\"\"\"\"\"'\" \nk \n(al \n....... ..................... \n.. \nI----N------l \nk \n!\"\" \n................................................... \n(bl \n.......... \n.... \n. \nâ¢....... \n..... \nI----N------j \nk \n(e) \n(d) \n................... \n.....â¢â¢. \n(e) \n....... \n. \n... \n........ \n..... \nk \n. \n..... . \nk \nChap. 10 \nFigure 10.13 Reconstructed results of the discrete convolution of Fig. 10.12. \nComputational Efficiency of FFT Sectioned \nConvolution \nIn both of the sectioning techniques described, the choice of N seems \nto be rather arbitrary as long as N = 2'1. This choice determines the number \nof sections that must be computed, and thus the computing time. If an M-\n1. Refer to Figs. 10.9 and 10.10 for a graphical interpretation \nof the algorithm. \n2. Let Q be the number of samples representing h(t). \n3. Choose N according to Table 10.2. \n4. Form the sampled periodic function h(k): \nh(k) = h(kT) \n=0 \nk = 0,1, ... , Q -\n1 \nk = Q, Q + 1, ... , N -\n1 \n5. Compute the FFT of h(k): \nN-1 \nH(n) = L h(k)e- i2'1fnklN \nk~O \n6. Form the sampled periodic function: \nx;(k) = x(kT) \nk = 0,1, ... , N -\n1 \n7. Compute the FFT of x;(k): \nN-1 \nX;(n) = L x;(k)e- i2'1fnklN \nk~O \n8. Compute the product \nY;(n) = X;(n)H(n) \n9. Compute the inverse FFT of Y;(n) (note scaling by 1/N): \ny;(k) = N~1 (~ \nYi(n))e- i2'1fnklN \nn~O N \n10. Delete samples y;(O), y;(1), ... , y;( Q -\n2), and save the \nremaining samples. \n11. Repeat Steps 6 to 10 until all sections are computed. \n12. Combine the sectioned results by the relationships \ny( k) undefined \ny(k) = Y1(k) \ny(k + N) = Y2(k + Q -\n1) \ny(k + 2N) = Y3(k + Q -\n1) \nk = 0,1, ... , Q -\n2 \nk = Q -\n1, Q, ... , N -\n1 \nk = 0,1, ... , N -\nQ \nk = 0, 1, ... , N -\nQ \n13. Scale the results by the sample interval T. \nFigure 10.14 Computation procedure for FFT convolution: select-savings method. \n221 \n222 \n1. Refer to Figs. 10.12 and 10.13 for a graphical interpretation \nof the algorithm. \n2. Let Q be the number of samples representing h(t). \n3. Choose N according to Table 10.2. \n4. Form the sampled periodic function h(k): \nh(k) = h(kT) \nk = 0.1 ..... Q -\n1 \n=0 \nk = Q. Q + 1 \n..... N -\n1 \n5. Compute the FFT of h(k): \nN-1 \nH(n) = L h(k)e-i27rnkiN \nk~O \n6. Form the sampled periodic function: \nXi( k) = x( kT) \n= 0 \nk = 0.1 ..... N -\nQ \nk= N- Q+ 1 \n..... N-1 \n7. Compute the FFT of xj(k): \nN-1 \nX;(n) = L x;(k)e-i27rnkiN \nk=O \n8. Compute the product \nY;(n) = X;(n)H(n) \n9. Compute the inverse FFT of Y;(n) (note scaling by 1/N): \ny;(k) = N\ni' \n(~ \nYi(n))e-i27rnkiN \nn~O N \n10. Repeat Steps 6 to 9 until all sections are computed. \n11. Combine the sectioned results by the relationships \ny(k) = Y1(k) \nk = 0.1 ..... N -\nQ \ny( k + N -\nQ + 1) = Y1 (k + N -\nQ + 1) + Y2( k) \nk = 0.1 ..... N -\nQ \ny[k + 2(N -\nQ + 1)] = h(k + N -\nQ + 1) + Y3(k) \nk = 0,1, .... N -\nQ \n12. Scale the results by sample interval T. \nFigure 10.15 Computation procedure for FFT convolution: overlap-add method. \nSec. 10.3 \nEfficient FFT Convolution \n223 \npoint convolution is desired, then approximately M/(N - Q + 1) sections \nmust be computed. If \nit is assumed that M is sufficiently greater than N -\nQ + 1, then the time required to compute H(n) via the FFT can be ignored. \nEach section requires a forward and an inverse transform; hence, the FFT \nmust be repeated approximately 2M/(N - Q + 1) times. We have exper-\nimentally determined the optimum value of \nN; the results of \nthis investigation \nare given in Table 10.2. One can depart from the values of N shown without \ngreatly increasing the computing time. \nTABLE 10.2 Optimum \nValue of N for FFT \nConvolution \nQ \nN = 2\" \n:s10 \n32 \n11- 19 \n64 \n20- 29 \n128 \n30- 49 \n256 \n50- 99 \n512 \n100-199 \n1024 \n200-299 \n2048 \n300-599 \n4096 \n600-999 \n8192 \nWe describe the step-by-step computational procedure for the select \nsaving and the overlap-add methods of sectioning in Figs. 10.14 and 10.15, \nrespectively. Both algorithms are approximately equivalent with respect to \ncomputational efficiency. \nIf \nthe functions x(t) and h(t) are real, then we can use additional tech-\nniques to more efficiently compute the FFT. In the next section, we describe \nexactly how this is accomplished. \n10.3 EFFICIENT FFT CONVOLUTION \nWe have to this point in the discussion considered that the functions being \nconvolved are real functions of time. As a result, we have not utilized the \nfull capabilities of the FFT. In particular, the FFT algorithm is designed for \ncomplex input functions; thus, if we only consider real functions, then the \nimaginary part of the algorithm is wasted. In this section, we describe how \nto divide a single real waveform into two parts, calling one part real, one \npart imaginary, and how to compute the convolution in one-half the normal \nFFT computing time. Alternately, our technique can be used to convolve \ntwo signals with an identical function simultaneously. \n224 \nFFT Convolution and Correlation \nChap. 10 \nConsider the real periodic sampled functions g(k) and s(k). It is desired \nto convolve simultaneously these two functions with the real function h(k) \nby means of the FFT. We accomplish this task by applying the technique \nof efficient discrete transforms, which was discussed in Sec. 9.3. First, we \ncompute the discrete Fourier transform of h(k), setting the imaginary part \nof h(k) to zero: \nN-\\ \nH(n) = L h(k)e -j27rnkIN \n= Hr(n) + jHi(n) \nNext, we form the complex function \np(k) = g(k) + js(k) \nk = 0, 1, ... , N -\n1 \nand compute \nN-\\ \nP(n) = L p(k)e -j27rnkIN \n= R(n) + j/(n) \nUsing the discrete convolution theorem, Eq. (6.50), we compute \n(10.19) \n(10.20) \n(10.21) \n1 N-I \n. \ny(k) = Yr(k) + jYi(k) = p(k) * h(k) = - L P(n)H(n)eJ27rnkIN \nN k~O \n(10.22) \nFrom Eqs. (9.6) and (9.7), the frequency function P(n) can be expressed as \nP(n) = R(n) + j1(n) \nwhere \n= [RAn) + Ro(n)] + j[le(n) + lo(n)] \n= G(n) + jS(n) \nG(n) = Re(n) + jlo(n) \nS(n) = I An) -\njRo(n) \nProduct P(n)H(n) is then given by \nP(n)H(n) = G(n)H(n) + jS(n)H(n) \nand thus the inversion formula yields \n1 N-I \n. \ny(k) = Yr(k) + jYi(k) = - L P(n)H(n)eJ27rnkIN \nN n~O \n(10.23) \n(10.24) \n(10.25) \n(10.26) \nSec. 10.4 \nFFT Correlation of Finite-Duration Waveforms \n225 \nwhere \n1 N-I \nYr(k) = - L G(n)H(n)ej2-rrnkIN \nN k~O \n1 N-I \n. \njYi(k) = -\nL jS(n)H(n)eJ2-rrnkIN \nN k~O \n(10.27) \nwhich is the desired result. That is, Yr(k) is the convolution of g(k) and h(k), \nand Yi(k) is the convolution of s(k) and h(k). If g(k) and s(k) represent \nsuccessive sections, as described in the previous section, then we have re-\nduced the computing time by a factor of two by using this technique. One \nstill must combine the results as appropriate for the method of sectioning \nbeing employed. \nNow consider the case where it is desired to perform the discrete con-\nvolution of \nx(k) and h(k) in one-half the time by using the imaginary part of \nthe complex time function, as discussed in Sec. 9.3. Assume x(k) is described \nby 2N points; define \ng(k) = x(2k) \nk = 0, I, ... , N -\n1 \ns(k) = x(2k + I) \nk = 0, I, ... , N -\n1 \nand let \np(k) = g(k) + js(k) \nk = 0, I, ... , N -\n1 \nBut Eq. (10.29) is identical to Eq. (10.20); therefore, \n1 N-I \n. \nz(k) = zr(k) + jZi(k) = -\nL P(n)H(n)eJ 2-rrnkIN \nN n~O \nwhere the desired convolution y(k) is given by \ny(2k) = zr(k) \ny(2k + I) = zi(k) \nk = 0, I, ... , N -\n1 \nk = 0, I, ... , N -\n1 \n(10.28) \n(10.29) \n(10.30) \nAs in the previous method, we must still combine the results as appropriate \nfor the method of sectioning being considered. \n10.4 FFT CORRELATION OF FINITEÂ·DURATION \nWAVEFORMS \nApplication of the FFT to discrete correlation is very similar to FFT con-\nvolution. As a result, our discussion on correlation will only point out the \ndifferences in the two techniques. \n226 \nFFT Convolution and Correlation \nChap. 10 \nConsider the discrete correlation relationship \nN-] \nz(k) = L hU)x(k + i) \n(10.31) \n;=0 \nwhere both x(k) and h(k) are periodic functions with period N. Figure \n1O.16(a) illustrates the same periodic functions x(k) and h(k) considered in \nFig. 1O.1(b). Correlation of these two functions according to Eq. (10.31) is \nshown in Fig. 1O.16(b). Scaling factor T has been introduced for ease of \ncomparison with continuous results. Note from Fig. 1O.16(b) that the shift \nfrom the origin of the resultant correlation function is given by the difference \nbetween the leading edge of x(k) and the trailing edge of h(k). Recall that a \npositive shift for h(k) is to the left. \nIn convolution, either function can be folded and shifted. The results \nare unchanged. This is not the case for correlation. Figure 1O.16(c) illustrates \nthe correlation function resulting from the shift x(k) rather than h(k). Note \nthat the results give the same waveform but the waveform is shifted to the \nright by a -\nd in Fig. 1O.16(b) and shifted to the left by a -\nd in Fig. \n10.16(c). Care should be exercised in interpreting the correlation results of \nFig. 1O.16(c) to ensure that the correct shift from the origin has been de-\ntermined. As in our convolution example, the correlation computation il-\nlustrated in Fig. 1O.16(b) is inefficient because of the number of zeros in-\ncluded in the N points defining one period of the periodic correlation \nfunction. Restructuring of the data is again the solution we choose for ef-\nficient computation. \nIf we shift both functions to the origin, as shown in Fig. 1O.17(a), then \nx(k) \nh(k) \nb \nd \nk \nI----N---li \nk \n\\---N---j \n(8) \nz(k) = T ~ \nxli) h(k+i) \nz(k) = T~h(i) \nx(k+i) \n... \n.............................. \na-d \nk \nb \nI----N-----i \n(b) \n(e) \nFigure 10.16 Example illustrating inefficient discrete correlation. \nSec. 10.4 \nFFT Correlation of Finite-Duration Waveforms \nxlk) \n1Â·Â·Â· \n.. \nÂ·Â·Â·Â·Â·Â·Â·Â· \nla) \nlZlk) \nÂ·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·b~-~sÂ·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â· \nIb) \nxlk) \nIe) \nN - 1 \nzlk) = T \n~ h(i)xlk+i) \ni = 0 \nId) \nhlk) \n! ........... \n. \n2 \nk \nk \nFigure 10.17 Discrete correlation of restructured data. \n227 \nk \nthe resulting correlation is as iIIustrated in Fig. lO.17(b). Although the cor-\nrelation waveform is correct, it must be unraveled before it is meaningful. \nWe can remedy this situation by restructuring the waveform x(k), as shown \nin Fig. lO.l7(c). For this condition, the resulting correlation waveform is as \nillustrated in Fig. lO.17(d). This is the desired waveform with the exception \nof a known time shift. \nTo apply the FFT to the computation of Eq. (10.31), we choose the \nperiod N to satisfy the relationships \nN:2:.P+Q-l \n(10.32) \nN = 2\"Y \n'Y integer valued \n228 \nFFT Convolution and Correlation \nChap. 10 \n1. Let x(t) and h(t) be finite-length functions shifted from the \norigin by a and b, respectively. \n2. Let P be the number of samples defining x(t) and Q be the \nnumber of samples defining h(t). \n3. Choose N to satisfy the relationships \nN?:.P+Q-1 \nN = 2\"1 \n\"y integer valued \n4. Define x(k) and h(k) as follows: \nx(k) = 0 \nx(k) = x(kT + a) \nh(k) = h(kT + b) \nh(k) = 0 \nk = 0, 1, ... , N -\nP \nk=N-P+1, \nN -\nP + 2, ... , N -\n1 \nk = 0,1, ... , Q -\n1 \nk = Q, Q + 1, \n... , N -\n1 \n5. Compute the FFT x(k) and h(k): \nN-1 \nX(n) = L x(k)e-j27rnkiN \nk~O \nN \nH(n) = L h(k)e-j27rnkiN \nk=O \n6. Change the sign of the imaginary part of H(n) to obtain H*(n). \n7. Compute the product \nZen) = X(n)H*(n) \n8. Compute the inverse FFT using the forward FFT: (note scal-\ning by 1/N): \nz(k) = N~1 (~Z*(n)) e-j27rnkiN \nn~O N \n9. Scale the results by sample interval T. \nFigure 10.18 Computation procedure for FFT correlation of finite-length functions. \nChap. 10 \nProblems \n229 \nWe shift and sample x(t) as follows: \nx(k) = 0 \nk = 0, 1, ... , N - P \nx(k) = x[kT + a] \nk = N -\nP + 1, N -\nP + 2, ... ,N -\n1 \n(10.33) \nThat is, we shift the P samples of \nx(k) to the extreme right of the N samples \ndefining a period. Function h(t) is shifted and sampled according to the \nrelations \nh(k) = h(kT + b) \nh(k) = 0 \nk = 0, 1, ... , Q -\n1 \nk = Q, Q + 1, ... , N -\n(10.34) \nBased on the discrete correlation theorem, Eq. (7.13), we compute the \nfollowing: \nN-] \nX(n) = L x(k)e -j2-rrnkIN \nk~O \nN-J \nH(n) = L h(k)e-j2-rrnkIN \nk=O \nZ(n) = X(n)H*(n) \n1 N-J \nz(k) = - L Z(n)ej2-rrnkIN \nN n~O \nThe resulting z(k) is identical to the illustration of Fig. 10.17(d). \n(10.35) \n(10.36) \n(10.37) \n(10.38) \nComputing times of Eqs. (10.35) through (10.38) are essentially the \nsame as the convolution Eqs. (10.4) through (10.7) and the results of the \nprevious section are applicable. The computations leading to Eq. 00.38) are \noutlined in Fig. 10.18 for easy reference. \nThe key to carrying one's knowledge of FFT convolution techniques \nto FFT correlation is to remember that in correlation there is no folding \noperation and that a shift to the left is positive. This latter factor is probably \nresponsible for the majority of \nerrors in interpreting FFT correlation results. \nPROBLEMS \n10.1 Given the functions h(l) and X(l) illustrated in Fig. 10.19, determine the opti-\nmum choice of N to eliminate overlap effects during convolution and corre-\nlation. Assume a sample period of T = 0.1 and a base-2 FFT algorithm. Graph-\nically show how to restructure the data for efficient convolution computation. \n10.2 Consider the functions x(l) and h(l) of Fig. 10.19. Graphically show how to \napply the overlap-save and overlap-add sectioning techniques for computing \nthe convolution of x(l) and her). \n230 \nFFT Convolution and Correlation \nChap. 10 \nh{l) \n2 \n3 \n4 \n5 \n6 \n2 \n3 \n(a) \nX{I) \nh{t) \n2 \n3 \n4 \n5 \n6 \n2 \n3 \n(b) \nX(I) \nhit) \n2 \n3 \n4 \n5 \n6 \n2 \n(e) \nFigure 10.19 Functions for Probs. 10.1 to lOA. \n10.3 Repeat Problem 10.2 for the correlation of x(t) and h(t). \n10.4 Repeat Problem 10.3 for the functions x(t) and h(t) illustrated in Fig. 10.6. \n10.5 Use the FFT to duplicate the results shown in Figs. 10.7, 10.9, 10.10, 10.12, \nand 10.13. Apply the efficient FFT convolution techniques described in Sec. \nlOA. \n10.6 Develop graphically the overlap-save and overlap-add sectioning techniques \nfor discrete correlation. \n10.7 Repeat Problem 10.5 for the case of correlation of the two waveforms. \nREFERENCES \nI. COOLEY, J. W., P. A. W. LEWIS, and P. D. WELCH. \"Application of the Fast \nFourier Transform to Computation of Fourier Integrals, Fourier Series, and Con-\nvolution Integrals.\" IEEE Trans. Audio and Electroacoust. (June 1967), Vol. \nAU-15, No.2, pp. 79-84. \n2. HELMS, H. D. \"Fast Fourier Transform Method of Computing Difference Equa-\ntions and Simulating Filters.\" IEEE Trans. Audio and Electroacoust. (June \n1967), Vol. AU-15, No.2, pp. 85-90. \n3. STOCKHAM, T. G. \"High-Speed Convolution and Correlation.\" AFJPS Proc. \nChap. 10 \nReferences \n231 \n(1966 Spring Joint Computer Conf.), Vol. 28, pp. 229-233. Washington, DC: \nSpartan. \n4. GENTLEMAN, W. M., and G. SANDE. \"Fast Fourier Transforms for Fun and \nProfit.\" AFlPS Proc. (1966 Spring Joint Computer Conf.), Vol. 29, pp. 563-578, \nWashington, DC: Spartan. \n5. COOLEY, J. W., P. A. W. LEWIS, and P. D. WELCH. \"The Finite Fourier Trans-\nform.\" IEEE Trans. Audio and Electroacoust. (June 1969), Vol. AU-17, No.2, \npp.77-85. \n6. AGARWAL, R. c., and J. W. COOLEY. \"New Algorithms for Digital Convolution.\" \nIEEE Trans. Acoust. Speech Sig. Proc. (October 1977), Vol. ASSP-25, No.5, \npp. 392-410. \n7. BORGIOLI, R. C. \"Fast Fourier Transform Correlation versus Direct Discrete \nTime Correlation.\" Proc. IEEE (September 1968), Vol. 56, No.9, pp. 1602-\n1604. \n8. NUSSBAUMER, H. J. Fast Fourier Transforms and Convolution Algorithms. New \nYork: Springer-Verlag, 1982. \n1 \n1 \nTWO-DIMENSIONAL \nFFT ANALYSIS \nIn previous chapters, we applied the FFT to the analysis and processing of \none-dimensional waveforms. Many of the techniques, procedures, and con-\ncepts discussed can be readily extended to two-dimensional FFT signal pro-\ncessing. A two-dimensional signal is a function h(x \n,y) of two variables x and \ny. Two-dimensional FFTs are of considerable computational importance in \nthe digital processing of two-dimensional waveforms such as images, geo-\nphysical arrays, gravity and magnetic data, and antenna analysis. Our ap-\nproach is to develop the fundamental principles on which these applications \nof the FFT are based. \nWe will discuss in this chapter the concepts and techniques for applying \nthe FFT to two-dimensional forward and inverse Fourier transforms. Ap-\nplications of the FFT to two-dimensional convolution and correlation inte-\ngrals are also addressed. As we will see, these applications are an extension \nof the previously developed one-dimensional case. However, because the \ntwo-dimensional Fourier transform is generally a less familiar analysis tool \nthan the one-dimensional transform, we have chosen to develop our results \nfrom two-dimensional definitions rather than generalizing one-dimensional \nresults. \n11.1 TWOÂ·DIMENSIONAL FOURIER TRANSFORMS \nA two-dimensional function h(x,y) has a two-dimensional Fourier transform \nH(u,v) given by \n232 \nSec. 11.1 \nTwo-Dimensional Fourier Transforms \n233 \nH(u,v) == f_oooo f_oox h(x,y)e -j27r(UX + vy) dx dy \n01.1) \nAnalogous to the one-dimensional case, Eq. 01.1) describes the analysis of \nthe two-dimensional function h(x,y) into components of the form COS[21T(UX \n+ vy)] and sin[21T(ux + vy)]. \nAn example of a two-dimensional waveform is illustrated in Fig. \n11.1(a). The function shown represents a cosinusoidally corrugated two-\ndimensional surface. If a section is made through the corrugation in the y-\nh plane, the sectioned function oscillates with a frequency of Vo cycles per \nunit of y (i.e., analogous to cycles per second). To distinguish between fre-\nquencies associated with functions of time and functions of length, the terms \ntemporal and spatial are used, respectively. The two-dimensional Fourier \ntransform of Fig. 11.1(a) as determined from Eq. 01.1) is the pair of impulse \nfunctions shown in Fig. 11.1(b). \nThe concept of a two-dimensional waveform is further illustrated in \nFig. 11.2(a). For this example, a section is made through the corrugations \nin the x-h plane. The waveform oscillates with a spatial frequency of \nVo sineS) \ncycles per unit of x. Similarily, a section made through the corrugations in \nthe y-h plane oscillates with a frequency of Vo cos(S) cycles per unit of y. \nFigure 11.2(a) is simply that of Fig. 11.1(a) rotated through an angle S. \nThe two-dimensional Fourier transform of Fig. 11.2(a) is illustrated in \nFig. 11.2(b). As shown, the spacial frequency at which the corrugation os-\ncillates in a section perpendicular to the lines of zero phase is given by \n[vo cos2(S) + Vo sin2(S)]'/2 = Vo. Note that the frequency impulse functions \nare located on an axis rotated through an angle S with respect to the results \nof Fig. 11.I(b). A comparison of Figs. 11.1 and 11.2 shows that if a function \nh(x,y) is rotated through an angle S, then its two-dimensional Fourier trans-\nform is also rotated through an angle S. \nExample 11.1 Two-Dimensional Pulse Waveform \nFind the two-dimensional Fourier transform of the function illustrated in Fig. 11.3(a). \nFrom Fig. 11.3, \nh(x,y) = 1 \n-l<x<I;-l<y<1 \n= 0 \notherwise \nSubstitution of Eq. (11.2) into Eq. (11.1) yields \nH(u,v) = J' e-j27rvy dy J' e-j27rux dx \n-, \n-, \n= f, \n[COS(21TVY) - j sin(21Tvy)] dy \nx L'( [COS(21TUX) - j sin(2'lTux)] dx \n(11.2) \n(11.3) \nBecause the sin(~) term integrates to zero over the interval ( - 1,1), then integration \nN \nCo) \n.,.. \nh(x,Y) = cos (21TUoY) \nH(u,v) \nVo \nv \n(a) \n(b) \nFigure 11.1 Two-dimensional Fourier transform of a co \nsinusoidally corrugated surface, \nu \ntj \nU1 \nH(u.vl \nh(x.yl: cosl2m \nvoY costsl + vOl- sio(811 \n~ \n~/ \n/ --\n--Ie \n/ \nv \n~~\\ \n\",,~f...o \nv \n/ \n/ \n/ \n/ \n/ \nI \n/ \n(hI \nVos;0(8) \nu \n(al \n..... \n\" \\1.' Tw~dim~'iom\" \nFouri\" treD,fo,m of \", \n.\",tU,,\\<, ,url'\" of figÂ· II. \nt \"'\"\". \na \ndegrees. \n236 \nTwo-Dimensional FFT Analysis \nChap. 11 \nh(x.V) \nv \n(a) \nH(u.v) \nIb) \nFigure 1l.3 Two-dimensional pulse-waveform Fourier transform pair. \nSec. 11.1 \nTwo-Dimensional Fourier Transforms \nof Eq. (11.3) yields \nsin(2-rru) II \nH(u,v) = \ncos(2'ITvy) dy \n'lTU \n- I \nsin(2'ITu) sin(2'ITv) \n'lT2UV \nFigure 11.3(b) illustrates this two-dimensional Fourier transform result. \nExample ll.2 Two-Dimensional Fourier Transform: Separable Functions \nFind the two-dimensional Fourier transform of the function \nh(x,y) = cos(2'ITuox) cos(2'ITvoY) \nFrom Eq. (11.1), \nH(u,v) = L\"'\", f:\", cos(2'ITuoX) cos(2'ITvoy)e -j2Tr(ux + vy) dx dy \n= f-\"'\", cos(2'ITvoy)e -j2.rt'y dy f-\"'\", cos(2'ITuox)e -j2TrUX dx \n= Y2 f-\"'\", (ej2TrVOY + e -j2TrVOY)e -j2Trvy dy \nx [Y2 f-\"'\", (ej2TrUOX + e - j2TrUOX)e - j2Trux dx ] \n= Yzf_\"'\", (e-j2TrY(1'-VO) + e-j2TrY(V+VO) dy \n+ Y2f_oo\", {e-j2Trx[u-uoJ + e-j2TrX[u+uOJ} dx \n= Y2 8(u,v -\nvo) + \n1/28(u,v + vo) \n+ 1/28(u -\nuo,v) + \n1/28(u + Uo,v) \n237 \n(11.4) \n(11.5) \n(11.6) \nThe relationship of Eq. (11.5) is termed a separable function in that its two-\ndimensional Fourier transform can be computed as the product of two single-variable \nintegrals. Note that the function given in Eq. (11.2) is also separable. \nOne-Dimensional Interpretation of Two-Dimensional \nTransforms \nThe two-dimensional Fourier transform H(u,v) can be viewed as two \nsuccessive one-dimensional transforms. To develop this viewpoint, we first \nrewrite Eq. (11.1) as \nH(u,v) = J:oo e-j2-rrvy [J:\", h(x,y)e-j2-rrux dxJ dy \n(11.7) \nNote that the term in brackets is simply the one-dimensional Fourier trans-\nform of h(x,y) with respect to x, that is, \nZ(u,y) = f:\", h(x,y)e -j2-rrux dx \n(11.8) \n238 \nTwo-Dimensional FFT Analysis \nChap. 11 \nEquation (11.7) can then be rewritten as \nH(u,v) = J:\"\" Z(u,y)e -j2-rrvy dy \n(11.9) \nwhere we have substituted Z(u,y) for the term in brackets in Eq. (11.7). An \nexamination of Eq. (11.9) reveals that H(u,v) is the one-dimensional trans-\nform of Z(u,y) with respect to y. Hence, the two-dimensional Fourier trans-\nform H(u,v) can be interpreted as the two successive one-dimensional trans-\nforms given by Eqs. (11.8) and (11.9). \nAnalytical evaluation of \na two-dimensional integral can be implemented \nby simply determining the two successive one-dimensional integrals of Eqs. \n(11.8) and (11.9). These single-dimension integrals are evaluated exactly by \nthe procedures followed in the one-dimensional Fourier transform case. \nHence, the two-dimensional Fourier transform can be evaluated using the \nmethods and fundamentals developed in previous discussions. As we will \nsee, this interpretation is of considerable importance in applying the FFT \nto the computation of two-dimensional Fourier transforms. \nExample 11.3 Two-Dimensional Fourier Transforms: Successive \nOne-Dimensional Transforms \nIn Fig. 11.2, we illustrated the two-dimensional Fourier transform of the function \nh(x,y) = cos{21r[voY cos(6) + VOX sin(6)]} \n(11.10) \nTo compute the transform analytically, we substitute Eq. (11.10) into Eq. (11.1): \nH(u,v) = J:\"\" J:\", COS{21T[VoY cos(6) \n+ VOX sin(6)]}e -j2'T1\"(ux+vy) dx dy \n(11.11) \nWe next apply the principles developed in Eqs. (11.8) and (11.9) by rewriting Eq. \n(11.12) as two successive single-dimension Fourier transforms: \nH(u,v) = I-~~ e-j2'ffVY (I-oo~ COS{21T[VoY cos(6) \n+ VOX sin(6)]}e -j2'ffIiX dX) dy \n= 1/2 f-~~ e - j2'ffvy [f-~oo (e j2'ff[VOY cos(8) + vox sin(8)J)e - j2'ffIiX dx \n+ f-~~ (e - j2'ff[VOY cos(8) + vox sin(8)J)e - j2'ffIiX dxJ dy \n= 1/2 \nf-~oo e -j2'ffvy [f_OOoo (e j2'ff[VOY COS(8)-X(II-vo)sin(8)J) dx \n+ f_oooo (e -j2'ff[vOY COS(8)+X(II+vo)sin(8)J) dXJ dy \nSec. 11.1 \nTwo-Dimensional Fourier Transforms \n= 1/2 f-\"'\", e -j2TrUY (ej2TrVOY cosIO) f-\"'\", e \n-j2Trx(u-vo)sin(0) dx \n+ e - j 2TrvOY cosIO) f-\"'\", e - j2Trx(u + vo) sin(O) dX) dy \n= \nVz f-\"'\", e -J2TrvY{ej2TrVOY cosIO) 8[u -\nVo sin(8),y] \n+ e - j2Trvoy cosIO) 8[u + Vo sin(8),y]} dy \n= 1/2 f-\"'\", 8[u -\nVo sin(8),y]e -j2TrY(v-vo cos(O)1 dy \n+ \n1/2 f-\"'\", 8[u + Vo sin(8),y]e -j2TrY(v-vo cos(O)1 dy \n= Y2 8[u -\nVo sin(8),v -\nVo cos(8)] \n+ \n1/2 8[u + Vo sin(8),v + Vo cos(8)] \n239 \n(11.12) \nThe two-dimensional frequency function of Eq. (11.12) is illustrated in Fig. 11.2(b). \nInverse Fourier Transform \nThe two-dimensional inverse Fourier transform is given by \nh(x,y) = f:oo f:oo H(u,v)eJ2-rr(ux+vy) du dv \n(11.13) \nAnalogous to the one-dimensional inverse Fourier transform, Eq. (11.13) \nimplies that corrugations of appropriate frequencies, orientations, phases, \nand amplitudes can be summed to produce the original two-dimensional \nwaveform. However, it is recognized that the two-dimensional inverse Four-\nier transform is much more difficult to visualize than the one-dimensional \ntransform. \nExample 11.4 Two-Dimensional Inverse Fourier Transform \nFind the inverse two-dimensional Fourier transform of the frequency function \nFrom Eq. (11.13), \nH(u,v) = n \n= 0 \n-a ::5 U ::5 a, -b ::5 v ::5 b \notherwise \nh(x,y) = L\"'\", L\"'\", nej2Tr(ux+vy) du dv \n= n f:b ej2-rruy dv [fa eJ2TrUX dUJ \n= n f:b cos(21rvy) dv [f~a \nCOS(21TUX) duJ \n= n [sin~;bY) \n] [ sin~;aX) \n] \n(11.14) \n(lLl5) \n240 \nTwo-Dimensional FFT Analysis \nChap. 11 \nExample 11.4 demonstrates the property that if the frequency function \nH(u,v) can be decomposed into a product of a function of the variable u and \na function of the variable v, then the function h(x \n,y) can be decomposed into \na product of a function of the variable x and a function of the variable y. \nSummary \nWe normally find two-dimensional Fourier transform relationships \nmuch more difficult to picture than one-dimensional relationships. This gen-\nerally follows from the emphasis of one's formal training and experience in \nthe analysis and synthesis of single-dimension functions. As with any new \narea of study, a thorough and fundamental understanding comes only with \nconsiderable exposure and practical experience. The previously developed \nbasic principles should form the foundation for such an investigative en-\ndeavor. A comprehensive treatment of two-dimensional Fourier transform \nproperties is given in Ref. [I]. \n11.2 TWO-DIMENSIONAL FFTs \nRecall from the development of Eqs. (11.8) and (11.9) and Ex. 11.3 that the \ntwo-dimensional Fourier transform can be written as two successive single-\ndimension Fourier transforms. This interpretation of the two-dimensional \ntransform is also readily seen in the two-dimensional discrete Fourier trans-\nform. We assume that the two-dimensional function h(x,y) has been sampled \nin the x dimension with sample interval Tx and sampled in the y dimension \nwith sample interval Ty. The resulting sampled function is h(pTx,qTy), where \np = 0, I, ... , N -\nI and q = 0, I, ... , M -\nI. \nAnalytical Development \nAnalogous to the one-dimensional case, the two-dimensional discrete \nFourier transform is defined as \nH(nINTx,mIMTy) = :~~ [:~~ h(PTx,qTy)e-j2-rrnPINJ e-j2-rrmqIM \np = 0, I, ... ,N -\nn = 0, I, ... ,N -\nq = 0, I, ... ,M -\nI \nm = 0, I, ... , M -\nI \n(l1.l6) \nNote that the term in brackets is simply a one-dimensional discrete \nFourier transform along the data array defined by parameter p. To evaluate \nthe term in brackets, we compute M one-dimensional transforms: one for \nSec. 11.2 \nTwo-Dimensional FFTs \n241 \neach q, where q = 0, I, ... , M -\nI, along the data array defined by p. If \nwe call each of these FFT results Z(nINTx,qTy), then Eq. (l1.l6) can be \nrewritten as \nM-I \nH(nINTx,mIMTy) = L Z(nINTx,qTy)e -j2TrmqlM \n(l1.l7) \nq=O \nEquatioll (11.17) is evaluated by N one-dimensional discrete Fourier trans-\nforms, each along the data array defined by parameter q. As shown ana-\nIytically, the two-dimensional discrete Fourier transform can be imple-\nmented straightforwardly by computing one-dimensional discrete Fourier \ntransforms: first on the function h(pTx,qTy), where p = 0, I, ... , N -\n1 \nfor each q; and then a second one-dimensional transform on the function \nZ(nINTx,qTy), where q = 0, I, ... , M -\n1 for each n = 0, I, ... , N -\n1. Equations (l1.l6) and (l1.l7) must be multiplied by the scale factor TxTy \nto obtain equivalence between the continuous and discrete transforms. \nGraphical Development \nTo further illustrate the one-dimensional computation of a two-dimen-\nsional Fourier transform, consider Fig. 1l.4(a). As illustrated, we interpret \nthe sampled data of the two-dimensional waveform as a data matrix with M \n= 8 rows, where q = 0, I, ... , M -\nI, and N = 8 columns, where p = \n0, I, . . . , N -\nl. Because the terms in brackets in Eq. (l1.l6) sum on the \nparameter p, then this summation corresponds to computing the one-di-\nmensional discrete Fourier transform for each row of data, that is, a trans-\nform is computed for each q = 0, I, ... , M -\nI. \nThe discrete Fourier transform or FFT of row Â° \nis a frequency function \ndefined by all zero values because the values of the sampled function rep-\nresented by row Â° \nis a zero-valued function. In Fig. 11.4(b), row 0, we plot \nthis FFT result. The sampled function defined by row 1 is also zero and, \ncorrespondingly, the FFT computed for this row is a zero-valued function, \nas shown in Fig. 1 \n1.4(b) , row l. \nNow consider the sample values of \nrow 2. These samples define a pulse \nor rectangular waveform that has a I \n[sin(f)]lf I \ntransform. The magnitude \nof the FFT of row 2 is the frequency function illustrated in Fig. 11.4(b), row \n2. Note that we display the FFT results in the standard one-dimensional \nformat, that is, the first NI2 values represent positive frequency results and \nthe remaining values represent negative frequency results. The sample val-\nues of rows 3 to 5 in Fig. 11.4(a) also define a pulse waveform and hence, \nthe magnitude of the FFT of each of these rows is the I \n[sin(f)]lf I \nfunction \nshown in Fig. 1 \nl.4(b) , rows 3 to 5. Rows 6 and 7 of Fig. 11.4(a) are zero-\nvalued; rows 6 and 7 of Fig. 11.4(b) are then zero-valued. \nTo this point, we have computed the FFT of the sampled matrix of \nFig. 11.4(a) for each row. The complex data matrix represented by the mag-\n242 \nTwo-Dimensional FFT Analysis \nChap. 11 \nFigure 11.4 Graphical development of the two-dimensional FFf as a sequence \nof one-dimensional transforms. \nSec. 11.2 \nTwo-Dimensional FFTs \n243 \nnitude function shown in Fig. 11.4(b) corresponds to computing the terms \nin brackets in Eq. (11.16) for each value of q. That is, we have effectively \nset q = 0 and computed an FFT over the p = 0, I, ... , N -\n1 sample \nvalues; set q = 1 and computed the FFT over the p = 0, I, ... N -\n1 \nsample values; etc. Next, we proceed to compute the outer sum of Eq. \n(11.16). Note that this summation is on q, the row data values of Fig. 11.4(b) \nfor each n = 0, I, ... , N -\nI. Hence, we compute the FFT of the complex \nsample values of each column of the matrix. \nIn Fig. 11.4(b), the sample values of column 0 define a pulse waveform. \nThe FFT of this waveform is then the I \n[sin(f)]/f I \nfunction illustrated in \nmagnitude form in Fig. 11.4(c), column o. As before, we display the results \nin the standard one-dimensional FFT format, where the first M/2 values \nrepresent positive frequency results and the remaining values are negative \nfrequency results. \nObserve that the sampled functions defined by each nonzero column \nof Fig. 11.4(b) are of the same form, a pulse waveform with differing am-\nplitude. Our input to the FFT in each case is the complex result determined \nin Fig. 11.4(b). Hence, the FFT of each nonzero column is a I \n[sin(f)]lf I \nfunction and the results for each column differ only in amplitude. Figure \n11.4(c) illustrates the magnitude of the FFT for each column of Fig. 11.4(b). \nAs in the one-dimensional FFT, we must consider both the sampled \ndata matrix that forms the input to the FFT and the two-dimensional FFT \nresults to be one period of a two-dimensional periodic sequence with period \n(N,M). For this reason, we must interpret the illustrations of Figs. 11.4(a) \nto (c) as one period of \na waveform that is periodic in both the row and column \nindices. This periodicity constraint is examined further in a later section. \nAlso analogous to the one-dimensional case, spacial frequency reso-\nlution in two-dimensional FFT results is given by \nl1u = lI(NTx) \nI1v = lI(MTy) \nComputations Required for Two-Dimensional FFTs \n(11.18) \nFigure 11.4 readily illustrates the concept of computing the two-di-\nmensional discrete Fourier transform by determining successive single-di-\nmensional transforms. We first compute the FFT of each row of data, that \nis, M transforms of N samples each. We organize these results, as shown \nin Fig. 1 \n1.4(b) , and then compute the FFT of each column of data, that is, \nN transforms of M samples each. Therefore, a data matrix of size N x M \nrequires N + M FFTs to be computed. From Chapter 8, the total number \nof computations is NM log2 NM. \n244 \nTwo-Dimensional FFT Analysis \nChap. 11 \nReorganizing Two-Dimensional FFTs \nfor Conventional Viewing \nRecall in the one-dimensional FFT case that it was necessary to rear-\nrange the FFT results if we wished to display them in a format for conven-\ntional viewing (Sec. 9.1). A similar situation is encountered for two-dimen-\nsional FFTs. Figure 11.4(c) must be rearranged or reorganized if the results \nare to be viewed conventionally. We repeat' Fig. 11.4(c) in Fig. 11.5(a) and \nillustrate the required reorganization of Fig. 11.5(a) in Fig. 11.5(b). The same \nreorganization procedure is shown in Figs. 11.5(c) and (d), but from a data-\nmatrix perspective to further clarify the required data restructuring. \nNote that if we examine the data matrix in terms of quadrants, then \nthe restructuring procedure is simply one of a right circular shift through \ntwo quadrants. An examination of Figs. 11.5(c) and (d) illustrates this point. \nThe FFT output data in quadrant I ends up in quadrant III after restructuring. \nQuadrant III is a right circular shift through two quadrants from quadrant \nI. We repeat the Nyquist spacial frequency sample values, H(nINTx ,4) and \nH(4,mIMTy ), in each quadrant. For the real array, quadrant III is a positive \nreflection of quadrant I, and quadrant IV is a positive reflection of II. For \nthe imaginary array, quadrant III is a negative reflection of I and quadrant \nIV is a negative reflection of II. \nExample 11.5 Two-Dimensional FFT Computation \nTo further demonstrate two-dimensional FFT computation, consider the cosinu-\nsoidally corrugated two-dimensional surface illustrated in Fig. 11.6(a). We first sam-\nple the surface with sample intervals Tx and Ty , resulting in 4 rows and 16 columns \nof data. Note that the row samples define exactly a multiple period of the cosine \nwaveform surface. \nWe next compute the one-dimensional FFT of each row of sampled data. From \nFig. 11.6(a), row 0 is a cosine waveform and hence the FFT of row 0 is the impulse \nfunctions shown in row 0 (columns 2 and 14) of Fig. 11.6(a). Recall that the impulse \nfunction in column 14 is a negative frequency result because columns 9 to 15 represent \nnegative frequencies. Because each row of sampled data defines the same cosine \nwaveform, the FFf results for each row are identical, as shown in Fig. 11.6(b). This \ndata matrix is the input to the second series of one-dimensional FFfs. We next \ncompute the FFT of each column of data in Fig. 11.6(b). Each column of data is a \nzero-valued function except for columns 2 and 14. Columns 2 and 14 are both con-\nstant-value functions whose FFTs are impulse functions at zero spacial frequency \n(ml(MTy ), where m = 0). These results are shown in Fig. 11.6(c). \nFigure 11.6(c) illustrates the results of the two-dimensional FFf obtained by \nimplementing successive single-dimension FFTs. However, it is necessary to rear-\nrange these results according to the restructuring procedure illustrated in Fig. 11.5. \nRestructured two-dimensional FFT results are shown ;11 Fig. 11.6(d). Note the re-\nstructuring procedure is one of a right circular shift through two quadrants. Nyquist \nspacial frequency data values are repeated in each quadrant. \nN \n.,.. \nU1 \n(a) \nDIMENSION \n~T, \n(0,0) \n(0,1)' \n(0,2) \n(0,3) \n(0,4) \n(0,5) \n(0,6) \n(0,7) \nI \n(I,D) \n(1,1) \n(1,2) \n(1,3) \n(1,4) \n(1,5) \n(1,6) \n(1,7) \nI \nI~ r \n12.Oi \n12. \" \n12.\" \n12.'J \n\";\" \n\".\" \".0) \nIVI \nE E \n(3,0) \n(3,1) \n(3,2) \n(3,3) \n(3,4) \n(3,5) \n(3,6) \n(3,7) \nZ \nI \nQ \n(4,0)-(4,1 )-(4,2)-(4,3)- (4,4)-(4,5)-(4,6)-(4,7)-\n~ \nI \nrE \n(5,0) \n(5,1) \n(5,2) \n(5,3) \n(5,4) \n(5,5) \n(5,6) \n(5,7) \n~ \nI \nis \n(6,0) \n(6,1) \n(6,2) \n(6,3) \n(6,4) \n(6,5) \n(6,6) \n(6,7) \nI \n(7,0) \n(7,1) \n(7,2) \n(7,3) \n(7,4) \n(7,5) \n(7,6) \n(7,7) \nIV \nI \nIII \n(e) \n(b) \nIII \nIV \n(4.4) \n(4,5) \n(4,6) \n(4,7) \n(4,0) \n(4,1) \n(4,2) \n(4,3) \n(4.4) \nI \n(5.4) \n(5,5) \n(5,6) \n(5,7) \n(5,0) \n(5,1) \n(5,2) \n(5,3) \n(5.4) \nI \n(6,4) \n(6,5) \n(6,6) \n(6,7) \n(6,0) \n(6,1) \n(6,2) \n(6,3) \n(6,4) \nI \n(7.4) \n(7,5) \n(7,6) \n(7,7) \n(7,0) \n(7,1) \n(7,2) \n(7,3) \n(7.4) \nI \n-(0.4)-(O,5)-(O,6)-'.(O,7)-(OjO)-(0,1)-(0,2)-(0,3)-(0,4)--\n~T, \n(1.4) \n(1,5) \n(1,6) \n(1,7) \n(I,D) \n(1,1) \n(1.2) \n(1,3) \n(1.4) \nI \n(2.4) \n(2,5) \n(2,6) \n(2,7) \n(2,0) \n(2,1) \n(2,2) \n(2,3) \n(2.4) \nI \n(3.4) \n(3,5) \n(3,6) \n(3,7) \n(3,0) \n(3,1) \n(3,2) \n(3,3) \n(3,4) \nI \n(4.4) \n(4,5) \n(4,6) \n(4,7) \n(4,0) \n(4,1) \n(4,2) \n(4,3) \n(4.4) \nII \n, \nm \nmT, \n(d) \nFigure 11.5 Graphical presentation of two-dimensional FFT reorganization required for con-\nventional viewing, \n246 \nTwo-Dimensional FFT Analysis \nChap. 11 \nh(x.V) \n(a) \nqT, \n(b) \nFigure 11.6 Two-dimensional FFT computation of a cosinusoidaIly corrugated \nsulface: (a) sampled wavefonn, (b) FFT of row data, (c) FFT of columns of part \n(b), and (d) reorganization of part (c). \nExample 11.6 Alternative Two-Dimensional FFf Computational Procedure \nIn Ex. 11.5, we first compute the FFT of each row of the input data matrix \nand then the FFT of each column of the intermediate computational matrix. Equiv-\nalent results could be obtained if we first compute the FFT of each column and \nsubsequently compute the FFT of each row of the intermediate results. This simply \nstates the fact that Eq. (11.16) can be rewritten in a form such that one first sums \non q, which corresponds to computing the one-dimensional discrete Fourier trans-\nform for each column of data. \nFigure 11.7 illustrates the alternate two-dimensional FFT computational pro-\ncedure. In comparison to the previous example, the two-dimensional waveform \nshown is a corrugated sinusoidal surface. Note that the row samples define exactly \na mUltiple period of the sinusoidal surface. Because each column of sampled data \nis a constant-value function, then the FFT of each column of data is an impulse at \nzero spacial frequency. The amplitude of each impulse is equal to the amplitude of \nthe constant sample value for each column. FFT results of the column data are \nillustrated in Fig. 11.7(b). \nWe next compute the FFT of each row of the data matrix of Fig. 11.7(b). Row \nSec. 11.2 \nTwo-Dimensional FFTs \nm \nMT, \n,~-----, \n~~--~ \n,. \n..... \" \n..... \" \n\" \n\" \n\" \n\",' \n,,'\" \nI \n, \n\" \n/' \n\" \n\" \nn \nI \nâ¢â¢â¢â¢â¢â¢â¢ \n/~ \nâ¢â¢â¢â¢â¢â¢â¢ \nII \n/ \nNT \n.. \n~ \n- ....... \n,\". ...... -.. --,\" \n, \n, \n, \nI \nâ¢â¢â¢â¢â¢â¢â¢ \n.,. â¢ â¢ â¢ â¢ â¢ â¢ \nI \n'IV \n\",,' \nIII \n,1 \n' ..... _--_ \n... \", \n\"\",' \n.... \n-\" \n..... _-----\n(0) \n, ... -\n, \n.,.----- ...... , \n...... \n, \n, \n\" IIIÂ·.Â·Â· \nâ¢ \nâ¢ \nâ¢â¢â¢â¢â¢â¢â¢â¢ IV,' \n,~ \nI \n, \nII â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ \nI \nI â¢â¢â¢â¢â¢â¢â¢â¢ â¢ â¢â¢â¢â¢.â¢â¢ I \n/ \n, _____ \n-\nI \n\" \n,---------\nm \n(d) \nFigure 11.6 (continued) \n.. \n~, \nn \n, \n247 \no \ndata defines a sinusoidal waveform whose FFT is the set of impulse functions \nshown in Fig. 11. 7(c). The impulse function of column 14 is a negative frequency \nvalue, as illustrated in the restructured two-dimensional results of Fig. 11. 7(d). \nTwo-Dimensional Periodicity Constraints \nIn Chapter 6, we saw that the discrete Fourier transform is defined \nonly for periodic sampled functions. A similar result can be shown for the \ntwo-dimensional discrete Fourier transform. A two-dimensional sampled \nfunction is periodic in the row index p with period N and in the column \nindex q with period M if \n(11.19) \nwhere c and d are arbitrary positive or negative integers. \nFigure 11.8 illustrates the implications of Eq. (11.19). The 4 x 4 matrix \nwithin the dotted square is assumed to be the sampled surface. Note that \nwe sampled the two-dimensional function only for positive values of x and \ny. However, due to the periodicity constraint of Eq. (11.19), we must in-\n248 \nm \nMT, \nTwo-Dimensional FFT Analysis \nh(x.y) \nâ¢â¢â¢â¢â¢â¢ â¢â¢â¢â¢â¢â¢â¢â¢â¢ \n(b) \npT. \nI \nI-Row1 \n-Row2 \n-Row3 \nChap. 11 \nFigure 11.7 Example of an alternate two-dimensional FFf computation proce-\ndure: (a) sampled waveform, (b) FFT of column data, (c) FFT of \nrows of \npart (b), \nand (d) reorganization of part (c). \nterpret this matrix of data as one period of a periodic two-dimensional func-\ntion. Figure 11.8 shows four periods of this sampled waveform. It is im-\nportant to observe the relationship of (row,column) indices for each period \nwith the (row,column) indices within the dotted square. \nExample 11.7 Periodicity in Two Dimensions \nTo further demonstrate the two-dimensional periodicity constraint, consider the sur-\nface shown in Fig. 11.9(a). Let us assume that it is desired to sample this surface \nand compute the two-dimensional FFT. Considerable care must be paid to the pe-\nriodicity constraint. A review of Fig. 11.8 points out that if we wish to sample the \nsurface shown in Fig. 11.9(a), then it is necessary to actually sample the function \nillustrated in Fig. 11.9(b). Although this two-dimensional surface looks significantly \nSec. 11.2 \nTwo-Dimensional FFTs \nn \nâ¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ \nNT \nâ¢ \nm \nMT, \n........ \nm \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n(e) \nn \n(d) \nFigure 11.7 \n(continued) \nI \n(0.0) \n(0.1) \n(0.2) \n(0.3) \n(0.0) \n(0.1) \n(0.2) \n(0.3) \nI \n(1.0) \n(1.1) \n11.2) \n11.3) \n(1.0) \n(1.1) \n(1.2) \n(1.3) \nI \n(2.0) \n(2.1) \n(2.2) \n(2.3) \n(2.0) \n(2.1) \n(2.2) \n(2.3) \nI \n(3.0) \n(3.1) \n(3.2) \n(3.3) \n(3.0) \n(3.1) \n(3.2) \n(3.3) \n-(0.0)-(0.1 \n)-(0.2)-(0.3)~(ot;:.:..(0:; \n)~(0-:2i=iO.3)+-\nI \nI \n) \nx \n(1.0) \n(1.1) \n(1.2) \n(1.3) I 11.0) \n(1.1) \n(1.2) \n(1.3) I \nI \nI \nI \n(2.0) \n(2.1) \n(2.2) \n(2.3): (2.0) \n(2.1) \n(2.2) \n(2.3): \nI \nI \nI \n(3.0) \n(3.1) \n(3.2) \n(3.3) I (3.0) \n(3.1) \n(3.2) \n(3.3) I \n~-f~ \n----- -----~ \nFigure 11.8 Example showing periodicity implied by the 4 x 4 sampled matrix \nwithin the dotted square. \n249 \n250 \nTwo-Dimensional FFT Analysis \nChap. 11 \nr\" \n(al \nh(pT \nâ¢. qT,1 \npT. \n(bl \nqT, \nFigure 11.9 Example of two-dimensional waveform restructuring to satisfy pe-\nriodicity constraints. \ndifferent from Fig. 11.9(a), the sampling periodicity constraint yields the original \nsurface. \nCare must be exercised when setting up the two-dimensional matrix of \nsampled \nvalues to ensure that the function being FFTed is that which is actually desired. \nRecall that this warning is the same as that given in Chapter 9. \nTwo-Dimensional Data Windows \nRecall from Chapter 9 that a potential negative effect of the periodicity \nconstraint is to introduce truncation, a discontinuity or abrupt change in the \nSec. 11.2 \nTwo-Dimensional FFTs \n251 \ndata between periods. Consequentially, FFT frequency-domain results ex-\nhibit oscillations or side lobes. A similar result is encountered with two-\ndimensional FFTs. \nIn Fig. 11.1O(a), we show a sampled corrugated two-dimensional co-\nsinusoidal surface. Note that the sample values do not define exactly a mul-\ntiple of a period of the cosine waveform surface. As a result, truncation \nintroduces a discontinuity between periods in the pTx dimension and the \nm \nhlx.v' \nla, \nHln.m, \n............... \n............... \n. \n. \n.. . \n.. . \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n.. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n.. . \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n.. . \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n.. . \n. \n.. . \n. \n. \n. \n. \n. \n. \n.. .. . \n. \n.. . \n. \n.. . \n. \n. \n. \n. \n. \n. \n. \n. \n. \n. \n.. . \n. \n. \n. \n. \n. \n. \n. \n. \n. \nIb, \nn \nFigure 11.10 Graphical illustration of two-dimensional FFf results before a \nweighting function is applied. \n252 \nTwo-Dimensional FFT Analysis \nChap. 11 \ntwo-dimensional FFT results shown in Fig. 11.10(b) exhibit large side lobes. \nAs in the one-dimensional case, we use a weighting function to reduce the \nside lobes. \nThe weighting function techniques discussed in Chapter 9 are directly \nextendable to two dimensions. As expected, a rectangular two-dimensional \nweighting function exhibits large side lobes (see Fig. 11.3). Haung [9] has \nshown that a good two-dimensional symmetrical weighting function w' \n( \n.) \ncan be obtained from a one-dimensional window from the relationship: \nw'(x,y) = w[(x2 + y2)1/2] \nI \nx2 + y21 < T'/2 \n(11.20) \n= 0 \notherwise \nwhere w(Â·) is centered at [x = 0, y = 0] and T' is the truncation interval. \nFunction w(Â·) is any weighting function such as Hanning or Dolph-Che-\nbyshev. Figure 11.11 illustrates the two-dimensional Hanning weighting \nfunction as determined from Eq. 01.20) for M = N and as correctly posi-\ntioned with respect to the two-dimensional period. \nFigure I 1. 11 (b) illustrates the two-dimensional FFT results when the \nHanning function is applied to the sampled waveform of Fig. 11.1O(a). As \nexpected, side lobes are reduced with respect to Fig. 1 \n1. lO(b) , but the fre-\nquency function has been broadened in two dimensions. We have not rear-\nranged the FFT results for conventional viewing. \nTwo-Dimensional Inverse FFTs \nThe two-dimensional inverse discrete Fourier transform is defined as \np = 0, I, ... , N -\n1 \nn = 0, I, ... , N -\nI \nq = 0, I, ... , M -\n1 \nm = 0, I, ... , M -\nI \n(11.21) \nAs in the direct transform case, we implement the inverse transform \nby first inverse transforming each row (or column) and then inverse trans-\nforming each column (or row) of the intermediate computation matrix. \nWhen applying the two-dimensional inverse FFT, we must be careful \nin setting up the spacial frequency-data matrix. Recall from Fig. 11.4(c) that \nthe output of the two-dimensional FFT is not in the form for conventional \nviewing. To compute Eq. (11.21), we must ensure that the data is in the \nformat of Fig. 11.4(c). If \nthe data to be inversed transformed is in a format \nfor conventional viewing, then we simply reverse the procedure illustrated \nin Fig. 11.5 before inputting the data to Eq. 01.21). This process is analogous \nto that described in Chapter 9 for computing the inverse FFT of a one-\nSec. 11.2 \nTwo-Dimensional FFTs \nm \nw'(p,q) \n(a) \nH'(n,m) \n0.1 \n0.05 \n....... Â·'1-Â·Â·Â·Â· \nâ¢â¢â¢â¢â¢â¢â¢â¢ \nâ¢ \nâ¢â¢â¢â¢â¢â¢â¢â¢â¢ \nâ¢ â¢â¢ \n........ \n~ \n.. \n:::::::~.. :::: \n\\:-....... . \n. \n~\" ..... \n. \n. \n\"' ...... \n. \n\"' .... . \n(b) \nn \nFigure 11.11 (a) Figure 11.10(a) multiplied by the two-dimensional Hanning \nweighting function, and (b) two-dimensional FFT of part (a). \n253 \ndimensional frequency function. Equation 01.21) must be multiplied by the \nfactor t:.ut:.v to obtain results equivalent to the continuous inverse two-di-\nmensional Fourier transform. \n254 \nTwo-Dimensional FFT Analysis \nChap. 11 \nTwo-Dimensional Sampling \nIn the development of the two-dimensional discrete Fourier transform, \nwe did not discuss the requirements imposed by the two-dimensional sam-\npling theorem. To develop the theorem, we simply extend the one-dimen-\nsional concept. Ifwe are given a two-dimensional surface h(x,y) whose Four-\nier transform is H(u,v), where \nH(u,v) = 0 \n(11.22) \nthen we simply state the Nyquist criteria independently in the two dimen-\nsions. That is, we must sample h(x,y) to obtain h(pTx,qTy), such that T< and \nTy satisfy the relationships \nSummary \nTx :5 1I2uc \nTy :5 1I2vc \n(11.23) \nA BASIC computer program for the two-dimensional FFT of an array \nW(n,m) is given in Fig. 11.12. The real component of the two-dimensional \nsignal is placed in WIREAL(II%,JJ%) and the imaginary component is \nplaced in WlIMAG(II%,JJ%). Parameters N%, NU%, M%, and MU% must \nbe initialized. Real and imaginary spacial frequency-domain results are re-\nturned in WIREAL(II%,JJ%) and WlIMAG(II%,JJ%), respectively. Note \nthat the program first computes the FFT of each column of the data array \nusing the one-dimensional FFT program listed in Fig. 8.7. The program then \ncomputes the FFT of each row of the intermediate array, again using the \none-dimensional FFT. XREAL(I%) and XIMAG(I%) should be dimensioned \nby the larger of N% or M%. Users must rearrange the output results for \nconventional viewing and scale by TxTy to obtain equivalence to the con-\ntinuous two-dimensional transform. \nTwo-dimensional inverse FFTs can be computed with the program by \nfirst conjugating the spacial frequency function by exactly the procedure \nused in one-dimensional inverse transforms. \nIn this section, we have developed the basic fundamentals for applying \nthe FFT to the computation of the two-dimensional Fourier and inverse \nFourier transforms. The discussion by no means has been exhaustive but \nthe fundamental principles necessary for further investigations have been \nestablished. If one carefully extends the concepts of one-dimensional FFT \nanalysis to each of the FFTs computed in the two-dimensional transform, \nthen few difficulties should be encountered. As developed, the mathematics \nof two-dimensional transform analysis is sufficiently close to that of \nthe one-\ndimensional case to justify such a conclusion. \nSec. 11.3 \nTwo-Dimensional Convolution and Correlation \n9000 REM: \nTWO-DIMENSIONAL FFT SUBROUTINE- THE MAIN \n9002 REM: \nPROGRAM SHOULD DIMENSION THE DATA ARRAYS \n9004 REM: \nW1REAL( I I%,JJ%) AND W1 IMAG( I I%,JJ%). \n9006 REM: \nN%,NU%,M%, AND MU% MUST BE INITIALIZED. \n9008 REM: \nXREAL( 1%) AND XIMAG(J%) SHOULD BE DIMENSIONED \n9010 REM: \nTHE LARGER OF N% OR M%. THIS PROGRAM \n9012 REM: \nCALLS THE FFT ROUTINE (FIG. 8-7) BEGINNING \n9014 REM: \nAT LINE 10000. \n9026 \nNN%=N%:NNU%=NU%:MM%=M%:MMU%=MU% \n9028 REM: COMPUTE THE FFT OF EACH COLUMN. \n9030 FOR JJ%=1 TO MM% \n9040 \nFOR 11%=1 TO NN% \n9050 \nXREAL( I 1%)=W1REAL( I I%,JJ%) \n9060 \nXIMAG(1 1%)=W1 IMAG(I I%,JJ%) \n9070 \nNEXT 11% \n9080 \nGOSUB 10000 \n9090 \nFOR KK%=1 TO NN% \n9100 \nW1REAL(KK%,JJ%)=XREAL(KK%) \n9110 \nW1IMAG(KK%,JJ%)=XIMAG(KK%) \n9120 \nNEXT KK% \n9130 NEXT JJ% \n9140 REM: COMPUTE THE FFT OF EACH ROW. \n9150 FOR JJ%=1 TO NN% \n9160 \nFOR 11%=1 TO MM% \n9170 \nXREAL( I 1%)=W1REAL(JJ%, I 1%) \n9180 \nXIMAG( I 1%)=W1 IMAG(JJ%, I 1%) \n9190 \nNEXT 11% \n9200 \nN%=MM%:NU%=MMU% \n9210 \nGOSUB 10000 \n9220 \nFOR KK%=1 TO MM% \n9230 \nW1REAL(JJ%,KK%)=XREAL(KK%) \n9240 \nW1IMAG(JJ%,KK%)=XIMAG(KK%) \n9250 \nNEXT KK% \n9260 NEXT JJ% \n9270 N%=NN%:NU%=NNU% \n9280 RETURN \n9290 END \nFigure 1l.12 Subroutine in BASIC for computing the two-dimensional FFf. \n11.3 TWO-DIMENSIONAL CONVOLUTION \nAND CORRELATION \nThe convolution integral for two-dimensional functions is defined as \ng(x,y) = I-'''''oo J:oo r(Tx,Ty)h(x-Tx,y-Ty)dTxdTy = r(x,y) ** h(x,y) \n255 \n(11.24) \n256 \nTwo-Dimensional FFT Analysis \nChap. 11 \nInterpretation of Eq. 01.24) is analogous to the one-dimensional case, as \nwe demonstrate in the following graphical analysis. \nGraphical Evaluation \nLet r( \nT x, \nT y) and h( \nT x, \nT y) be given by the graphs shown in Figs. I 1. 13(a) \nand (b), respectively. For ease of presentation, we do not show the ampli-\ntudes of the two-dimensional functions. To evaluate Eq. 01.24) for the point \ng(x' ,Y '), function hex' - Tx,Y' - T y) is required. From Fig. 11.13(c), note that \nh( -Tx, -Ty) is obtained by rotating h(Tx,Ty) 1800 about the origin. Function \nhex' -Tx,Y' -Ty) is obtained by displacing h( -Tx, -Ty) by the amount x' \nalong the T \nx axis and by the amount y' along the T \ny axis, as illustrated in \nFig. 11.13(d). The volume (double integral) of the product r(Tx,Ty) x \nh(x'-Tx,y'-Ty) yields the convolution result g(x',y'). \nExample 11.7 Two-Dimensional Convolution: Line Functions \nAn example of two-dimensional convolution is illustrated in Fig. 11.14. The two \nfunctions to be convolved are shown in Figs. 11.I4(a) and (b). Because h(x,y) is \nT, \nT, \nh(T \nâ¢â¢ T,) \n(a) \n(b) \nT, \nh(' \nÂ·T \nâ¢. yÂ· 'T,) \nT, \nh(-T \nâ¢â¢ Â·T,) \n(e) \n(d) \nFigure 11.13 Graphical evaluation of two-dimensional convolution. \nSec. 11.3 \nTwo-Dimensional Convolution and Correlation \n257 \nsymmetrical about the Ty axis, then the 1800 rotation required by Eq. (11.23) yields \nthe same function, h(Tx,Ty) = h( -T\" \n-Ty ), as shown in Fig. 11.14(a). Figure 11.14(c) \nshows the displacement function h(x' -Tx,y'-Ty ); multiplication with r(T\"Ty) and \nintegration yields the spacial point of the resulting two-dimensional convolution il-\nlustrated in Fig. 11.14(d). \nExample 11.8 Two-Dimensional Convolution: Impulse Functions \nTo demonstrate two-dimensional convolution involving impulse functions, consider \nFig. 11.15. In Fig. 11.15(a), we show a two-dimensional sequence of impulse func-\ntions. The impulse functions are separated by Tx in the x dimension and by Tv in \nthe y dimension. The function to be convolved with these impulses is shown in Fig. \n11.15(b). \nFor ease of presentation, we have deleted the amplitude information of Figs. \n11.15(a) and (b) and show only the length, width, and displacement information in \nFigs. 11.15(c) and (d). To convolve Figs. 11.15(c) and (d), recall from Chapter 4 that \nconvolution with an impulse function requires centering the function to be convolved \non the impUlse. The resulting convolution is shown in Figs. 11.15(e) and (t). Results \nof this example can be extended to graphically demonstrate the two-dimensional \nsampling theorem (Prob. 11.15). \nT, \nT. \nT, \nT. \n(a) \n(b) \nglx,y) \ng(x',y') \nT. \nH(XI_Tx,yl_Ty) \nIe) \nId) \nFigure 11.14 Graphical example of two-dimensional convolution of line functions. \n258 \nTwo-Dimensional FFT Analysis \nChap. 11 \ng(x.v) \n/ \n/ \nhlx.v) \nt t V \n/ \n+ â¢ \n/ \nx \nâ¢ \nv \n(a) \nIb) \nâ¢ \nâ¢ \nâ¢ \nâ¢ \nâ¢ \nâ¢ \nâ¢ \nâ¢ \n-:-1 \nv \nv \nIe) \n(d) \ng(x.V)\"h(x,V) \nv \n(e) \nIf) \n'Figure 11.15 Graphical example of two-dimensional convolution with impulse \nfunctions. \nx \nx \nSec. 11.3 \nTwo-Dimensional Convolution and Correlation \n259 \nExample 11.9 Two-Dimensional Convolution: Amplitude Determination \nTo further illustrate two-dimensional convolution, consider the square surfaces \nshown in Fig. 11.16(a). Both functions have unity amplitude. As shown in Fig. \n11.16(b), the dimensions of the nonzero convolution result is a square (a + b) on a \nside. Constant-amplitude contours are also shown. The amplitude of the convolution \nresult along the x axis is shown in Fig. 11.16(c); appropriate shifts and integration \nareas required to evaluate example amplitude values on the x axis are also shown. \nTwo-Dimensional Convolution Theorem \nWe can compute the two-dimensional convolution by multiplication in \nthe Fourier transform domain. Hence, if r(x,y) and h(x,y) are two waveforms \nV \na \n'VI \n\"2 \nÂ·a \n0 \na \n2: \n2: \nÂ·a \n2: \nT, \nx \n(al \nÂ·b \n2: \nT, \n(cl \n(bl \nT, \nT, \nx;: a+b \n2 \nT, \nFigure 11.16 Graphical example of amplitude computation in two-dimensional \nconvolution. \n260 \nTwo-Dimensional FFT Analysis \nChap. 11 \nwith transforms R(u,v) and H(u,v), respectively, then \nr(x,Y) ** h(x,y) ~ \nR(u,v)H(u,v) \n(11.25) \nTo show the relationship of \nEq. (11.25), we apply the one-dimensional trans-\nform interpretation of the two-dimensional Fourier transform, which was \ndeveloped in Eqs. (11.7) to (11.9). \nIf \nZr(U,y) and Zh(U,y) are the one-dimensional transforms of \nr(x,y) and \nh(x,y) with respect to x, respectively, we can write from the one-dimensional \nconvolution theorem: \n(11.26) \nHence, \nf:\", f:\", r(Tx,Ty)h(x-Tx,y-Ty)dTxdTy ~ \nf:oo Zr(U,Ty)Zh(U,y-Ty)dTy \n~ \nR(u,v)H(u,v) \n(11.27) \nwhich is Eq. (11.25). As in the one-dimensional case, we utilize the two-\ndimensional convolution theorem as a means for applying the FFT to com-\npute the two-dimensional convolution. \nTwo-Dimensional Correlation \nThe two-dimensional correlation integral is given by \np(x,y) = f:\"\" f:\", r(Tx,Ty)h(x+Tx,y+Ty)dTx dTy \n(11.28) \nAs in the one-dimensional case, Eq. (11.28) implies that we do not fold the \nfunction h(x,y) prior to the shifting operation. With this exception, the pre-\nviously developed graphical analysis techniques apply. \nThe correlation theorem is given by \nf:\", f:\", r(Tx,Ty)h(x+Tx,y+Ty)dTx dTy ~ \nR*(u,v)H(u,v) \n(11.29) \nwhere the notation R*(u,v) implies a conjugate operation on both the vari-\nables u and v. \n11.4 TWO-DIMENSIONAL FFT CONVOLUTION \nAND CORRELATION \nA two-dimensional FFT convolution is computed in exact analogy with one-\ndimensional FFT convolution. The two-dimensional discrete convolution \nSec. 11.4 \nTwo-Dimensional FFT Convolution and Correlation \n261 \nrelationship is given by \nM-I N-I \ng(pTx,qTy) = L L r(iTx,jTy)h[(p - i)Tx,(q - j)Ty] \nj=O ;=0 \np = 0, 1, ... , N -\n1 \ni = 0, 1, ... , N -\nq = 0, 1, ... , M -\n1 \nj = 0, 1, ... , M -\n1 \n(11.30) \nwhere g(pTx,qTy), r(pTnqTy), and h(pTx,qTy) are periodic functions with \nperiods NTx and MTy in the x and y coordinates, respectively. \nWe use the frequency convolution theorem to compute the discrete \nconvolution by means of the FFT. First, we compute the two-dimensional \nFFT of the functions rCiTx,jTy) and hCiTnjTy): \nR(nINTx,mIMTy) = M:i I \n[N:i I \nr(pTx,qTy)e -j2-rrnP\nIN] e -j2-rrmqIM \nq=O \np=o \n(11.31) \nn = 0, 1, ... , N -\np = 0, 1, ... , N -\n1 \n(11.32) \nm = 0, 1, ... , M -\n1 \nq = 0, 1, ... , M -\nNext, we compute the product R(nINTx,mIMTy)H(nINTx,mIMTy) (taking \ninto account that both functions are in general complex), and then we com-\npute the inverse FFT of this product: \ng(pTx,qTy) = (1INM) :~~ [:~~ \nR(nINTx,mIMTy) \nx H(nINTnmIMTy)ej2-rrnPIN] ej2-rrmqlM \n(11.33) \nThe convolution result appears in the real array of the FFT output. \nBecause Eq. (11.30) represents a periodic convolution of the periodic \nsampled function r(pTx,qTy) and h(pTx,qTy), then we must ensure that these \nsampled functions contain sufficient zero values to prevent the end effect \nor circular convolution. In general, if a nonzero-value function of dimension \n(N I,M d is convolved with a second nonzero-value function of dimension \n262 \nTwo-Dimensional FFT Analysis \nChap. 11 \n(N2,M2), the resulting function is of dimension (N] + N2 -\nI,M] + M2 \n-\nI). Hence, we should append sufficient zeros to each of the functions, \nas illustrated in Fig. 11.17, to accommodate this relationship. The appendage \nof additional zeros may be necessary to make the number of data samples \nin each row and column compatible with the FFT algorithm being used. \nAfter zeros have been appended to functions r(pTx,qTy) and h(pTx,qTy), \nthen we apply Eqs. (11.31) to (11.33). The result is the desired two-dimen-\nsional convolution. All results must be multiplied by the scale factor TxTy \nto approximate the continuous two-dimensional convolution integral. \nExample 11.10 Two-Dimensional Convolution \nTo illustrate the procedure for implementing a two-dimensional discrete convolution \nby means of the FFT, consider the two data arrays shown in Fig. 11.18(a). We note \nthat the two arrays are of dimension (2,2) and (2,4). Hence, the convolution result \nis of dimension (3,5) and sufficient zeros must be appended to each array to increase \nits respective dimensions to this size to prevent the end effect or circular convolution. \nFor a base-2 FFT algorithm, we must add zeros to each data array to obtain data \narrays of dimension (4,8). The augmented data arrays are illustrated in Fig. 11.18(b). \nWe input the data arrays of Fig. 11.18(b) to Eqs. (11.31) and (11.32) and then im-\nplement Eq. (11.33). The result of this procedure is shown in Fig. 11.18(c). \nExample 11.11 Two-Dimensional Convolution When One Function Is Separable \nIf one of the functions to be convolved is separable, that is, \n(11.34) \nthen the two-dimensional discrete convolution can be accomplished by repeated \nevaluations of one-dimensional discrete convolutions. To show this, substitute Eq. \n(11.34) into Eq. (11.30): \nM-\\ N-\\ \ng(pTx,qTy) = L L r](iTx)r2(jTy)h[(p -\ni)Tx,(q - j)Ty] \nj=O ;=0 \n(11.35) \nThe term inside the brackets is a one-dimensional convolution, which is evaluated \nfor each value of i, where i = 0, 1, ... , N -\nI. That is, we convolve the function \nr2(jTy) with each row of the data matrix h[iTxJTy]. This resulting data matrix is then \nconvolved, by column, with the function r\\(iTx), as described by Eq. (11.35). \nZEROS \nZEROS \nM,+M,Â·l'-----------' \nFigure 11.17 Appending zeros in two-dimensional FFT convolution to avoid the end effect. \nSec. 11.4 \nTwo-Dimensional FFT Convolution and Correlation \niT. \n-2-1 \n__ \nI \n-1-0-0-1 \n____ \nI \n3 \n2 \n1 \n3 \n2 \nl \nr(IT.,]T,} \nJT, \n, \nl \nh(iT.,iT,} \niT, , \n(a) \nI \nIT. \nI \nIT. \n-2 \n-1-0-0-0-0-0-0 \n__ -1-0-0-1-0-0-0-0----\nI \nI \n3000000 \n21320000 \nI \nI \n00000000 \n00000000 \nI \n10000000 \nIT, tOO \n0 \n0 \nr~(iT.,~,} 0 \niT, ~ \nh'(iT \nâ¢. iT,} \n(b) \nI \n01234567 \nI \n-0---2-1-0-2-1-0-0-0--\nI \nP \n7 \n5 \n7 \n10 \n3 \n0 \n0 \n0 \nI \n2 \n6 \n5 \n10 9 \n2 \n0 \n0 \n0 \nI \n300000000 \nq , \ng(P.q} \n(e) \n263 \nFigure B.IS Base-2 example of appending zeros in a two-dimensional FFr con-\nvolution to avoid the end effect. \nTo develop the FFT computational approach to this two-dimensional convo-\nlution case, we use the following relationship for separable functions (see Prob. 11.3): \n(11.36) \nApplication of the FFT convolution theorem procedures described by Eqs. (11.31) \nto (11,33) requires that we determine the product \nR(nINTx,mIMTy)H(nINTx,mIMTy) \n(11.37) \nRecall that N + M one-dimensional FFTs are required to determine R(nl \nNTx ,ml \nMTy). However, because r(pTx,qTy) is separable, only 2 one-dimensional FFTs are \nrequired. The computational savings is readily apparent and the utilization of sep-\n264 \nTwo-Dimensional FFT Analysis \nChap. 11 \narable filter functions in two-dimensional signal-processing applications is common \nbecause of these computational simplifications. \nTwo-Dimensional FFT Correlation \nThe two-dimensional discrete correlation function is given by the \nrelationship \nM-I N-I \n~(pTx,qTy) = L L r(iTxJTy)h[(p + i)Tx, (q + J)Ty] \nj=O ;=0 \np = 0, 1, ... , N -\n1 \n(11.38) \nq = 0, 1, ... , M -\n1 \nwhere ~(pTx,qTy), r(pTx,qTy), and h(pTx,qTy) are periodic functions with \nperiods NTx and MTy in the x and y coordinates, respectively. To apply the \ntwo-dimensional FFT to the computation of Eq. (11.38), we follow the pro-\ncedures previously developed for the FFT discrete convolution theorem \nexcept that we apply the two-dimensional discrete correlation theorem. \nSummary \nA BASIC computer program for computing a two-dimensional con-\nvolution using the FFT is given in Fig. 11.19. The data to be convolved are \nstored in arrays WIREAL(II%,JJ%) and W2REAL(II%,JJ%) and the im-\naginary arrays WlIMAG(II%,JJ%) and W2IMAG(II%,JJ%) should be set to \nzero for real functions. Two-dimensional convolution results are returned \nin arrays WIREAL(II%,JJ%) and W2REAL(II%,JJ%) and must be scaled \nby TxTy. Note that the program branches to the two-dimensional FFT pro-\ngram listed in Fig. 11.12 and hence also branches to the one-dimensional \nFFT program listed in Fig. 8.7. N%, NU%, M%, and MU% must be ini-\ntialized and XREAL(I%), and XIMAG(I%) must be dimensioned to the \nlarger of N% or M%. The reader is responsible for preventing convolution \nend effects. \nTwo-dimensional signal processing often involves large data matrices \nthat can exceed computer memory capacity. For this reason, these types of \ndata are normally stored on magnetic disk or tape units. As a result, one \nencounters a data-access problem. Recall that the two-dimensional FFT can \nbe computed by first performing a one-dimensional FFT on each row of \ndata. If the data is stored sequentially by rows, then data access for this \nstep is straightforward. However, the next step in computing the two-di-\nmensional transform requires performing the FFT on the columns of the \nmatrix that is generated by row transforms. We now encounter a memory-\naccess problem in extracting column data. One could transpose the matrix, \nSec. 11.4 \nTwo-Dimensional FFT Convolution and Correlation \n8000 REM: \nTWO-DIMENSIONAL FFT CONVOLUTION PROGRAM- THE \n8002 REM: \nMAIN PROGRAM SHOULD DIMENSION THE DATA \n8004 REM: \nARRAYS W1REAL( I 1%,JJ%),W1 IMAG( I I%,JJ%), \n8006 REM: \nW2REAL( I I%,JJ%) ,W2IMAG( I I%,JJ%) AND \n8008 REM: \nDUMMY ARRAYS W3REAL( I I%,JJ%), W3IMAG( I I%,JJ%). \n8010 REM: \nN%,NU%,M%, AND MU% MUST BE INITIALIZED. \n8012 REM: \nXREAL( 1%) AND XIMAG( 1%) SHOULD BE DIMENSIONED \n8014 REM: \nTHE LARGER OF N% OR M%.THE PROGRAM CALLS \n8016 REM: \nTHE TWO-DIMENSIONAL FFT ROUTINE(FIG. 11.12) \n8018 REM: \nBEGINNING AT LINE 9000, WHICH IN TURN CALLS \n8020 REM: \nTHE ONE-DIMENSIONAL FFT PROGRAM (FIG. 8.7) \n8022 REM: \nBEGINNING AT LINE 10000. \n8024 REM: \nCOMPUTE THE TWO-DIMENSIONAL FFT OF W1(N,M) \n8030 \nGOSUB 9000 \n8040 FOR 11%=1 TO N%. \n8050 \nFOR JJ%=1 TO M% \n8060 \nW3REAL( I 1%,JJ%)=W1REAL(1 I%,JJ%) \n8070 \nW3IMAG( I 1%,JJ%)=W1 IMAG(I I%,JJ%) \n8080 \nW1REAL( I 1%,JJ%)=W2REAL(1 I%,JJ%) \n8090 \nW1 IMAG( I 1%,JJ%)=W2IMAG( I I%,JJ%) \n8100 \nNEXT JJ% \n811 0 NEXT I 1% \n8120 REM: \nCOMPUTE THE TWO-DIMENSIONAL FFT OF W2(N,M) \n8130 \nGOSUB 9000 \n265 \n8140 REM: COMPUTE THE PRODUCT OF W1( I I%,JJ%) AND W2( I I%,JJ%) \n8150 FOR 1%=1 TO N% \n8160 \nFOR J%=1 TO M% \n8170 \nW2REAL( 1%,J%)=W1REAL( I%,J%) \n8180 \nW1REAL( 1%,J%)=W1REAL(I%,J%)*W3REAL( I%,J%) \n-W1 I \nMAG ( 1%, J%) *W3 I \nMAG ( 1%, J%) \n8190 \nW1 IMAG( 1%,J%)=-W2REAL(I%,J%)*W3IMAG( I%,J%) \n-W1 I \nMAG ( 1%, J%) *W3REAL ( 1%, J%) \n8200 \nNEXT J% \n8210 NEXT 1% \n8220 REM: \nCOMPUTE THE TWO-DIMENSIONAL FFT OF THE \n8222 REM: \nPRODUCT CONJUGATE \n8230 \nGOSUB 9000 \n8240 RETURN \n8250 END \nFigure 11.19 Subroutine in BASIC for two-dimensional FFT convolution. \n266 \nTwo-Dimensional FFT Analysis \nChap. 11 \nbut this is not possible if the matrix exceeds the computer memory. Ref-\nerences [10] to [12] propose various methods for partitioning the matrix so \nthat it will fit into available memory. Alternate transposition procedures are \ndescribed in Refs. [14] to [17]. Another approach to efficient computation \nof the two-dimensional discrete transform is to derive a direct two-dimen-\nsional FFT. This method is described in Refs. [18] and [19]. An excellent \nintroduction to the field of digital image processing is presented in Oppen-\nheim [5]. \nPROBLEMS \n11.1 Prove each of the following properties of the two-dimensional Fourier trans-\nform directly from the defining relationships of Eqs. (11.1) and (11.13): \n(a) Addition: \nh(x,y) + g(x,y) <0 H(u,v) + G(u,v) \n(b) Shifting: \nh(x -\na,y -\nb) <0 e-j2-rr(au+bv)H(u,v) \n(c) Modulation: \nh(x,y) cos(21rfox) <0 Y2 H(u + fo,v) + \nY2 H(u -\nfo,v) \n(d) Scaling: \nh(ax,by) \n<0 (II I \nab I \n)H(ula,vlb) \nh(x,y)e j2-rr(ax+by ) <0 H(u -\na,v -\nb) \n(e) Convolution: \nh(x,y) ** g(x,y) <0 H(u,v)G(u,v) \n11.2 Repeat Prob. 11.1 but derive all results from a one-dimensional viewpoint \nutilizing the relationships of Eqs. (11.8) and (11.9) and the one-dimensional \nFourier transform. \n11.3 If a function h(x,y) is separable such that \nh(x,y) = hI \n(x)h 2(y) \nthen the two-dimensional Fourier transform of h(x,y), that is, H(u,v) is \nseparable: \nProve this result. \n11.4 Determine the two-dimensional Fourier transform of the following: \n(a) h(x,y) = cos(21ruoX) \n(b) h(x,y) = sin(21TvoY) \n(c) h(x,y) = cos{21r[x cos(e) + y sin(e)]} \nChap. 11 \nProblems \n(d) h(x,y) = \n(e) h(x,y) = \n(f) h(x,y) \n(g) h(x,y) \n(h) h(x,y) \n(i) h(x,y) \nsin(21Tuox) sin(21TvoY) \ncos(21TUox) sin(21TvoY) \nsin(21Tuox) sin(21TvoY) \n21TUox \n8(y) \n21TVoY \ncos(21TVoY )8(x) \nI \n(x 2 + y2)112 < ~ \n= 0 \notherwise \n11.5 Parseval's Theorem for one-dimensional functions is given by \nDerive the corresponding relationship for two-dimensional functions. \n267 \n11.6 Figure 11.2(a). can be redrawn in terms of lines of zero phase, as illustrated \nin Fig. 11.20. Develop analytic expressions for the following: \n(a) A function of the form y = mx + Tib for the zero-phase lines, where m \nis the slope, b is the y intercept, and Ti is an integer. \n(b) An expression for 8 in terms of the spacial frequency terms Uo and Vo. \n(c) An expression for L, the spatial period, i.e., the distance between zero-\nphase lines. \n11.7 If we introduce the polar coordinates x = r cos(8), y = r sin(8), U = w cos(<IÂ», \nand v = w sin(<IÂ», then h(x,y) and H(u,v) become hp(r,8) and Hp(w,<IÂ», re-\nspectively, where \nhp (ar,8 + \n80 ) 0 \n(l/a 2)Hp (w/a,<I> + \n80 ) \n(11.39) \nNote that Eq. (11.39) states that if h(x,y) is rotated through an angle 80 , then \nthe transform H(u,v) is also rotated through the angle 80 , Derive Eq. (11.39). \nFigure 11.20 Function for Prob. 11.6. \n268 \nTwo-Dimensional FFT Analysis \nChap. 11 \nHint: \nh(ax + \nby,cx+ \ndy) ~ \n(11 I ad -\ncd I )h(Au+Bv,Cu+Dv) \nwhere \n11.8 An interesting special case of two-dimensional discrete Fourier transforms \noccurs when the sampled function h(pTx,qTy) separates as hl(pTx)h2(qTy), \nwhere p = 0, 1, ... , N -\n1 and q = 0, 1, ... , M -\n1. Show that in this \ncase the discrete Fourier transform H(n/ \nNT,ml \nMT) can be evaluated by one \nN-point FFT and one M-point FFT. \n11.9 Figure 11.4 shows the two-dimensional FFT as a set of one-dimensional FFTs \non the rows of the data matrix followed by a set of one-dimensional FFTs on \nthe columns of the data matrix computed in the first step. Show by sketches, \nas in Fig. 11.4, that equivalent results can be obtained by FFTing first the \ncolumns of the data array followed by the rows of the intermediate computed \narray. \n11.10 Consider the sampled waveform shown in Fig. 11.21. Analytically compute \nthe two-dimensional discrete Fourier transform by using one-dimensional \ntransforms: \n(a) Transform each column and then each row of the result. \n(b) Transform each row and then each column of the result. \n11.11 Develop an alternate inversion formula for two-dimensional transforms that \nallows one to use the forward two-dimensional FFT. \n11.12 In Chapter 8, we developed techniques for increasing the efficiency of the \none-dimensional FFT for the case of real data. If only two-dimensional real \ndata are being considered, develop the procedures and appropriate relation-\nships for the following: \n(a) Computing the two-dimensional FFToftwo real functions simultaneously. \n(b) Computing the two-dimensional FFT of a (2N,2M) real data array with \nan (N,M) two-dimensional FFT program. \n11.13 It is of value to consider two-dimensional convolution and correlation from \nan area viewpoint without regard to amplitude. For each of the functions il-\nlustrated in Fig. 11.22, determine the area function (i.e., follow Fig. 11.11) for \nboth convolution and correlation. \n11.14 If a small two-dimensional area is to be convolved with a much larger two-\ndimensional area, then sectioning techniques as developed in Chapter 10 must \nbe used. Extend the overlap-add sectioning technique to two-dimensional \nconvolution. \n11.15 Figure 11.15 develops the basic concepts of two-dimensional convolution in-\nvolving impulse functions. By using this approach, extend the two-dimensional \nNyquist sampling criteria development of Fig. 5.3 to two dimensions. Assume \nthat the transform H(u,v) is band-limited according to Eq. (11.23). \n11.16 Develop a two-dimensional FFT computer program. An input variable should \nallow the output to be in either a conventional viewing format or standard \ntwo-dimensional FFT format. \nI \n-----1-0-0-0~ \nI \nx \nO\np \n0 1 \no 1 0 \nI \n,y \nFigure 11.21 Sampled waveform for Prob. 11.10 \n. \nâ¢ \nâ¢ Â· Â· â¢ Â· \nâ¢ \nÂ· \nâ¢ â¢ Â· Â· \nâ¢ \n~ \nx \nâ¢ â¢ â¢ \n~ \nâ¢ â¢ â¢ \nâ¢ â¢ â¢ \nFigure 11.22 Functions for Prob. 11.13. \n269 \n270 \nTwo-Dimensional FFT Analysis \nChap. 11 \n11.17 Apply the program developed in Prob. 11.16 to the two-dimensional waveform \nfor Fig. 11.I(a). Explain any differences with the theoretical transform results \nof Fig. Il.l(b). \n11.18 Compute the two-dimensional FFT of the function shown in Fig. 11.7. \n11.19 Compute the two-dimensional FFT of the waveform shown in Fig. 11.2(a). \nExplain any differences with the theoretical transform results of Fig. 11.2(b). \n11.20 Apply the two-dimensional Hanning weighting function to the results of Prob. \n11.18. \nREFERENCES \nI. ROBINSON, E. A., T. S. DURRANI, AND L. G. PEARDON. Geophysical Signal Pro-\ncessing. Englewood Cliffs, NJ: Prentice-Hall, 1986. \n2. ROBINSON, E. A., AND M. T. SILVIA. Digital Foundations of \nTime Series Analysis, \nVol. 2: Wave-Equation Space-Time Processing. Oakland, CA: Holden Day, \n1981. \n3. PAPOULlS, A. Systems and Transforms with Applications in Optics. New York: \nMcGraw-Hill, 1968. \n4. RABINER, L. R., AND B. GOLD. Theory and Application of Digital Signal Pro-\ncessing. Englewood Cliffs, NJ: Prentice-Hall, 1975. \n5. OPPENHEIM, A. V. Applications of \nDigital Signal Processing. Englewood Cliffs, \nNJ: Prentice-Hall, 1978. \n6. ANDREWS, H. c., AND B. R. HUNT. Digital Image Restoration. Englewood Cliffs, \nNJ: Prentice-Hall, 1977. \n7. LEGAULT, R. \"Aliasing Problems in Two-Dimensional Sampled Imagery.\" in L. \nBiberman, ed., Perception of \nDisplayed Information. New York: Plenum, Chap. \n7, 1973. \n8. SWING, R. E. \"The Optics of Microdensitometry.\" Opt. Eng. (November 1973), \nVol. 12, No.6, pp. 185-198. \n9. HUANG, T. S. \"Two-Dimensional Windows.\" IEEE Trans. Audio and Elec-\ntroacoust. (March 1972), Vol. AU-20, No. I, pp. 88-89. \n10. SINGLETON, R. C. \"A Method for Computing the Fast Fourier Transform with \nAuxiliary Memory and Limited High Speed Storage.\" IEEE Trans. Audio and \nElectroacoust. (June 1967), Vol. AU-IS, No.3, pp. 91-98. \nII. BRENNER, N. M. \"Fast Fourier Transform of Externally Stored Data.\" IEEE \nTrans. Audio and Electroacoust. (June 1969), Vol. AU-17, No.3, pp. 128-132. \n12. BUlJs, H. L. \"Fast Fourier Transformation of Large Arrays of Data.\" Appl. \nOpt. (January 1969), Vol. 8, No. I, pp. 211-212. \n13. EKLUNDH, J. O. \"A Fast Computer Method for Matrix Transposing.\" IEEE \nTrans. Comput. (July 1972), Vol. C-21, No.7, pp. 801-803. \n14. ONOE, M. \"A Method for Computing Large-Scale Two-Dimensional Transforms \nwithout Transposing Data Matrix.\" Proc.IEEE(January 1975), Vol. 63, No. I, \npp. 196-197. \nChap. 11 \nReferences \n271 \n15. TWOGOOD, R. E., AND M. P. EKSTROM. \"An Extension of Eklundh's Matrix \nTransposition Algorithm and Its Application in Digital Image Processing.\" IEEE \nTrans. Comput. (September 1976), Vol. C-25, No.9, pp. 950-952. \n16. DELOTTO, I., AND D. DOTII. \"Two-Dimensional Transform by Minicomputers \nwithout Matrix Transposing.\" Comput. Graph. and Imag. Proc. (September \n1975), Vol. 4, No.3, pp. 271-278. \n17. ANDERSON, G. L. \"A Stepwise Approach to Computing the Multidimensional \nFast Fourier Transform of Large Arrays.\" IEEE Trans. Acoust. Speech Sig. \nProc. (June 1980), Vol. ASSP-28, No.3, pp. 280-284. \n18. HARRIS, D. B., J. H. MCCLELLAN, D. CHAN, AND H. S. SCHUESSLER. \"Vector \nRadix Fast Fourier Transform.\" IEEE Int. Conf Acoust. Speech Sig. Proc., \nRec. (May 1977), pp. 548-551. \n19. HOYER, E. A., AND W. R. BERRY. \"An Algorithm for the Two-Dimensional \nFFT.\" IEEE Int. Conf Acoust. Speech Sig. Proc., Rec. (May 1977), pp. 552-\n555. \n12 \nFFT DIGITAL FILTER DESIGN \nDigital filtering is the realization of the convolution integral in discrete form. \nRecall from Ex. 4.4 that the output of a linear system is determined by \nconvolving the system impulse response h(t) with the system input waveform \nx(t). Common signal-processing terminology characterizes h(t) as a filter, \nthat is, the input signal x(t) is filtered by a system with filter impulse response \nh(t) to produce the output signal y(t). \nA straightforward realization of a digital filter can be achieved by sam-\npling the impulse response h(t) and performing the discrete convolution op-\neration with the sampled input waveform x(kT). In the literature, this design \napproach is termed a Finite-Impulse Response (FIR) filter because the sam-\npled impulse response h(kT) is described by N samples. FIR digital filters \nrequire considerable computational complexity because each system output \nvalue y(kT) requires multiplication of the N samples of the sampled impulse \nresponse h(kt) with N sample values of the input signal x(kT) and N -\n1 \nadditions of these product terms. \nComputational complexity can be reduced significantly by imple-\nmenting digital recursive filters. Recursive filtering, as the name implies, is \nrealized by expressing the discrete convolution equation as a summation of \nweighted input sample values x(kT) and a weighted sum of previously com-\nputed output values: \n13 \ny(kT) \n~ \na;x[(k -\ni)T] + ~ \nbiy[(k - j -\nl)T] \n(12.1) \n;=0 \n272 \nSec. 12.1 \nFFT Time-Domain Digital Filter Design \n273 \nRecursive filters can reduce the required number of multiplications by an \norder of magnitude with respect to a FIR filter implementation. However, \nthe design of a recursive digital filter is complicated if the filter function to \nbe realized is not characterized by a well-defined analytical function [1]. \nIn this chapter, we discuss the basic techniques for applying the FFT \nto the design and implementation of nonrecursive (i.e., FIR) digital filters. \nFFT digital filter design can be accomplished by two basic techniques. We \ncan begin with the desired time-domain impulse response of the filter (time-\ndomain specification) or with the desired filter frequency-domain response \nfunction (frequency-domain specification). In either case, the filter response \nfunction can be specified by an analytic function or by experimental samples. \nFFT digital filter designs are particularly valuable where the impulse or \nfrequency response of the filter has been determined experimentally. Unless \nthe experimental data describes a well-known filter shape, then the design \nof a suitable recursive digital filter is extremely time consuming (or \nimpossible). \nOur design approach is focused on design-time efficiency and the prac-\nticality of implementation. Practicing professionals must constantly evaluate \nthe time required to design a better digital filter against the savings in data-\nprocessing time that results from a more elegant filter design. The FFT design \napproach presented here is best suited for quick-solution laboratory analysis \nor for signal-processing problems involving unconventional filter response \nfunctions. Our discussion of system-simulation analysis in Chapter 14 is an \nexample where our techniques are more efficient to apply than more elab-\norate design techniques. \n12.1 FFT TIME-DOMAIN DIGITAL FILTER DESIGN \nAssume we are given the time-domain impulse response of a desired filter \ngraphically, analytically, or as numerical values determined from an exper-\niment. It is desired to design a nonrecursive digital filter that produces results \nequivalent to the specified filter. The digital filter is to be implemented by \nFFT convolution techniques (Chapter 10) and, therefore, its impulse re-\nsponse must be of finite duration. The design of a FFT digital filter from a \ntime-domain specification is very similar to the development of the discrete \nFourier transform that is graphically developed in Fig. 6.2. \nDesign Procedure \nIf \nthe time-domain specification is an analytical function, we begin our \ndesign by sampling the given impulse-response function. For an experi-\nmentally obtained impulse function, we begin with the numerical sampled \nvalues. In both cases, we require that the sample interval T is sufficiently \n274 \nFFT Digital Fi Iter Design \nChap. 12 \nsmall to produce negligible aliasing. Because the FFT is used to implement \nthe designed digital filter, it may be necessary to truncate the sampled im-\npulse response. If truncation is not required, we know from Chapter 6 that \nthe discrete Fourier transform of the digital filter is a good approximation \nto the continuous Fourier transform of the specified analog filter. The sam-\npled impulse-response function thus satisfies the digital filter design goal in \nthat its frequency function is a good approximation to the specified frequency \nfunction. Further, the filter can be implemented by means of FFT convo-\nlution techniques. \nRecall from Table 10.2 that efficient application of FFT sectioning tech-\nniques requires that the fillter impulse response be represented by a small \nnumber of samples with respect to N, the number of sample values to be \nFFTed. If \nan experimenter's computer capacity accommodates a digital filter \ndesigned without impulse-response truncation, then the design is complete. \nHowever, it is often the case that the number of nonzero samples that define \nthe impulse response of the digital filter must be minimized. \nThe number of nonzero samples of the impulse-response function can \nbe modified by multiplication with a truncation (weighting) function. As \nshown in Figs. 6.2(d) and (e), time-domain truncation can introduce ripples \nin the frequency function unless a weighting function that smoothly tapers \nto zero is used. To determine an acceptable truncation width, we experi-\nmentally decrease the width, or duration, of the weighting function until the \nresulting digital filter frequency function as computed with the FFT differs \nunacceptably from the desired analog frequency function. The minimum \nwidth of the weighting function that yields acceptable results corresponds \nto a minimum impulse-response duration and hence a more efficient FFT \nimplementation. \nExample 12.1 FFT Digital Filter Design: Time-Domain Specification \nTo illustrate the FFT filter design procedure, consider the filter impulse function: \nI ~ \n0 \n= 0 \n1<0 \nThe Fourier transform is given by \nwhere \nH(f) = 0. 2/(0. + J27rf)2 \n= I \nH(f) I \nejo(f) \nI \nH(f) I = 0.2/{[0.2 -\n(27rfff + (47rfo.f}I/2 \n6(f) = tan -I{ -47rfo./[0.2 -\n(27rf)2]) \n(12.2) \n(12.3) \n(12.4) \n(12.5) \nThe parameter 0. was chosen as 27r to yield a filter frequency-domain 6 dB cUloff \nfrequency of 1 Hz. We show the time-domain impulse response function ofEq. (12.2) \nand the amplitude and phase response functions of Eqs. (12.4) and (12.5) in Figs. \n12.1(a) and (b), respectively. \nSec. 12.1 \n4.0 \n3.0 \nw \no \n:::l \nt:: \n~ 2.0 \n~ \n1.0 \nÂ·10 \nÂ·20 \nÂ·30 \n~ \nÂ·40 \nw \no \n~ Â·50 \n:::; \nIl. \n~ Â·60 \nÂ·70 \nÂ·80 \nÂ·90 \nÂ·100 \nFFT Time-Domain Digital Filter Design \n\"'\" \nI I~ \n.25 \nhIt) ; a'teÂ·at \na;21T \n\"-~ \nI----\n.5 \n(a) \n.75 \nTIME (SEC) \n1.0 \nFREQUENCY (Hz) \n10 \n100 \n1000 \n~ \n, \n-........ \n-AMPLITUDE RESPONSE \n, , \nI , \n.... \nt'-.... \nIH(I))-\na \n-\nÂ·30 \n.... \n\"\" \n- [Ia\" (2m),]' + (4ma)'j\"-\n.... \nÂ·60 \n.... \n, \n-\n, \n--PHASE RESPONSE \niii \nw \n\"\" \n8(1 -\n-'[ Â·4ma 1 \nÂ·90 \nw \n, \n) - tan a'. (2m)' \na: \n, \nt!l \n.... \nÂ·120 e \n, \n\\ \n, \n.... \na; 21T \nw \n.... \nÂ·150 ~ \n, \"- ~- - -----\n:I: \n---------- .180 Il. \n'\\ \nÂ·210 \n'\\ \n\\ '\\ \n'\\ \n(b) \n275 \nFigure 12.1 Example filter continuous time- and frequency-domain response functions. \n276 \nFFT Digital Filter Design \nChap. 12 \nTo sample the impulse response of Eq. (12.2), we must choose the sample \ninterval T. We know that T must be sufficiently small to minimize the effect of \naliasing. Let us assume that our design goal is a digital filter whose magnitude re-\nsponse approximates that shown in Fig. 12.I(b) to approximately 500 Hz. As a result, \nwe must ensure that we choose the sample interval T such that aliasing is negligible \nup to a frequency of 500 Hz. For a sampling frequency fs = 1000 Hz, thefo/dover, \nor aliasing, frequency is f sl2 = 500 Hz, and we have ensured that aliasing is rea-\nsonably negligible below 500 Hz. \nWe next choose the maximum number of sample values N that can be con-\nveniently FFTed. We assume for purposes of discussion that N = 2048. With T = \nIIfs = 0.001 and N = 2048, we sample Eq. (12.2) to obtain \nk = 0, I, ... , 2047 \n(12.6) \nThe 2048 samples of Eq. (12.6) represent an impulse duration of 2.048 s. We observe \nfrom Fig. 12.I(a) that the choice of 2048 samples (or 2.048 s) introduces negligible \ntruncation. Hence, we expect the FFT of the sampled impulse response and the \ncontinuous Fourier transform to agree closely. The FFT of the function given by \nEq. (12.6) for the chosen parameters is essentially the same as samples of \nFig. 12.I(b). \nThe next step in our time-domain FFT digital filter design procedure is to \ntruncate the impulse response in order to minimize the number of near-zero sample \nvalues. From Table 10.2, the number of samples representing the impulse-response \nfunction should not exceed 299 for efficient FFT convolution with a 2048-point trans-\nform. Hence, we will experimentally sequentially reduce the width of the truncation \nfunction with a goal of reducing the digital filter impulse-response duration to 299 \nsample values (0.299 s). \nTo truncate the sample impulse response of Eq. (12.6), we have arbitrarily \nchosen for discussion the rectangular and Hanning truncation (or weighting) func-\ntions. Hence, for the rectangular truncation function, we compute \nh(kT) = h(kT) \n= 0 \nO:sk:sWHIT \nWHIT < k :s N -\nWHIT \nand for the Hanning truncation function, we compute \nh(kT) = h(kT)[ 112 + 1/2 cos('lTkTIW \nH)] \n= 0 \no \n:s k :s WHIT \nWHIT < k:s N -\nWHIT \n(12.7) \n(12.8) \nWe then compute the FFT of Eqs. (12.7) and (12.8) to determine the frequency \ncharacteristics of the truncated filter. \nIf \nwe choose W \nH to reduce the impulse response to 1 s (i.e., 1000 samples), \nthen the results of truncation become apparent. In Fig. 12.2(a), we see that rectan-\ngular truncation produces a small rippling effect. Application of the Hanning trun-\ncation function produces the digital filter shown in Fig. 12.2(b), which rather closely \napproximates the desired amplitude- and phase-response characteristics. Note the \neffects of aliasing on the amplitude response around 500 Hz. \nThe digital filter illustrated in Fig. 12.2(b) is characterized as one of \nquick and easy design. However, the filter impulse response is represented \nby 1000 nonzero sample values. The designer must evaluate the trade-off \nSec. 12.1 \nFFT Time-Domain Digital Filter Design \nt-_, \n----\nFREQUENCY (Hz) \n10 \nI \n100 \n1000 \n............ \n.............f'..... \n.101----.:..,..,--+'--\" \n'...-__ \nAMPLITUDE RESPONSE _ \nÂ·30 \n, \n\"\" \n- -\n-\n-\n-\n-\nPHASE RESPONSE \n.... \nÂ·60 en \n.201------'\"'<+-----''<--\nRECTANGULAR TRUNCATION WR = 1.0 \n_ \nttl \n, \n\\. \nÂ·90 :Â§ \n.301-------+---\"''<----.....,,~+_------1__-----___+ \n.120 e \n-\n\"\"\" \n\\ \n150 ~ \n~.401-------+---~~-~------1-------~Â· \n~ \n~ \n\"- ~----~ \n.. \n-180 \n::J \nÂ·210 \n~ \n.501_------+------+--~\\.~---~-----~ \n<>. \n~ \n.601-------+------+---~~\"\"'--~-----~ \n.701-----------~----------_+--------~'\\~~----------~ \n.801_------+------+-------~~~----~ \n.901_------+------+-------1--~~~---~ \nÂ·1001-------L------~------L---~~~_~ \n(a) \nFREQUENCY (Hz) \n10 \n100 \n1000 \nÂ·10 \n-20 \nÂ·30 \nill Â·40 \n!< \nw \nC -50 \n:::l \nf-\n::J \n<>. -60 \n::;: \n~ \n-70 \n-.... \n.:::......... \nI \nI \n..... , \nr---.... \nAMPLITUDE RESPONSE \n-30 \n~ \n-\n\" \n------\nPHASE RESPONSE \nen \n.... \n-60 \nw \n, \n.---\nTHEORETICAL RESPONSE \nw \n\\. \n-\na: \nHANNING TRUNCATION WH Â· 1.0 \n-90 .., \n, \nw \n\\ \nc \nÂ·120 ;;;-\n\" '\\ \nen \n.... \n~ \n\"-\nÂ·150 ~ \n-- ~------ --------\n-180 \n\\. \nÂ·210 \n\\ \n\\ \n-80 \n-90 \nÂ·100 \n~ \n, \n'\\ \nb \n( ) \nFigure 12.2 FFf filter design amplitude- and phase-response characteristics: (a) \nrectangular truncation at 1.0 s, and (b) Hanning truncation at 1.0 s. \n277 \n278 \nFFT Digital Filter Design \nChap. 12 \nbetween increased design time and decreased processing time for a filter \nwith a reduced number of nonzero sample values. In general, we recommend \nthat the designer accept the inefficiency of \nimplementation caused by the large \nnumber of nonzero sample values of the filter impulse response unless there \nare compelling reasons to further decrease data-processing time. \nTo reduce this number of samples, we decrease W \nH. The digital filter \napproximation to the desired characteristics continue to degrade as W \nH is \ndecreased. Figure 12.3(a) illustrates amplitude and phase functions resulting \nfrom rectangular truncation with W \nR = 0.5 s (500 samples). As shown, \nrectangular truncation produces unacceptable rippling. In Fig. 12.3(b), we \nshow the filter design characteristics with Hanning truncation width W \nH = \n0.5 s. Although the design approximates rather closely the theoretical re-\nsponse, the amplitude function differs from the desired frequency response \nin that the low-frequency components have been attenuated by approxi-\nmately 4 dB. \nHanning truncation reduces the area under the impulse-response func-\ntion and, as a result, produces the attenuation that we observe. Recall that \nthe Fourier transform for zero frequency is simply the integral or area of \nthe impulse-response function. If this attenuation is objectionable for the \ndigital filter design of concern, then the effect can be partially compensated. \nWe simply multiply the truncated impulse response by the appropriate con-\nstant that yields a windowed impulse response with an area equal to that of \nthe theoretical impulse-response function. \nFigure 12.4 illustrates the amplitude-response functions obtained by \nmultiplication by the appropriate constant to increase the area of the trun-\ncated impulse response. A comparison of the truncated impulse response of \nFig. 12.4 and the truncated impulse response of Fig. 12.3(a) illustrates the \neffect of multiplication. \nThe amplitude-response function that results from multiplication agrees \nmore closely with the desired response at the lower frequencies but is shifted \nto the right at higher frequencies. That is, the amplitude-response charac-\nteristic shows the 6 dB cutoff is at approximately 1.5 Hz rather than at the \ndesired 1 Hz. If the exact cutoff frequency is of importance, then we can \nbegin our design procedure with a specified cutoff frequency that is lower \n(i.e., 0.5 Hz). The phase-response function is not affected by the multipli-\ncative constant. \nFor W \nH = 0.5 s, we have developed a digital filter represented by 500 \nsamples values. Although we have not reached our goal of 299, recall from \nthe discussion of Sec. 10.3 that an increase of this optimum number of sam-\nples by a factor of 2 increases computing time only slightly. As a result, this \ndigital filter can be implemented efficiently by FFT convolution techniques. \nIn summary, we have followed the design procedure of successively \nreducing parameter W \nH, the truncation function width. We note that in each \nreduction of the parameter W \nH, we must accept a compromise in the digital \nSec. 12.1 \n.1 \nFFT Time-Domain Digital Filter Design \n-\n.... \nFREQUENCY (Hz) \n10 \nÂ·101-----..:~----1~'_--\nAMPLITUDE RESPONSE \n-\n---\nPHASE RESPONSE \n-\n- -\nTHEORETICAL RESPONSE \nÂ·201-----~f----..3~ RECTANGULAR TRUNCATION WR = 0.5 \nÂ·30 \nÂ·60 \nCii \nÂ·90 ttl \na: \nÂ·120 m \ne \niii \n:g \n- --IIIII---l Â·150 W \n~ \nw \n0 \n::J Â·50 \nI-\n::::; \n... \n~ Â·60 \nÂ·70 \nÂ·80 \nÂ·90 \nÂ·100 \nÂ·10 \nÂ·20 \nÂ·30 \n~ \nÂ·40 \nw \no \n~ \nÂ·50 \n::::; \n... \n~ \nÂ·60 \nÂ·70 \nÂ·80 \nÂ·9 \n0 \nÂ·100 \nÂ·180 iE \n4 \nÂ·210 \n2 \n(a) \nFREQUENCY (Hz) \n10 \n100 \n1000 \n--\n~-.~ \n--AMPLITUDE RESPONSE \n,~ \n.. , \n-..;: r-.... \n, \nÂ·30 \n---- PHASE RESPONSE \n~.r\\. \n\"\" \nI \n\" \n\" \n---THEORETICAL RESPONSE \nÂ·60 Cii \nI \n\" ' \nHANNING TRUNCATION WH = 0.5 _ \nW \n~ \nW \n, \nÂ·90 a: \n~:~\" \nCO \nw \n\" \n'\\ \nÂ·120 ~ \n\". \nw \n, , \nÂ·150 ~ \n\"~ \n:I: \n,,~~ \n~-------\n... \n----------\nÂ·180 \n~ \n4 \n~ \nÂ·210 \nf---3 \n2 /'.... \n'\\ \n111 \n\"-\n\\ \n0.25 \n0.50 \nt \n1\\ \n, \nIMPULSE RESPONSE \n\"-\nHANNING1TRUNCATION - W1 \n= 0.5 \n(b \nFigure 12.3 FFT filter design amplitude- and phase-response characteristics: (a) \nrectangular truncation at 0.5 s, and (b) Hanning truncation at 0.5 s. \n279 \n280 \nÂ·10 \nÂ·20 \nÂ·30 \n~ \n-40 \nw \no \n~ -50 \n::::; \n... \n-~ \n5 \n4 \n~ \nFFT Digital Filter Design \nChap. 12 \nFREQUENCY (Hz) \n10 \n100 \nT \nI \n1000 \nNORMALIZED HANNING TR~NCATION \nWH = 0.5 \n'~ \n._- THEORETICAL RESPONSE \nI \n-- NORMALIZED AMPLITUDE \n'~ \nRESPONSE \n'\\\\ \n'\\., \n'\\. \n,'\\ \nf---3 ,....... \n'\\ \n'\\. \n~ -60 \n2 \n1 \n'\" \n'\\ \n'\\ \n0.25 \n0.50 \nI \n,'\\ \nI \nNORMALIZED IMPULSE RESPONSE \n.~ \nHANNING TRUNCATION - WH = 0.5 \nI \nÂ·70 \nÂ·80 \n-90 \nÂ·100 , \nFigure 12.4 FFT filter design amplitude response with Hanning truncation at 0.5 \ns and normalized impulse-response area. \nfilter design. As W \nH is decreased, we encounter an increased spreading of \nthe frequency function. Hence, a design for maximum implementation ef-\nficiency (Le., minimum number of sample values) deviates from the desired \ncharacteristics. However, if we are willing to represent the impulse response \nby a large number of sample values, then, as shown, we can achieve an \nexcellent approximation to the specified filter frequency response and the \ndesign can be achieved very efficiently. \n12.2 FFT FREQUENCY-DOMAIN DIGITAL FILTER DESIGN \nFrequency-domain specification of \na filter implies that the digital filter design \nbegins with an analytical expression for the frequency response of a filter \nor with numerical values of amplitude and phase obtained from an experi-\nment. As in the time-domain specification case, the goal is to design a digital \nfilter that approximates the given frequency response and that can be im-\nplemented by FFT convolution. One could claim that this is identical to the \nprevious discussion; however, there are subtle design differences in the two \ndevelopments. In this section, we will investigate these differences as well \nas develop the fundamentals of FFT frequency-domain digital filter design. \nSec. 12.2 \nFFT Frequency-Domain Digital Filter Design \n281 \nGraphical Development \nConsider the time- and frequency-response functions of the example \nfilter shown in Fig. 12.5(a). Our approach to digital filter design follows the \npresentation used to develop the discrete Fourier transform in Chapter 6. \nBecause we assume that the filter characteristics are known to us only in \nthe frequency domain, it is necessary to first sample in the frequency domain. \nThe frequency-sampling function and its inverse Fourier transform are \n~ \n, \"lit) \n-To \n1 \nTo \nI \n!\\ \nV\\ ( \nI \ng \n10) \ng \nIb) \ng \nIe) \nJK' \n1 \nd,ll) \nÂ· \n.. 1111111 t \n11111 III I! I! I \n.. Â· \n-'---!1!--\nI \nTo \nâ¢ JHIf)d, (f)1 \nIGIII) \n---I \nN \nFigure 12.5 Graphical development of FFT digital filter designed from a fre-\nquency-domain specification. \n282 \nFFT Digital Filter Design \nChap. 12 \nshown in Fig. 12.5(b). From the time-convolution theorem, the resulting \nsampled frequency function and its inverse Fourier transform are shown in \nFig. 12.5(c). Because sampling in the frequency domain corresponds to con-\nvolution in the time domain, we can have time-domain aliasing. It is nec-\nessary that the frequency-domain sampling period liTo is sufficiently small \nto ensure that time-domain aliasing is negligible. As illustrated in Fig. 12.5(c), \naliasing is negligible for this example. \nBecause only a finite number of \nsample values of \nthe frequency function \ncan be inverse Fourier transformed, it is necessary to truncate the sampled \nfrequency function. As illustrated in Fig. 12.5(d) we attempt to ensure that \nthe truncation function significantly exceeds the width of \nthe frequency func-\ntion. A rectangular truncation function is used to simplify the graphical \ndevelopment. \nWe recognize that the wider one allows the frequency-truncation func-\ntion to become, the larger the number of sample values representing the \nimpulse response becomes. Our design approach ultimately reduces the \nnumber of impulse-response samples by employing a time-domain weighting \nfunction design procedure similar to that discussed in the previous section. \nAs a result, we incur no penalty for choosing the frequency-truncation func-\ntion extremely wide. Figure 12.5(e) illustrates the Fourier transform pair \nobtained by truncating the sampled frequency response. \nTo complete the description of \nthe digital filter, it is necessary to sample \nwith the time-domain sampling function shown in Fig. 12.5(f). Note that the \nsampling period T has already been set because NT must equal To. The \nresulting sampled functions shown in Fig. 12.5(g) represent the digital filter \nas designed from a frequency-domain specification. If we ensure that time-\ndomain aliasing and frequency-domain truncation effects are insignificant, \nthen the digital filter of Fig. 12.5(g) is essentially equivalent to that discussed \nin Sec. 12.1; we simply proceed from this point using the design techniques \ndescribed in the previous section. \nThe design of FFT digital filters from a frequency-domain specification \nappears to be a straightforward extension of time-domain design techniques. \nThis is in fact the case if we exercise caution with respect to two pertinent \nassumptions: frequency-domain truncation and time-domain aliasing. We \nwill now explore further the implication of time-domain aliasing in FFT \nfrequency-domain filter design. \nTime-Domain Aliasing and End Effects \nTo demonstrate the potential problem of \ntime-domain aliasing, we show \nin Fig. 12.6(a) a filter frequency-response characteristic that digital filter \ndesigners often try to obtain. Following the design procedures for frequency-\ndomain design of digital filters that were described in Fig. 12.5, we first \nsample in the frequency domain using the sampling function illustrated in \nSec. 12.2 \nFFT Frequency-Domain Digital Filter Design \n283 \nL \ng \nm \n'-' \n'-\"\" \nlal \nr\"\" \ng \nIÂ·\"\" \nIbl \n\"'llIlII1! 11.1 ! \n1111111 t \n.. Â· \nÂ·To \nTo \nI \n-1,1--\nI \nTo \nÂ£\"\"\"\" \nHIIIA,III \ng \n~ \n~ \n~ (' \nlei \nV \nVV V \n-1: \n/\"\" \ng \nI \nÂ·v VÂ· \nIdl \nhIt!Â· A ,It!'xlll \nH(f)A,II)XII) \n. -\ng \n, \n, \n, \nIe) \nf-NT= T.-! \nI-NT=T.-I \nÂ·Â·Â·1 11111 II II 1 \niii'; \n111111Â·Â·Â· \ng \nI \n~'\" \nII) \nglt! \nGIl) \ng \nIg) \nFigure 12.6 Graphical development of an apparently perfect rectangular filter. \nFig. 12.6(b). We purposely have chosen a large frequency sample interval \nso that there is significant time-domain aliasing, as shown in Fig. 12.6(c). \nRecall that the interval liTo is the variable that the FFT filter designer uses \nto limit time-domain aliasing to an acceptable level. \nThe filter frequency-response function is of finite duration and, as a \nresult, multiplication by a frequency-truncation or weighting function of \ngreater duration, as shown in Fig. 12.6(d), introduces no distortion but does \nset the truncation width NT. Time-domain sampling with N samples over \nthe time-function period To is achieved by multiplication with the time-sam-\n284 \nFFT Digital Filter Design \nChap. 12 \npIing function illustrated in Fig. 12.6(0. The resulting time- and frequency-\ndomain sampled approximations to the desired time- and frequency-domain \nfilter characteristics are shown in Fig. 12.6(g). \nBased on the illustrations of Fig. 12.6, one could conclude that a filter \nhas been designed that has a perfectly square frequency response. This, \nhowever, is not the case as this design cannot be implemented by FFT \nconvolution techniques. Recall that in order to apply FFT convolution tech-\nniques without end effects (Sec. 10.1), it is necessary that the number of \nnonzero samples defining the digital filter impulse-response function be less \nthan the total number of samples to be FFTed. As a result, zeros must be \nadded to the N points of the first period of get), Fig. 12.6(g). The impulse \nresponse with appended zeros then defines the frequency-function charac-\nteristics of the designed filter. \nTo illustrate this addition of zeros, we repeat the time and frequency \nfunctions of Fig. 12.6(g) in Figs. 12.7(a) and (d), respectively. Assume that \nthe number of data points to be processed by the FFT is 2N; the N points \ndefining the digital filter of Fig. 12.6(a) must therefore be combined with N \nzeros to form a periodic function of period 2NT. To determine the time-\ndomain result of adding these zeros, we multiply by the periodic square-\nwave function illustrated by Fig. 12.7(b); the corresponding time function \nis shown in Fig. 12.7(c). Multiplication of the functions of Figs. 12.7(a) and \n(b) yields the results illustrated in Fig. 12.7(c), a periodic function with 2N \npoints per period. The frequency function corresponding to Fig. 12.7(c) is \nobtained by convolution of Figs. 12.7(d) and (e). This function is illustrated \nin Fig. 12.7(0; the function no longer closely approximates the desired fre-\nquency-domain response function. \nAs shown, the addition of zeros results in rippling in the digital filter \nfrequency response. To reduce this effect, it is necessary to substitute a \nsuitably shaped time-domain weighting function for the rectangular function \nused in Fig. 12.7(b). \nExample 12.2 Notch Filter Design \nAs an illustration of FFT frequency-domain digital filter design, consider the fre-\nquency function illustrated in Fig. 12.8(a). As shown, the desired filter function has \nnotch filter characteristics between 1.5 and 2.5 Hz and a cutoff frequency of 5 Hz. \nWe wish to design a digital filter approximation to the illustrated filter function. \nAssume that the data to be filtered has no frequency component above 10 Hz. \nHence, our time-domain sample interval T should be less than 0.05 s. For N = 1024, \nchoose T = 0.0391 s and thus ilf = 0.25 Hz. When sampling the frequency function \nillustrated in Fig. 12.8(a), recall that the computation of the inverse FFT requires \nthat we must fold the real frequency function about n = N12. We assume the phase \nfunction is zero and hence the imaginary frequency function is zero. Note that be-\ncause the phase function is zero, then the frequency function is even and hence the \nimpUlse-response function is even (noncausal). \nThe inverse FFT (scaled by T) is shown in Fig. 12.8(b). This impUlse-response \nSec. 12.2 \nFFT Frequency-Domain Digital Filter Design \ng(1) \n/ \nt \nj\"'\" \n... \n(bl \nt \ng(tlw(1) \n, \nI , \n~ \nI ' \n, \n-\n2N \nGill \n(dl \n(f) \nFigure 12.7 Graphical development illustrating the effect on filter amplitude re-\nsponse resulting from appending zeros to the impulse response. \n285 \nfunction is also symmetric about n = N12, but we show only values for positive \ntime. Following our design procedure, we apply a Hanning truncation or weighting \nfunction to the impUlse-response function shown in Fig. 12.8(b). The weighting func-\ntion is positioned to have unity value at time t = 0, zero value at time t = Te. and \nto be symmetric about n = N12. We compute the FFT of the weighted impulse \nresponse to determine the frequency response of the designed filter. The log am-\nplitude response of the FFT designed digital filter is illustrated in Fig. 12.8(c) for T, \n286 \nH(I) \n1.0+----~ \nh(l) \n8 \n6 \n4 \n2 \n-2 \n-10 \n-20 \nCD \n-30 \n\" \nw \n0 \n-40 \n:::> \nt: \n-' \n0-\n-50 \n:; \n<t \n(!) -60 \n0 \n-' \n-70 \n-80 \nFFT Digital Filter Design \n2 \n2 \n2 \n,,-\n/,' \n\\ \n'-\" \nI \n\\ \nI \n\\ \nI \n, \nI \n\\ \n: \nI \nI \nI \nI \nI \n3 \n4 \n(a) \n3 \n(b) \n3 \n4 \n: \n---Tc=10sec. \nI \n: \n1'\\ \" I \n------ Te = 2.5 sec. \n1\n1 \n.. \n/ '1 \nII!: \n---T,=l.Osec. \njl \nII \nII \nU \nd ! \n(c) \n5 \n6 \n4 \n5 \n5 \n6 \nChap. 12 \nI-Hz \nI-sec. \nI-Hz \nFigure 12.8 FFI notch filter designed from a frequency-domain specification. \nSec. 12.2 \nFFT Frequency-Domain Digital Filter Design \n287 \n= 1,2.5, and 10 s. As shown, severe truncation (Tc = 1 s) results in a filter whose \ncharacteristics are not useful. \nWe use FFT convolution techniques to implement the designed filter. For T \n= 0.0391 s, a 1024-point FFT can process 40.04 s of \ndata. The filter impulse response \nfor Tc = 2.5 s has a duration of 5 s because we must include the negative time values \n(mirror image) if a zero-phase filter is desired. If this filter is acceptable to the de-\nsigner, it can process data rather efficiently. A 20-s duration impulse response (Tc \n= 10 s) gives excellent attenuation characteristics but is less efficient. However, we \nH'(n) \n-\nr---\n,...-- -\n0 \nN \nNÂ·' \nn \n2\" \n(a) \nh(k) \n8 \n4 \nN \nNÂ·' \nk \n2\" \n(b) \nw(k) \nTc \nN \nNÂ·' \nk \nT \n2\" \n(e) \nh'(k) \n8 \n4 \n2Tc \nN \nN-! \nT \n2\" \n(d) \nFigure 12.9 Illustrations showing correct formatting of time and frequency func-\ntions for the digital filter design of Fig. 12.8. \n288 \nFFT Digital Filter Design \nChap. 12 \nargue again that the designer should predetermine if design simplicity or data-pro-\ncessing time is of first priority. \nFor clarity of presentation, Figs. 12.9(a) to (d) illustrate the appropriate fre-\nquency function to be sampled, the FFT computed impulse response, the Hanning \nweighting function, and the zero-phase-shift filter impulse response, respectively. \nThe impulse response of Fig. 12.9(d) is that which must be convolved with the data \nby means of the FFT if a zero-phase-shift filter is desired. \nSummary \nIn this chapter, we have explored the basic concepts of nonrecursive \nFFT digital filter design. The advantages of the proposed design procedure \nare design efficiency and implementation simplicity. A disadvantage is that \na large number of sample values may be required to adequately represent \nthe filter impulse-response function. Implementation of a digital recursive \nfilter can often give an order-of-magnitude speed advantage over an FFT \nconvolution implementation of a digital filter [I]. If the filter to be used is \nof a conventional shape and vast quantities of data are to be processed, then \ncertainly the time spent designing a digital recursive filter is worthwhile. \nBut if the design is for an experimental effort or if the filter function is of \nunusual shape, then the FFT design procedure presented here is a simple \nand cost-effective approach. \nAlternate truncation or weighting functions can be employed to alter \nthe side-lobe characteristics of FFT filter designs. Recall from Sec. 9.2 that \nthe side-lobe level can be specified for the Dolph-Chebyshev weighting func-\ntion. Using the previously described design approach, Helmns [2] has applied \nthe Dolph-Chebyshev weighting function to FFT digital filter design prob-\nlems and has achieved a specified side-lobe level. \nPROBLEMS \n12.1 Analytically determine the Fourier transform and plot the amplitude and phase \nspectrums of the waveform h(t) = a 2te -at, where a = 2 and t > O. \n12.2 If \nthe aliasing level that one is willing to accept has been set at x dB, then why \ndoes one select the crossover frequency to be that where the frequency function \nis down (x + 3) dB? \n12.3 If \nthe waveform of Prob. 12.1 is sampled at \n(a) 250 \n(b) 500 \n(c) 1000 \n(d) 1500 \nsamples per second, what is the aliasing level in dB? \n12.4 When the Hanning weighting function is applied to the impulse-response func-\ntion illustrated in Figure 12.l(a), should the weighting function have its max-\nChap. 12 \nReferences \n289 \nimum amplitude centered on the origin or at the midpoint of \nthe interval [0, W \nH]? \nExplain. \n12.5 Design a digital filter to be implemented using FFT convolution that approx-\nimates the following time-domain specified filters: \n(a) h(t) = e- I \n(b) h(t) = e -I COS(21Tt) \n(c) h(t) = [sin2(t)]lt 2 \n12.6 The FFT time-domain specification technique for digital filter design is of con-\nsiderable importance in those cases where designs for digital recursive filters \ndo not exist. Give examples. \n12.7 Refer to the impulse-response function of Fig. 12.5(g). Explain the result if the \nN sample values shown are used to implement a digital filter. \n12.8 Refer to Fig. 12.6. Why can't the problems cited be resolved by selecting the \nfrequency-sampling interval in Fig. 12.6(b) much smaller than that shown? \n12.9 Design a digital filter to be implemented using FFf convolution that approx-\nimates the following frequency-domain specified filters: \n1 \n(a) H(f) = 1 + (21Tf)2 \np \n(b) H(f) = f4 + 1 \n(c) H(f) = Sin(21Tf~:;S(21Tf) \nAliasing should be below - 50 dB. \nREFERENCES \n1. OPPENHEIM, ALAN V., AND R. W. SCHAFER. Digital Signal Processing. Englewood \nCliffs, NJ: Prentice-Hall, 1975. \n2. HELMS, H. D. \"Non-recursive Digital Filters: Design Methods for Achieving \nSpecification on Frequency Response.\" IEEE Trans. Audio and Electroacoust. \n(September 1968), Vol. AU-16, No.3, pp. 336-342. \n3. RABINER, L. R., AND B. GOLD. Theory and Application of Digital Signal Pro-\ncessing. Englewood Cliffs, NJ: Prentice-Hall, 1975. \n4. RADER, C. M., AND B. GOLD. \"Digital Filter Design Techniques in the Frequency \nDomain.\" Proc. IEEE (February 1967), Vol. 55, No.2, pp. 149-171. \n5. HAMMING, R. W. Digital Filters, 2d ed., Signal Processing Series. Englewood \nCliffs, NJ: Prentice-Hall, 1983. \n6. MCCLELLAN, J. H., T. W. PARKS, AND L. R. RABINER. \"A Computer Program \nfor Designing Optimum FIR Digital Filters.\" IEEE Trans. Audio and Electro-\nacoust. (December 1973), Vol. AU-21, No.6, pp. 506-526. \n7. GOLD, B., AND K. L. JORDAN, JR. \"A Direct Search Procedure for Designing \nFinite Duration Impulse Response Filters.\" IEEE Trans. Audio and Electro-\nacoust. (March 1%9), Vol. AU-17, No.1, pp. 33-36. \n8. RABINER, L. R., B. GOLD, AND C. A. MCGONEGAL. \"An Approach to the Ap-\n290 \nFFT Digital Filter Design \nChap. 12 \nproximation Problem for Nonrecursive Digital Filters.\" IEEE Trans. Audio and \nElectroacoust. (June 1970), Vol. AU-18, No.2, pp. 83-106. \n9. RABINER, L. R., AND R. W. SCHAFER. \"Recursive and Nonrecursive Realizations \nof Digital Filters Designed by Frequency Sampling Techniques.\" IEEE Trans \nAudio and Electroacoust. (September 1971), Vol. AU-19, No.3, pp. 200-207. \n13 \nFFT MULTICHANNEL \nBAND-PASS FILTERING \nIn radar, sonar, communications, and signal processing systems, the appli-\ncation of the FFT to digital multichannel band-pass filtering is of major \nimportance. These fields of FFT applications are based on the interpretation \nof \neach FFT resolution cell as the output of \na band-pass filter. In this chapter, \nwe develop graphically and analytically the fundamentals of FFT digital \nband-pass filtering. \nWe first explore the analogy of the FFT to a bank of integrate and \nsample filters. Both graphical and mathematical presentations are devel-\noped. We then review the data-weighting function discussed in Sec. 9.2 from \na filter-shaping perspective. The relationship of FFT resolution to band-pass \nfilter response characteristics is explored in detail. \nThe FFT filtering development is then extended to the interpretation \nof a sequence of FFT outputs as sequential time samples from a bank of \nband-pass filters. Interpreting FFT outputs as time samples is a concept that \nis contrary to one's intuition. One normally considers the FFT as a time-\nto-frequency transform. For these reasons, basic implementation consid-\nerations that one employs for FFT multichannel filtering are explored in \ndetail. Numerous examples are presented to solidify the development. \n13.1 FFT BANDÂ·PASS INTEGRATE AND SAMPLE FILTERS \nAn interpretation of the FFT that has been found useful is its realization as \na bank of band-pass integrate and sample filters. Such an interpretation \nrequires we show the FFT can be viewed as a time-domain convolution \n291 \n292 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \noperation. This follows because a filter is a linear system and the output of \na linear system is the convolution of the input to the system and the system \nimpulse response (Ex. 4.4). Hence, to show that the FFT can be interpreted \nas a bank of filters, we must show that the FFT operation is a convolution \noperation involving a set of \nimpulse-response functions whose Fourier trans-\nform (i.e., transfer or system frequency-response functions) are those of a \nband-pass filter bank. In this section, we develop both analytically and graph-\nically this interpretation of the FFT. \nDevelopment of Band-Pass Filter Equations \nTo develop the analytical relationships describing the FFT as a bank \nof band-pass integrate and sample filters, consider the discrete Fourier trans-\nform approximation to the continuous Fourier transform: \nN-1 \nY(nINT) = T ~ \ny(kT)e -j271'nklN \nk~O \nn = 0, I, . . . , N \n12 \n(13.1) \nRecall that Eq. (13.1) is simply the rectangular integration approximation to \nthe finite-interval continuous Fourier transform integral. Hence, if T is suf-\nficiently small, then Eq. (13.1) can be written with small error as \n(NT \nY(nfo) = Jo \ny(t)e -j271'nfot dt \n= foNT y(t) cos(27rnfot) dt - j foNT y(t) sin(27rnfot) dt \nn = 0, I, ... , NI2 \n(13.2) \nwhere fo = liNT. Although Eq. (13.2) is a continuous Fourier transform \nintegral, we are only evaluating the equation for the (NI2) + 1 discrete \nfrequencies evaluated in Eq. (13.1), that is, 0, fo, 2fo, ... , (NI2)fo. (Ob-\nserve that Eq. (13.2) holds also for negative frequencies, but this generali-\nzation is omitted for clarity.) From Eq. (13.2), we will proceed to demon-\nstrate the implied convolution operation of the FFT. To do so, we have \npurposely converted from the discrete (Eq. (13.1) to the continuous domain \n(Eq. (13.2Â». Although it is not necessary to prove our arguments in the \ncontinuous domain, we find that such an approach is generally more easily \nvisualized. \nOur approach to the development of the FFT band-pass integrate and \nsample filter concept is to first consider only the real term of Eq. (13.2) \n(assume y(t) is a real function): \n(NT \nYR(nfo) = Jo \ny(t) cos(27rnfot) dt \nn = 0, I, ... , N/2 \n(13.3) \nWe will show that Eq. (13.3) can be interpreted as a time-sampled output \nSec. 13.1 \nFFT Band-Pass Integrate and Sample Filters \n293 \nfrom a bank of filters with impulse-response functions u(t) cos(27Tnfot), \nwhere u(t) is unity over the interval (O,NT). To facilitate the development, \nwe first consider the low-pass FFT filter of the filter bank. \nLow-Pass Filter Development \nConsider Eq. (13.3) for the case n = 0: \n(NT \nY \nR(O) = Jo \ny(t) dt \n(13.4) \nFor this case, Eq. (13.3) reduces to a simple integration of y(t) over the \ninterval of 0 to NT. We claim that Eq. (13.4), that is, the FFT output for n \n= 0, is the linear-system convolution equation representing a low-pass filter \nfollowed by a sampler. To develop this viewpoint, consider the following \narguments. \nIn Fig. 13.1, we show the procedure for convolving the two waveforms \ny(t) and u(t): \n(13.5) \nIf we let y(t) represent the input waveform to a linear system and let \nu(t) be the impulse response of the system, then Eq. (13.5) determines the \nsystem output ro(t). In Fig. 13.1, we have assumed that the system impulse \nresponse u(t) is a unity amplitude function over the interval 0 to NT, as \nshown in Fig. 13.1(a). The system input y(t) is assumed to be a general \nwaveform, as shown in Fig. 13.1(b). Although Eq. (13.5) describes the linear-\nsystem output for all time t, we show the graphical evaluation of Eq. (13.5) \nfor the single point of time t = t' = NT. \nNow observe that the evaluation of the convolution relationship of Eq. \n(13.5) for the point in time t' requires only the integration of y(t) over the \ninterval 0 to NT, as illustrated in Figs. 13.1(e) and (t). Also note from Fig. \n13. I(e) that multiplication by u(t) required in the convolution procedure of \nEq. (13.5) actually determines the integration interval because u(t) is defined \nto have utility amplitude over the interval 0 to NT and to have zero amplitude \nelsewhere. Hence, the evaluation of the convolution equation for the point \nof time t = t', as shown in Fig. 13.1(t), reduces to integration of y(t) over \nthe interval 0 to NT. But this result is exactly Eq. (13.4), the FFT output \nfor the case n = O. We have then shown that Eq. (13.4) can be interpreted \nas the evaluation of a convolution equation for a single point of time. This \noperation is called integrate and sample filtering. \nBecause Eq. (13.5) describes the output of a linear system with input \ny(t) and impulse response u(t), the real FFT output at n = 0 can be char-\nacterized as the output of a linear system sampled at time t = t' = NT. The \nsystem-response characteristics are defined by the impulse response u(t), \n294 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \nU(T) \nVITI \nNT \nT \n(a) \n(b) \nU('T) \nFOLDING \n-NT \nT \n(e) \nU(t' -\nT) \nDISPLACEMENT ~ \nt' = NT \nT \n(d) \nuh' -T)V(T) \nMULTIPLICATION ~ \nINTEGRATION ~ \n(f) \nt' = NT \nFigure 13.1 Graphical development of the equivalence of the FFT and convo-\nlution for the case n = O. \nwhich is illustrated in Fig. 13.2(a). Magnitude of the Fourier transform of \nthis impulse response is shown in Fig. 13.2(b). This frequency function is \nthat of a low-pass filter with a magnitude frequency-response function of \nSec. 13.1 \nFFT Band-Pass Integrate and Sample Filters \n295 \nNl1sin(7rf/fo)]/(7rf/fo). As a result, the real FFT output for the case n = 0, \nas described by Eq. (13.4), is equivalent to the output of \na low-pass integrate \nand sample filter with a Nl1sin(7rf/fo)]/(7rf/fo) frequency response. Since \nEq. (13.4) evaluates the convolution integral for only one point of time, we \ninterpret the FFT for n = 0 as a low-pass filter followed by a sampler. \nNote that we have shown that the FFT output can be considered as a \nsampled value of a time function. This is in direct contrast to the normal \ninterpretation ofthe FFT as a time-to-frequency-domain transform. We now \nextend these low-pass filter results to the band-pass filter case. \nBand-Pass Filter Development \nLet us now consider Eq. (13.3) for the case n = 1: \nYR(fo) = foNT y(t) cos(27rfot) dt \n(13.6) \nOur objective is to demonstrate that Eq. (13.6) represents the output of a \nband-pass filter centered at frequency f \n0 followed by a sampler. Our ap-\nproach follows that used for the low-pass filter development. \nIn Fig. 13.3, we show the basic waveforms obtained in realizing the \nconvolution equation \n(13.7) \nwhere \nu'(t) = u(t) cos(27rfot) \n(13.8) \nAs in the low-pass filter case, Eq. (13.7) characterizes the output of a linear \nsystem with input y(t) and impulse response u'(t) defined by Eq. (13.8). \nFigure 13.3 graphically evaluates Eq. (13.7) for the single point of time t = \nt' = NT. As shown in Figs. 13.3(e) and (t), this single point of \nthe convolution \nresult is determined by multiplying y(t) by cos(27rfot) and integrating over \nthe interval 0 to NT. This sample value of the convolution result is exactly \nthe FFT output for n = 1 computed from Eq. (13.6). Hence, Eq. (13.6) can \nbe interpreted as the evaluation of the convolution equation (13.7) for the \nultl \nNT \n10 \n210 \n310 \nlal \n(bl \nFigure 13.2 FFT low-pass filter: time- and frequency-domain characteristics. \n296 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \nU'(T) \n-, \n(a) \n(b) \nU'(-T) \nU'(I'-T) \nI'=NT \n(e) \n(d) \nU'(I'-T) V(T) \nâ¢ \n,.(1) \nSHADED AREA \n(e) \n(I) \nFigure 13.3 Graphical development of the equivalence of the real FFT and con-\nvolution for the case n = 1. \nsingle point of time t = t' = NT, and, as a result, the real FFT output for \nn = 1 is that of a sampled output from a filter with the impulse-response \nfunction given by Eq. (13.8). \nIn Fig. 13.4(a), we show the impulse response of Eq. (13.8). Note that \nthis impulse response is simply the multiplication of the low-pass filter im-\npulse response u(t) and the term cos(27rfot). To determine the corresponding \nfrequency-response function, recall from the frequency-shifting theorem \n(Eq. (3.23) and Ex. 3.8) that multiplication of the low-pass impulse response \nu(t) by the cos(27rf \not) term translates the low-pass filter frequency-response \nfunction illustrated in Fig. 13.2(b) into a band-pass frequency-response func-\ntion centered at frequency fo, as shown in Fig. 13.4(b). The band-pass filter \nfrequency characteristic is the Nllsin(7rf/fo)]/(7rf/fo) function of the low-\npass filter shifted in frequency. \nThe FFT real output for the case n = 1 (Eq. (13.6Â» can then be in-\nterpreted as a single sample of the output of a band-pass filter centered at \nfrequency fo with NT{sin[7r(f -\nfo)/fo]}/[7r(f -\nfo)/fo] frequency-response \nSec. 13.1 \nFFT Band-Pass Integrate and Sample Filters \n297 \n-310 \n-210 \n-10 \nfo \n2fo \n3fo \nf \nla) \n(b) \nFigure 13.4 FFT band-pass filter centered at frequency fo = liNT: time- and \nfrequency-domain characteristics. \ncharacteristics (magnitude). As in the low-pass filter case, the output of the \nFFT can be considered as a single value of a time waveform. \nIf we consider Eq. (13.3) for the general case, we note that the previous \narguments apply. For each n, we can draw an illustration analogous to Fig. \n13.3. Hence. the FFT real output YR(nfo,NT) can be interpreted as the \noutput of a convolution integral: \nY \nR(nfo,NT) = foNT y(t) cos(27rnfot) dt \n= f:\"\" Y(T)u~(NT -\nT) dT \nn = 0, I, ... , N/2 \n(13.9) \nwhere the convolution integral must be interpreted as being evaluated only \nat t = NT and where u~(t) is the system impulse response given by \nu~(t) = u(t) cos(27rnfot) \n(13.10) \nThe superscript i in u~(t) indicates that the filter response is in phase and \nis determined by multiplication of the impulse response u(t) by a cosine \nterm. \nFrom the frequency-shifting theorem (Ex. 3.8), multiplication of the \nlow-pass filter impulse response u(t) by the function cos(27rnfot) translates \nthe NT[sin(7rf/fo)]/(7rf/fo) low-pass filterfrequency response to a band-pass \nfilter centered at frequency nfo. As a result, Eq. (13.3) can be interpreted \nas the band-pass filter bank illustrated in Fig. 13.5, where it is \nunderstood that we sample the outputs of the filter bank at time t = NT. \nAll side-lobe characteristics of the NT{sin[7r(f -\nnfo)/fo]}/[7r(f -\nnfo)/fo] \nfilters in Fig. 13.5 have been omitted for clarity. The frequency response of \neach filter is centered at frequency nfo, where n = 0, I, ... , N/2. Note \nthat we can easily extend our arguments to include nfo, where n = NI2 + \nI, ... , N -\nI, to determine the filter response for negative frequencies. \n298 \nFFT Multichannel Band-Pass Filtering \nChap, 13 \nU'(fl \nFigure 13.5 FFT band-pass filter bank frequency-domain characteristics (side lobes deleted). \nQuadrature Band-Pass Filter Bank \nWe have neglected the imaginary term in Eq. (13.2). If we treat this \nterm in the same manner as the real term, then we obtain another band-pass \nfilter bank described by \nfoNT yet) sin(2'iTnfot) dt \nJ:~ \ny(t)uq(NT -\nT) dT \nn = 0, 1, ... , N/2 \n(13.11) \nwhere the convolution integral is evaluated only at t = NT and where uq(t) \nis the system impulse response given by \nuq(t) = u(t) sin(2'iTnfot) \nn = 0, 1, ... , N/2 \n(13.12) \nSuperscript q in uq(t) indicates that the impulse response is in quadrature \nand is determined by multiplication of the impulse response u(t) by a sine \nterm. The negative sign in Eq. (13.11) is absorbed by the folding operation \nin the convolution process (see Prob. 13.2). A comparison of Eqs. (13.12) \nand (13.10) reveals that the two impulse-response functions differ only by \na phase shift, that is, the difference between sin(2'iTnfot) and cos(2'iTnfot). \nHence, we use the defining terms in phase and quadrature. We can then \nrepeat the arguments leading to Fig. 13.3 with the exception that the impulse-\nresponse function is now a sine function instead of a cosine function. Com-\npletion of this development results in another band-pass filter bank that \ndiffers only in phase-response characteristics from those of the previous \ndevelopment. Equation (13.11) can be interpreted as the time-sampled output \nof a filter bank with impulse response that is in quadrature or 90Â° out of \nphase with the filter bank described by Eq. (13.9). \nSummary \nBased on the previous discussions, FFT results can be interpreted as \nthe time-sampled outputs of two band-pass filter banks from which phase \nand amplitude information can be derived. Summarizing, the real and im-\nSec. 13.2 \nFFT Band-Pass Filter Frequency-Response Characteristics \naginary FFT terms from Eqs. (13.9) and (13.11) can be expressed as \nYR(nfo,NT) = foNT y(t) cos(27Tnfot) dt \n= J:,., y(T)u(NT -\nT) cos[27Tnfo(NT -\nT)] dT \nY[(nfo,NT) = -\nfoNT y(t) sin(27Tnfot) dt \n= J:= Y(T)u(NT -\nT) sin[27Tnfo(NT -\nT)] dT \nn = 0, 1, ... , N/2 \nHence, the FFT relationship of Eq. (13.2) can be written as \n(NT \n= Jo \ny(t) cos(27Tnfot) dt -\n(NT \nj Jo \ny(t) sin(27Tnfot) dt \n(NT \n= Jo \ny(t)e -j2-rrnfol dt \n= J:\", y(T)u(NT -\nT)ej 2-rrn fo(NT -\nT) dT \nn = 0, 1, ... , NI2 \nwhere the convolution integral is evaluated only at t = NT. \n(13.13) \n(13.14) \n(13.15) \nThe filter bank described by the real part of Eq. (13.15) is generally \ntermed the in-phase filter bank and the filter bank described by the imaginary \npart of Eq. (13.15) is termed the quadrature filter bank. Sampled real and \nquadrature band-pass filter bank outputs are an alternate way of interpreting \nthe conventional real and imaginary outputs of the FFT. \n13.2 FFT BANDÂ·PASS FILTER FREQUENCYÂ·RESPONSE \nCHARACTERISTICS \nWithin the context of interpreting the FFT as a bank of filters, it is of value \nto reexamine some of \nthe terminology associated with the FFT. In particular, \nwe wish to investigate the frequency-response characteristics of FFT filters \nand reinvestigate FFT resolution and data-weighting functions. \n299 \n300 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \nFFT Filter Bank Frequency-Response Characteristics \nFrom Fig. 13.5, we note that there is considerable overlap of the fre-\nquency-response functions of adjacent filters in the FFT filter bank. As a \nresult, a single sinusoid input to the FFT filter bank can produce an output \nat several adjacent filters. To examine this effect further, consider the FFT \nresults shown in Fig. 13.6. The input sinusoid to the FFT is a cosine wave-\nform of frequency 6.5/32 Hz. For N = 32 and T = 1, the filter frequency \nresponses of the FFT filter bank are centered at integer multiples of the \nfrequency fo = 1/NT = 1/32. Hence, the frequency of the input sinusoidal \nis exactly between the center frequencies of the two band-pass filters cen-\ntered at 6/32 and 7/32 Hz. \nAs shown, the FFT responds to the input sinusoid with maximum out-\nput at the two adjacent filters. The output magnitude is 0.637 of the input. \nAll other filters of the FFT filter bank also respond to the input sinusoid \nbecause of the side-lobe frequency response or side-lobe leakage of \nthe filter \nbank. The output value for each FFT filter in the filter bank is determined \nby the magnitude of the filter main lobe or side lobe at the frequency of the \ninput sinusoid. \nWe know from Sec. 9.2 that we can reduce side-lobe leakage by use \nof data-weighting functions. This concept is reexamined in this section in \nview of interpreting the FFT as a band-pass filter bank. \nFFT Resolution \nIn Chapter 9, we addressed FFT resolution. The interpretation of the \nFFT as a bank of integrate and sample filters further illustrates the concept \nof FFT resolution. Recall from previous developments that the frequency \n1.0 \n.8 \n\" \n-.2 \n-.4 \nâ¢ FILTER BANK INPUT / \\ / \\ / \\ / \nT \nFFT RESULTS \n3 \n32 \n4 \n32 \n/ \n/ \n/ \nV \n/ \n5 \n32 \n\\1/ \nV \n11\\ \ni\\ \n/ \\ \n/ \n\\ \n...,. \n'\\ Y L~ \n//-,~ \n, \n\" \n, \n\"1 \\'-5- ~ \nI \n' \n'---. ~ \n6 \n32 \n7 \n32 \n8 \n32 \n'-../ \n;\\ \n: \n, \n\\ \n\"\"\"l/ \n~. \nI \n9 \n32 \nI \nI \nI \n, \nI \n/ \nI \n..-' \n10 \n32 \nnfo \nFigure 13.6 Graphical development of side-lobe leakage encountered with an \nFFT band-pass filter bank. \nSec. 13.2 \nFFT Band-Pass Filter Frequency-Response Characteristics \n301 \nresponses of the filters in the FFT integrate and sample filter bank are cen-\ntered at integer multiples of the frequency fo, as illustrated in Fig. 13.7(a). \nThus, adjacent frequency responses of filters are separated by 11 NT, the \nresolution of the FFT. Note from Fig. 13.7(a) that crossover points of ad-\njacent frequency responses are also separated by the resolution term 11 NT. \nThese crossover points are at the - 4 dB values on the filter frequency-\nresponse characteristics. This crossover value contrasts to the normally en-\ncountered - 3 dB crossover definition for a filter bank. \nResolution of a filter is defined as the capability of a filter bank to \ndistinguish between frequencies. Sinusoids of frequencies contained within \nthe bandwidth of any filter of the band-pass filter bank can not be distin-\nguished at the output of that filter. Hence, the term resolution is used. When \na rectangular data-weighting function is used, the convention is to define \nthe bandwidth of each filter as f \n0 = 11 NT. \nTo illustrate the filter bank concept of FFT resolution improvement, \nconsider Fig. 13.7. Because NT is the duration of the time function that is \nbeing FFTed, then FFf resolution improvement, that is, a decrease in the \nbandwidth of each FFf filter, can be achieved by increasing the number of \n10 \n210 \n310 \n410 \n(a, \n----- ---\nIJ \n21J \n31J \n41J \n51J \nSIJ \n71J \n(b, \nFigure 13.7 FFT band-pass filter bank for (a) N samples and for (b) 2N samples. \n302 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \ndata points N (for a given T). In Fig. 13.7(a), we show the FFT band-pass \nfilters with resolution io = liNT. Figure 13.7(b) illustrates the improved \nresolution obtained by doubling NT. If \nwe increase the time duration of the \ndata record being FFTed to 2NT, then the FFT resolution becomes io \n= 1I2NT, as illustrated in Fig. 13.7(b). As shown, the filter bandwidths \ndecrease by a factor of two. \nFFT Data-Weighting Function: A Filtering Viewpoint \nAs developed in this chapter, the FFT can be considered as a bank of \nintegrate and sample band-pass filters with poor side-lobe characteristics. \nHowever, from Sec. 9.2, we know that it is possible to improve side-lobe \ncharacteristics by using data-weighting functions. This concept is further \nclarified by noting in Sec. 13.1 that we show the FFT can be characterized \nas the output of a linear system with impulse-response function u'(t). If we \nshape this impulse-response function, we can improve the band-pass filter \ncharacteristics. \nTo modify the impulse-response function of the filter bank, we proceed \nas in Sec. 9.2 and multiply the data by a weighting function. Equation (13.15) \nbecomes \n(NT \nY(nio,NT) = Jo \n[w(t)y(t)]e-j21Tnfot dt \n= L \nNT y(t)[ \nw(t)e - j 21Tnfot] dt \n(13.16) \nn = 0, 1, ... , NI2 \nwhere wet) is the data-weighting function. Note in Eq. (13.16) that multi-\nplication of the data by a weighting or window function is equivalent to \nmultiplication of the impulse-response function u(t)e -j21Tnfot by the weight-\ning function wet). \nAnalogous to the development in Sec. 13.1, we can show that the fre-\nquency-response characteristics of each filter in the FFT filter bank are \ndetermined by the Fourier transform of the weighting function. In Fig. 13 .8, \nwe show the FFT band-pass filter bank obtained by using the Hanning \nweighting function. The band-pass filter bank obtained by using the con-\nventional rectangular weighting function is also shown for comparison. Note \nthat the filters in the Hanning filter bank have a bandwidth greater than those \nof the filter banks obtained with rectangular weighting. However, we accept \nthis loss of \nresolution (increased bandwidth) in order to achieve the improved \nside-lobe performance (see Fig. 9.8(bÂ». As discussed in Sec. 9.2, the utili-\nzation of weighting functions becomes a trade-off between resolution and \nside-lobe characteristics. Note that the conventional definition of FFT res-\nSec. 13.3 \nMultichannel Band-Pass Filtering by Shifted FFTs \n1.0 \n~ 0.8 \n:J \n5 0.6 \n0-\n~ 0.4 \n0.1 \n10 \n210 \n- - -\nRECTANGULAR WEIGHTING \n_ \nHANNING WEIGHTING \n,--..... \n,-- --\n310 \n410 \n510 \n610 \n710 \nFigure 13.8 Comparison of the FFT band-pass filter bank frequency-response \ncharacteristics for rectangular and Hanning weighting functions. \n303 \nolution (f \n0 = 11 NT) implies a different bandwidth when weighting functions \nare used. \nSummary \nThe results of this section do not differ from those of Sec. 9.2. Only \nthe interpretation viewpoint has changed in that we have reexamined some \nof the basic concepts of the FFT in terms of linear filters. If one's formal \neducation includes linear-system theory, this section should lend additional \ninsight to some of the basic concepts of the FFT. \n13.3 MULTICHANNEL BAND-PASS FILTERING \nBY SHIFTED FFTs \nIn Sec. 13.1, we developed the interpretation of the FFT as a bank of in-\ntegrate and sample filters. We now extend that discussion to a series of FFTs \nwhere each sequential FFT is hopped or shifted along the time function being \ntransformed. This sequence of FFT outputs can be interpreted as time sam-\nples from the outputs of a bank of band-pass filters. That is, we can use a \nsequence of FFTs to filter a time signal into time samples of individual \nchannels with bandwidth equal to the resolution of the FFT. We will show \nthat the FFT outputs are complex time samples of the outputs of a bank of \nquadrature filters. \nBecause one normally considers the FFT as a time-to-frequency trans-\nform, the concept of realizing a sequence of FFT outputs as a filtered time \nsequence is often confusing. For this reason, we find it of \nvalue to investigate \nin detail this application of the FFT. In this section, we develop graphically \nand analytically the concept of FFT multichannel band-pass filtering. \n304 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \nGraphical Overview \nWe show in Fig. 13.9 a pictorial of the FFT multichannel band-pass \nfiltering concept. As illustrated in Fig. 13.9(a), multiple FFTs are performed \nsequentially on the time function y(t). Waveform y(t) of Fig. 13.9(b) is as-\nsumed to be a composite waveform, consisting of \na constant-value waveform \nand a sinusoid offrequency 2io. Each of the FFTs shown in Fig. 13.9(a) is \nequivalent to a bank of integrate and sample band-pass filters, as shown in \nFig. 13.9(t). The sequence of FFT outputs for the low-pass filter is shown \nin Fig. 13.9(c). FFT output complex samples for the filter centered at fre-\nFFT \nFILTER \n1 \nBANK \n10 \n210 \n(I) \n(a) \nINPUT \nSIGNAL \n(b) \n3 \n2 \nREAL \n3 \nFFT \nOUTPUT \n2 \n(d) \n_----_~ \nIMAGINARY 3 \nFFT \nOUTPUT \n2 \n(e) \n310 \n410 \nFFTÂ·l \nI â¢ \ni FFTÂ·2 \n'i \n' FFT~ \nII\nI, \n: ! i F~.5 \nII \ni \ni \nt \nFFTÂ·6 \nI \n'i :;: I I ~ .FFT-7 \nI I ! , i \nâ¢ \n0 \n0 \n0 â¢â¢ \nF~i1t_9 \nv(t); 2 + cos{217j210)t) \n-\nT -\nSAMPLE INTERVAL \nY,,(210. t,) \n\\ \nI \\ \nI ' \nI \\ \n\\ \n\\/ \ni \nI \nI \nI I I I I I I I I I I I I I \" \nI \", \ni ,. \nNT \nNT \n3NT \n2NT \n5NT \n, \n'2 \n'2 \n'2 \nY.(210â¢ t,) \nNT \n'2 \nI \nNT \n\\ \n\\ \nI \\ \nI \nI \n, \n\\ \n\\ \n3NT \n2NT \n5NT \n'2 \n'2 \nFigure 13.9 Graphical representation of the output of a sequence of FFTs with multiple input \nsignals. \nSec. 13.3 \nMultichannel Band-Pass Filtering by Shifted FFTs \n305 \nquency 2fo are shown in Figs. 13.9(d) and (e). Each sequence of FFT outputs \nfor a filter is the sampled output-time waveform for that respective filter. \nFigure 13.9(a) also illustrates the overlap of sequential FFTs. As \nshown, the hop or shiff for each FFT is one sample interval T of the input \nsignal. Hence, the sampling rate of the output signal of each filter is liT. In \nlater developments, we increase the FFT shift interval. \nNote from Fig. 13.9(c) that the output waveform of the real FFT low-\npass filter is the constant term of the input signal. Further, the output wave-\nform of the real FFT band-pass filter centered at frequency 2f \n0, as shown \nin Fig. 13.9(d), is the sinusoidal term of the input waveform. The output \nwaveform of the quadrature, or imaginary, FFT band-pass filter centered at \nfrequency 2fo, as shown in Fig. 13.9(e), is identical to the output of the real \nfilter with the exception of a time delay (900 phase shift). Because the hop \nor shift interval of successive FFTs is T, then the sample interval for each \nFFT filter output is T. We now develop a theoretical basis to support the \ngraphical results of Fig. 13.9. \nTheoretical Development \nIn Fig. 13.9, we show a sampled time function y(t) that we wish to \ndigitally band-pass filter using the FFT. The time domain over which each \nsuccessive FFT is to be taken is also shown. If \nwe assume a FFT weighting \nfunction w(t), then we can write the FFT approximation of Eq. (13.16) for \nthe time interval 0 to NT as \n(NT \nY(nfo,NT) = Jo \ny(f)w(f)e -j2-rrnfot df \nn = 0, 1, ... , NI2 \n(13.17) \nRecall that the parameter NT in Y(nfo,NT) is the end point of the time \ninterval over which the first FFT is to be taken. We can write Eq. (13.17) \nequivalently as a convolution integral: \n(13.18) \nwhere f. = NT and we assume that Eq. (13.18) is evaluated only at f = f â¢. \nRecall that the convolution procedure requires that we fold W(f) about the \ny axis and then shift the folded function. A shift of f\\ = NT is required in \nEq. (13.18) to obtain the weighting function with position as illustrated in \nFig. 13.9 for FFT-l. \nNow consider a second FFT over the interval 8 to 8 + NT, as shown \nby the placement for FFT-2 illustrated in Fig. 13.9. For this time interval, \nthe FFT approximation is \n(5+NT \nY(nfo,8 + NT) = J5 \ny(t)W(f -\n8)e-j2-rrnfoU-5) df \n(13.19) \n306 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \nAnalogous to Eq. (13.18), we can write Eq. (13.19) as a convolution integral: \n(13.20) \nwhere the convolution is evaluated only at t2 = 8 + NT. Note that if we \nfold w(-r) according to the rules of convolution, then the shift t2 = 8 + NT \nis required to obtain the placement illustrated in Fig. 13.9 for FFT-2. \nFrom Eqs. (13.18) and (13.20), the FFT approximation for any time \ninterval can be written as \nYCnfo,t;) = J:oo Y(T)W(t; -\nT)eJ2-rrn fo(ti- T ) dT \n(13.21) \nwhere t; is the end point of the interval of duration NT over which the FFT \nis taken, as illustrated in Fig. 13.9. Note that Eq. (13.21) is simply the con-\nvolution of y(t) with the function w(t)eJ2-rrn fo t , where we evaluate the con-\nvolution at times t = t), t2, ... , t;, .... \nAs discussed previously, the term w(t)eJ2-rrn fo t can be interpreted as \nthe impulse response of a linear system that yields an in-phase and quad-\nrature band-pass filter bank. As a result, for each t;, Eq. (13.21) and the \nsequence of FFTs illustrated in Fig. 13.9 can be interpreted as the sampled \noutput (at time t;) of a bank of analog band-pass filters. The interval \nt; -\nt;_) is the sample interval of the filter-bank outputs. \nConsider Eq. (13.21) for the case n = 0: \nY(O,t;) = J:oo Y(T)W(t; -\nT) dT \n(13.22) \nWe observe that Eq. (13.22) is the convolution of the waveform yet) with \nthe impulse-response function w(t), that is, we have filtered yet) with a low-\npass filter whose frequency-response characteristics are given by the Fourier \ntransform of w(t). Further, Eq. (13.22) is valid only for the points in time t \n= t), t 2, t 3, â¢ . . , which implies that we have sampled the output of the \nlow-pass filter. Hence, the FFT output sequence Y(O,t), Y(Oh), Y(0,t3), \n... is actually a set of time-domain samples of the output of a low-pass \nfilter with input y(t). \nSimilarly, the FFT output sequence for the band-pass filter centered \nat frequency fo, that is, Y(fo,td, YCfoh), Y(fo,t3), ... , is the complex \nsampled time waveform of the output from a band-pass filter (center fre-\nquency fo). The real part of Y(fo,t), Y(foh), Y(fO,t3), ... is the sampled \nwaveform out of the in-phase FFT filter bank and the imaginary part of \nY(fo,t), Y(foh), Y(fO,t3), ... \nis the sampled waveform output ofthe quad-\nrature FFT filter bank. The imaginary part of Y(fo,t;) is shifted in phase 90Â° \nwith respect to the real part of Y(fo,t;). Hence, a sequence of FFTs on the \ntime function yet) yields results equivalent to a bank of quadrature digital \nband-pass filters. \nSec. 13.3 \nMultichannel Band-Pass Filtering by Shifted FFTs \n307 \nExample 13.1 FFT Low-Pass Filtering \nTo further illustrate the special case of FFT low-pass filtering, assume w(t) is the \nrectangular weighting function. From Eq. (13.22), \n(13.23) \nWe show an example of FFT low-pass filtering according to Eq. (13.23) in Fig. \n13.10. For the graphical example shown, we have chosen the same input waveform \nassumed in Fig. 13.9: \ny(t) = 2 + cos(27rj't) \nf' = 21NT \n(13.24) \nSample interval T has been set to NTl8. \nFigure 13.IO(a) illustrates the window placement of the first FFT described by \nEq. (13.23): \n(NT \nY(O,Nn = Jo \ny(t) dt \n(13.25) \nSubstitution of Eq. (13.24) into Eq. (13.25) yields a value of 2NT because the cosine \nterm integrates to zero. We graphically obtain the same result from Fig. 13.IO(a) by \ndetermining the area under the waveform over the interval 0 to NT. The output of \nthe low-pass filter (i.e., the FFT) at the time NT is equal to 2NT, as shown in Fig. \n13.IO(d). Effectively. we have filtered the signal y(t) with a rectangle impulse-re-\nsponse low-pass filter and sampled the filtered waveform at time t I = NT, the first \nsample value shown in Fig. J3.IO(d). \nTo obtain another sample of the low-pass filter output waveform, we shift the \nFFT window one sample interval, as shown by the weighting-function location FFT-\nV(I) \nV(I) \nviI) \nI \nI \nFFT-3 --l \n~FFT-2-1 \n,--\nI \nI \nI \n-\nFFT-l----j \n3 \n3 \nI \n3 \nI \n2 \n2 \n5NT \n7NT \n4 \n4 \nNT \nNT \n3NT \n2NT \nI \n\"2 \n\"2 \n(d) \n2 \nNT \n4 \nFigure 13.10 Example of FFT low-pass filtering. \n(e) \n5NT \n4 \n308 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \n2 in Figs. 13.9 and 13.1O(b). For this case, Eq. (13.23) becomes \n1\n9NTIS \nY(0,9NTIS) = \ny(t) dt \nNTIS \n(13.26) \nSubstitution of Eq. (13.24) into Eq. (13.26) yields a value of 2NT because the cosine \nterm integrates to zero. From Fig. 13.1O(b), we obtain the same result by evaluating \nthe area under the waveform over the interval NTIS to 9NTIS. This result is equivalent \nto the output of a low-pass filter (i.e., the FFT output) sampled at time 9NTIS, as \nshown in Fig. 13.1O(d). Figure 13.lO(c) further illustrates the shifting or hopping FFT \nand the interpretation of the FFT as the sampled output of the low-pass filter. Note \nthat the sample interval of the low-pass filter output waveform is NTIS, the FFT \nshift interval. \nRecall that the frequency response of the FFT low-pass filter under the as-\nsumption of a rectangular window is a Nnsin(Trflfo)]/(Trflfo) function, which is zero \nat frequencies fo = liNT, 2INT, 3INT, ... \n(see Fig. 13.2). Hence, we would expect \nthe FFT low-pass filter to completely eliminate an input sinusoid of frequency fo = \n2INT. This is the case illustrated in Fig. 13.10. \nNote that the zero-frequency input signal to the low-pass filter has an amplitude \nof 2 and the output amplitude is 2NT. The amplitude-scaling factor NT should be \ninterpreted as the gain (multiplier) of the FFT filter bank. \nExample 13.2 FFT Band-Pass Filtering of a Sinusoid \nTo graphically illustrate the concept of FFT band-pass filtering of \na sinusoid, assume \ny(t) is given by \ny(t) = 2 + cos(2'ITf't) \nf' = 21NT \n(13.27) \nwhich is the identical waveform considered in Ex. 13.1. Here we evaluate the se-\nquence of FFT equations (13.20 for the case n = 2, that is, the band-pass filter with \ncenter frequency fo = 21NT is evaluated. \nSubstitution of Eq. (13.27) into Eq. (13.20 for n = 2 yields \nY(2fo,t;) = 1-\n00\n00 [2 + cOS(2Trf'T)]W(t; -\nT)e j4'Tffo(I'-T) dT \n(13.2S) \nFor simplicity, we let w(t) equal the rectangular function. Equation (13.2S) becomes \nY(2fo,t;) = 2 (\" \n[e j4'Tffoli]e -j4'TffOT dt \nJ/i-NT \n(13.29) \n(13.30) \n= ej4'Tffol, (Ii \ncos(2'ITf't)[cos(4Trfot) - j sin(4Trfot)] dt \n)t;-NT \nSince fo = liNT, the first integral of Eq. (13.29) is always over an integer number \nof periods and is zero. That is, the constant term in y(t) is removed from the output \nof the filter centered at frequency 2fo. Evaluation of Eq. (13.30) for t; = NT, the \nSec. 13.3 \nMultichannel Band-Pass Filtering by Shifted FFTs \n309 \nfirst of the sequence of FFTs, yields \n(NT \nY(2fo,NT) = eJ4-rrfO(ND Jo \nCOS(41Tfot)[cos(41Tfot) - j sin(41Tfot)] dt \n(NT \n(NT \n= Jo \ncos2(41Tfot) dt - j Jo \nCOS(41Tfot) sin(41Tfot) dt \n= foNT V2 \ndt + foNT V2 COS(81Tfot) dt \n(13.31) \n- j foNT [ V:z sin(O) + V2 sin(81Tfot)] dt \n= NTI2 \nComputation of Eq. (13.31) is graphically illustrated in Fig. 13.1l(a). We show \nthe window placement for the first FFT and the cosine impulse-response function \nof the FFT filter centered at frequency 2f \no. The product of this cosine term and the \n3 \n2 \n., \nvlt) eos[217(2folt) \nla) \nFFT REAL \nOUTPUT \nÂ·NT \n'2 \n3 \n2 \n-, \n-, \n-2 \n-3 \nvlt) \nNT \\ I 3NT\\ ISNT \n4\"4\"4 \nvlt) eos(217(2foXt-NT \n/ 4)] \nIb) \nId) \n3 \n2 \n-, \n-,+ \nI \nI \n:-- FFT-s---l \nI \nI \nIe) \nFigure 13.11 Example of FFf band-pass filtering of a sinusoid. \n310 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \nwaveform y(t) is also shown. Integration of the product term yields the result of Eq. \n(13.31), which is shown in Fig. 13.11(d). The corresponding product involving the \nsine term is not shown because it integrates to zero. The output value NT/2 is the \nsample output of the FFT band-pass filter (center frequency 2fo) at time tl = NT. \nWe evaluate the output of the same FFT band-pass filter at time ti = 9NT/S \n(FFT-2) in a later example (Ex. 13.3). To obtain the FFT output at time ti = 5NT/ \n4, which is FFT-3 in Figs. 13.9 and 13.11(b), we write Eq. (13.30) as \n1\n5NTI4 \nY(2fo,5NT/2) = ej 2-rrfO(5NTI4) \ncos[21r(2fo)t] [cos(4'lTfot) \nNTI4 \n- j sin(4'lTfot)] dt \n(13.32) \n1\n5NTI4 \n+ j \ncos(4'lTfot) sin(4'lTfot) dt \nNTI4 \n-NT/2 \nThe graphical development of Eq. (13.32) is illustrated in Fig. 13. l1(b). As \nshown, the cosine impulse response of the filter centered at frequency 2fo now ranges \nover the interval NT/4 to 5NT/4. The integral of the product of this cosine term and \nthe input waveform yields the - NT/2 result shown in Fig. 13.11(d). Again, we have \nnot shown the quadrature sine term of the FFT filter because the output is zero. \nEvaluation of the FFT filter output at time t \ni = 3NT/2 is graphically illustrated \nin Fig. 13.11(c) and can be analytically determined analogous to Eq. (13.2S). The \nresult is shown in Fig. 13.11(d). If we continue to evaluate Eq. (13.2S) for successive \nvalues of tj, the results depicted in Fig. 13.11(d) result. That is, successive outputs \nof the FFT band-pass filter centered at frequency 2fo are samples of the input \nsinusoidal waveform cos[2'lT(2fo)t]. The sample interval of the filter output is equal \nto the FFT shift interval NT/S. As expected, the constant term of the input signal \nis filtered by the FFT band-pass filter. \nExample 13.3 FFT Multichannel Filtering: Complex Samples \nAssume that y(t) is given as in the previous examples by \ny(t) = 2 + cos(21rf't) \nf' = 2/NT \n(13.33) \nIn Ex. 13.2, we purposely chose the FFT shift interval to ensure that all imaginary \nFFT output samples are zero. In this example, we remove this restriction and address \ncomplex samples. \nFrom Ex. 13.2, we first evaluate Eq. (13.30) for the case n = 2 and ti = NT. \nWe show in Fig. 13. 12(a) the evaluation of the real product term in Eq. (13.30). The \nintegration of this product term yields the FFT real sample output at time ti = NT, \nas shown in Fig. 13.12(c). The imaginary product term of Eq. (13.30) for ti = NT \nis illustrated in Fig. 13.12(b). By inspection, this term integrates to zero and hence \nthe FFT imaginary output sample is zero, as shown in Fig. 13. 12(d). \nSec. 13.3 \n3 \n2 \n. , \n3 \n2 \n-, \nFFT \nREAL \nMultichannel Band-Pass Filtering by Shifted FFTs \nvIti \nvIti eos[2lT(2fo)t] \n3 \n2 \n3 \n2 \n-, \n-2 \nvIti \n-vIti sin[21T(2fo)tj \n/ \n(b) \nFFT IMAGINARY OUTPUT \n311 \nOUTPUT C \nNT \n'2 \n-NT \nNT \n'2 \n'2 \n3 \n2 \n-, \n3 \n2 \n-, \n-2 \n,., IÂ·' \n(e) \nvIti \n\" \n, \n, \nNT' '5NT' '9NT \n'4V '4V '4 \nvIti eos[2lT(2foKt-NT \n14)] \n(e) \nNT \n'2 \n-NT \n'2 \nNT \n'2 \n3 \n2 \nvIti \n, \nI \n-1 \nNT'\" \n'4 \nNT \n'.l3NT'./2NT' \n... \n'5NT t \n2 \n2 \n(d) \nI \nFFT-2 _, \nI \n, I \nv \n5NT \n'4 \nI \nI \n9NT \n'4 \n- v(!) sin[2lT(2foXt-NT \n14)] \n3 \n2 \n-1 \n-2 \n(f) \nFigure 13.12 Example of FFT band-pass filter complex samples. \n312 \nFFT Multichannel Band-Pass Filtering \nTo evaluate Eq. (13.30) at time tj = 9NTI4 and l' = 2INT, we write \n1\n9NT'4 \nY(2fo,9NTI4) = e/2Trfo(9NTI4) \ncos[2'IT(2fo)t] [cos(4'ITfot) \nNTI4 \n- j sin(4'ITfot)] dt \n- j \ncos(4'ITfot) sin(4'ITfot) dt \n1\n9NT'4 \n] \nNTI4 \n[1\n9NT'4 \n= ej9Tr/2 \nVz dt \nNTI4 \n1\n9NTl4 \n+ \nVz cos(8'ITfot) dt \nNTI4 \n- j r9NTI4 cos(4'ITfot) sin(4'ITfot) dtJ \nJNTI4 \n= ej9Tr/2 (NTl2) \n= jNTI2 \nChap. 13 \n(13.34) \nEvaluation of the real term of Eq. (13.34) is graphically illustrated in Fig. 13.12(e). \nAs shown in Fig. 13.12(c), the real product term integrates to zero for t2 = 9NT14. \nGraphical evaluation of the imaginary term is shown in Fig. 13.12(0. The imaginary \nproduct term integrates to NTI2, as shown in Fig. 13. 12(d). \nIf we continue to evaluate the FFTs, Eq. (13.30) for subsequent values of ti, \nwe obtain complex samples represented by the waveforms illustrated in Figs. 13.12(c) \nand (d). Note that the real and quadrature filter output sampled waveforms are iden-\ntical with the exception of a time delay or phase shift. \nSummary \nRecall from Chapter 9 that the FFT halves the amplitude of sinusoids \nbetween the positive and negative frequency outputs. As a result, if we repeat \nExamples 13.2 and 13.3 for the frequency - fo, we obtain identical results. \nHence, if we add both positive and negative frequency results, we obtain a \nsinusoid with maximum amplitude NT, which is the amplitude (unity) of the \ninput sinusoid multiplied by the FFT filter bank gain NT. The sample rate \nof the FFT filter outputs is equal to lI(FFT shift interval). \nWe have shown that each FFT output is equivalent to a sample from \na convolution operation of the input waveform and the impulse response of \nthe respective FFT band-pass filter. The impulse response of each FFT filter \nis complex or in quadrature because the FFT weighting function (and hence \nthe input signal) is mUltiplied by a cosine term to obtain the real output and \nmultiplied by a sine term to obtain the imaginary output. Because the impulse \nSec. 13.4 \nSample Rate Considerations in FFT Multichannel Filtering \n313 \nresponses for each filter in the filter bank only differ by a 90Â° phase shift, \nthen the output waveforms of the in-phase and quadrature filters are in quad-\nrature and only differ by a 90Â° phase shift. \n13.4 SAMPLE RATE CONSIDERATIONS IN FFT \nMULTICHANNEL FILTERING \nFor clarity of presentation, we purposely oversampled the output of each \nFFT band-pass filter in the previous examples. Because we set the FFT shift \nor hop interval equal to the sample interval T of the input waveform, then \neach FFT band-pass filter output waveform is also sampled with interval T. \nHowever, a band-pass waveform can be sampled at a rate that is determined \nby the bandwidth of the waveform and not the maximum frequency com-\nponent of the signal. The procedures used are termed down sampling and \nquadrature sampling. Both sampling techniques are described in detail in \nSecs. 14.1 and 14.2. \nWe show in Sec. 14.1 that a bandpass signal with transmission band-\nwidth B T can be down sampled (with constraints) if the sampling frequency \nis 2: 2B T â¢ However, we show in Sec. 14.2 that further sampling efficiencies \ncan be obtained by representing the band-pass signal in complex or quad-\nrature form before sampling. A band-pass signal that has been translated or \ndown sampled to zero center frequency in quadrature form (i.e., complex) \ncan be sampled at a rate is 2: B T â¢ Both down-sampling and quadrature-\nsampling techniques can be applied to band-pass waveforms if a band-pass \nfilter is used to control aliasing. \nThe aliasing level for each FFT band-pass filter is determined by the \ncharacteristics of the window or weighting function. In most practical ap-\nplications of FFT band-pass filtering, weighting-function selection is a trade-\noff compromise between the time duration of the filter and the desired filter \nperformance in the passband (minimum ripple) and stop band (low side \nlobes). The weighting-function impulse response of a filter with low side \nlobes generally has a duration considerably longer than that of a rectangular \nweighting function with similar bandwidth (see Figs. 9.8(a) and (b)). A longer-\nduration weighting function increases the number of sample values N for \neach FFT and hence determines the practicality of FFT band-pass filtering. \nThe normal procedure is to adopt a weighting function that provides rea-\nsonable band-pass filtering characteristics and then translate each FFT band-\npass filter output to a baseband where a digital recursive filter is applied to \nimprove the filtering characteristics. \nIn Fig. 13. 13, we illustrate the correct procedure for defining the band-\nwidth of a FFT band-pass filter. We show the frequency-domain character-\nistics of an arbitrarily selected weighting function. The level of aliasing is \ndetermined by the filter side lobes. If an aliasing level considered to be \n314 \nIW(t)1 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \nSAMPLING BANDWIDTH \n~I.~-----BT-------Â­\nI \nI \nI \nâ¢ \nACCEPTABLE \nALIASING \nLEVEL \nFigure 13.13 Graphical definition of the sampling bandwidth BT of a band-pass filter. \nacceptable is as illustrated, then the output waveform ofthis band-pass filter \nhas a sampling bandwidth B \nT as shown. Bandwidth B \nT is also termed the \ntransmission bandwidth and is used in applying the sampling techniques \ndeveloped in Secs. 14.1 and 14.2 to the special case of FFT band-pass \nfiltering. \nFrom Sec. 14.1 we know, that a band-pass waveform can be down \nsampled to its low-pass equivalent. If only the FFT real band-pass filter \noutputs are used, then Eq. (14.1) determines the appropriate sample rate. \nWe can improve the sampling efficiency if we apply both down-sampling \nand quadrature-sampling techniques. Because FFT band-pass filtering re-\nsults in complex or quadrature sampled waveforms, we can down sample \nto zero center frequency both the real and imaginary outputs of each FFT \nband-pass filter and recover the original signal even though spectrum overlap \n(aliasing) occurs. The down-sampling frequency n must satisfy f; > B T , \nEq. (14.5). Note that each FFT filter is centered at integer multiples of the \nfrequency fo, where fo = liNT. Hence, from Sec. 14.1, translation to zero \ncenter frequency requires we set an integer multiple of the sampling fre-\nquency f; equal to the center frequency of each band-pass waveform that \nis to be down sampled. In most FFT band-pass filtering applications, the \nlowest frequency FFT filter used has a center frequency nf \n0, where n is \ninteger valued. We then set the sampling frequency n equal to an integer \nmultiple of fo, that is, T' = NT/n. The sample interval T' for each FFT \ncomplex band-pass filter must satisfy the following relationships: \nT' $ IIBT \nT' = NT/n \nT' = pT \nn is integer valued \np is integer valued \n(13.35) \n(13.36) \n(13.37) \nNote that Eq. (13.37) ensures that the output sample interval T' is selected \nas an integer multiple of T, the sample interval of the input waveform to the \nFFT. Sample interval T' is implemented by setting the FFT shift or hop \ninterval to T'. \nSec. 13.5 \nFFT Multichannel Demultiplexing \n315 \nQuadrature sampling requires we reconstruct a real signal from the \ncomplex samples representing the aliased zero center frequency down-sam-\npled spectrum. From Eq. (14.2), we multiply these complex time-domain \nsamples by the exponential e -j27rf't, where f' is the center frequency of the \ndesired baseband signal. The real part of the frequency-shifted complex \nwaveform is the desired signal. Interpolation may be required to increase \nthe sampling rate consistent with the bandwidth of the resulting baseband \nsignal (see Sec. 14.2). \n13.5 FFT MULTICHANNEL DEMULTIPLEXING \nA practical application of the fundamentals developed in this chapter is the \ndigital demultiplexing offrequency-multiplexed signals. We use the example \nmultiplexed signal shown in Fig. 13. 14(a) to outline the principles of FFT \nband-pass filtering for multichannel signals. Each 4-kHz spectrum shown is \nassumed to be the result of single side-band modulation of a voice signal. \nOur objective is to FFT band-pass filter each channel of the multiplexed \nsignal and reconstruct the signal so that the voice is audible. \nWe choose our FFT parameters to center a band-pass filter at 2 kHz. \nTherefore, let \nf \n0 = 11 NT = 2 kHz \n(13.38) \nand the corresponding FFT filter bank for a Hanning weighting function (see \nFig. 9.8(bÂ» is shown in Fig. 13. 14(b). As illustrated, this selection of fo \nresults in a filter whose passband frequency response is broader than re-\nquired to filter the single side-band voice spectrum. However, if we let fo \n= 1 kHz, then the resulting band-pass filter severely attenuates the voice \nspectrum in each channel. Also note that we have redundant FFT band-pass \nfilters. We use only those FFT outputs that are centered on the frequency \nof each channel of the multiplexed signal. The selected filters are shown by \nsolid lines. \nAssume that the multiplexed signal has been filtered with a low-pass \naliasing filter with a cutoff frequency of 20 kHz. The sample interval of the \ninput waveform to the FFT must satisfy the Nyquist sampling criteria: \n(13.39) \nIf we use a radix-2 FFT algorithm, the parameter N is chosen equal \nto an integer power of 2. Equations (13.38) and (13.39) are satisfied for the \nfollowing selected parameters: \nfo = 2 kHz \nN = 32 \n(13.40) \nT = 1/(64 X 103 ) \n316 \nw \nCl \n:::J \nt:: \n...J \n!1. \n~ \nÂ« \nUJ \nCl \n:::J \nt:: \n...J \n!1. \n~ \nÂ« \n-4 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \nINPUT MULTIPLEXED SIGNAL \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n(a) \nFFT FILTER BANK (HANNING WINDOW) \n2 \n~------------BT--------\n4 \n(e) \n6 \n8 \nDOWN-SAMPLED \nVOICE SPECTRUM \n10 \n12 \n14 \n(b) \n11kHz) \n-4 \n-2 \n, \nRECONSTRUCTED \n,. VOICE SPECTRUM \n.tr.\\ \nIe) \n16 \n18 \n16 \n1B \n2 \n(d) \n11kHz) \n2011kHz) \n2011kHz) \nLOW-PASS \nFILTER \nFREQUENCY \nRESPONSE \n4 \nf(kHz) \nFigure 13.14 (a) Example frequency-multiplexed signal spectrum, (b) FFT filter bank char-\nacteristics selected to demultiplex part (a), (c) frequency spectrum of a complex down-sampled \nFFT band-pass filter output, (d) low-pass filter frequency-response characteristics and the re-\nsulting filtered voice spectrum, and (e) the reconstructed voice spectrum. \nAssume that an acceptable aliasing level for each FFT band-pass filter \nis - 40 dB. From Fig. 9.8(b), the second side lobe of the Hanning function \nis down 41 dB. Hence, the sampling bandwidth BT of each band-pass filter \noutput is 12 kHz, as shown in Fig. 13.14(b). Note that the main lobe and \nChap. 13 \nProblems \n317 \nfirst side lobe of the Hanning function passes through the signal in the ad-\njacent channel. We eliminate this crosstalk with additional filtering to be \ndiscussed later. \nThe FFT band-pass filter output sample interval T' must satisfy Eqs. \n(13.35) to (13.37): \nT' ::5 11(12 X 103) \nT' = 32T/n \nT' = pT \nn is integer valued \np is integer valued \n(13.41) \nThe equation set of Eq. (13.41) is satisfied for T' = 4T = 11(16 x 103 ). \nEach successive FFT is then shifted or hopped over four samples of the \ninput waveform. This down sampling, or decimation, translates the output \nof each of the selected FFT band-pass filters to zero center frequency. \nThe frequency spectrum of a complex down-sampled band-pass filter \noutput is shown in Fig. 13.14(c). Note that the desired signal spectrum is \noverlapped and has a baseband bandwidth of 2 kHz. However, the Hanning \nband-pass filter characteristics allows frequency components from adjacent \nchannels into the quadrature baseband spectrum. To remove this crosstalk, \nwe apply a digital low-pass recursive filter to both the real and imaginary \nsample sequences. An assumed low-pass filter characteristic is shown in \nFig. 13.14(d) as well as the filtered overlapped spectrum. \nTo recover the voice waveform so that it is audible, we multiply the \ncomplex samples represented by the low-pass filtered spectrum of Fig. \n13.14(d) by the complex exponential e - j2-rrf' t, where f' = 2 kHz. The real \npart of the complex product is the desired voice waveform with spectrum, \nas shown in Fig. 13.14(e). Because the sampling rate of the down-sampled \nsignal is 16 kHz, then interpolation is not necessary. \nIn practical applications, channel selectivity is a key issue in the ap-\nplication of the FFT to demultiplexing. Weighting functions more sophis-\nticated than the Hanning function are generally required. Further, our sim-\nplistic example is computationally very inefficient in that our choice of a \nbase-2 FFT algorithm computed the output of all filters in the filter bank. \nApplication of the FFT to the general problem of time-division and fre-\nquency-division (TDM and FDM) transmultiplexing is discussed in detail in \nRefs. [2] to [6]. Our analysis is also applicable to single-sideband frequency-\ndivision multiplex (SSB-FDM) modulation and demodulation. \nPROBLEMS \n13.1 Equations (13.6) to (13.8) and Fig. 13.3 develop the FFT band-pass filter ar-\ngument for the case n = 1 and real functions. Repeat this analytical and graph-\nical development for n = 2 and n = 3 and only real terms. \n318 \nFFT Multichannel Band-Pass Filtering \nChap. 13 \n13.2 Develop analytically and graphically the quadrature filter bank for the cases n \n= 0, 1, and 2. Show graphically why the negative sign in Eq. (13.11) does not \nappear in the convolution form of the equation. \n13.3 Develop the interpretation of the FFT as a bank of band-pass integrate and \nsample filters by proceeding along the lines in Sec. 13.1 but using only discrete \narguments. Use the relationship \nN-I \nH(nlNn = L h(kT)e-j2-rrnkIN = h(O)eO + h(T)e-j2-rrnIN + ... + \nk=O \nwhich is equal to the discrete convolution of h(kn with the sequence 1, \ne -j2-rrnIN, â¢â¢â¢â¢ \n13.4 Repeat the graphical development of Fig. 13.6 for the case of an input cosine \nwaveform offrequency 6.75/32 Hz. \n13.5 Repeatthe analytical developments of Eqs. (13.3) through (13.15) and the graph-\nical developments of Figs. 13.1 through 13.5 for the case ofa Hanning weighting \nfunction. \n13.6 Repeat Ex. 13.3 for the waveform: \ny(t) = 1 + COS(27Tfot) \nfo = liNT \n13.7 Let \ny(t) = 1 + COS(27Tfot) + sin[27T(3fo)t] \nfo = liNT \nDevelop the band-pass filtering equations and FFT output sample results for \nti = NT. \n13.8 Let \ny(t) = COS(27TfIf) + sin(2'fTf2t) \nwhere fl = 3fo + fo/4 and f2 = 3fo -\nf o/4. For fo = liNT, use the FFT to \nband-pass filter y(t). \n(a) Discuss and graph the spectrum overlap that results from FFT band-pass \nfiltering and down sampling. \n(b) Discuss and show graphically why complex sampling avoids loss of infor-\nmation due to spectrum overlap. \n(c) Show how to reconstruct the band-pass filter output signal at a center fre-\nquency of f o/2. \n13.9 Consider the multiplexed waveform: \n4 \ny(t) = L COS[27T(fn + fnI4)] + sin[27T(fn -\nfnI4)] \nn=l \nOur objective is to use the FFT to demultiplex y(t). \n(a) Graphically sketch the spectrum of y(t). \nfn = n Hz \n(b) Determine all appropriate parameters for FFT band-pass filtering. Discuss \nyour assumptions. \n(c) Analytically reconstruct each demultiplexed signal at a new center fre-\nquency of \nf 0/2. Explain how the frequency chosen for reconstruction affects \nthe requirement for interpolation. \nChap. 13 \nReferences \nREFERENCES \n1. HARRIS, F. \"The Discrete Fourier Transform Applied to Time Domain Signal \nProcessing.\" IEEE Commun. Mag. (May 1982), Vol. 20, No.3, pp. 13-22. \n2. GREENSPAN, RICHARD L., AND PETER H. ANDERSON. \"Channel Demultiplexing by \nFourier Transform Processing.\" EASCON '74 Proc. (1974), pp. 360-372. \n3. BELLANGER, M. G., AND J. L. DAQUET. \"TDM-FDM Transmultiplexer: Digital \nPolyphase and FFT,\" IEEE Trans. Commun. (September 1974), Vol. Com-22, \nNo.9, pp. 1199-1205. \n4. Special Issue on Transmultiplexers. IEEE Trans. Commun. (July 1982), Vol. \nCom-30, No.7, pp. 1457-1656. \n5. Special Issue on TDM-FDM Conversions. IEEE Trans. Commun. (May 1978), \nVol. Com-26, No.5, pp. 489-741. \n6. SCHEUERMANN, H., AND H. GOCKLER. \"A Comprehensive Survey of Digital \nTransmultiplexing Methods.\" Proc. IEEE (November 1981), Vol. 69, No. 11, \npp. 1419-1450. \n319 \n14 \nFFT SIGNAL PROCESSING \nAND SYSTEM APPLICATIONS \nThe computing features identified in the previous chapters have resulted in \na multitude of signal-processing applications of the FFT. Many commercial \nand military systems utilize the FFT as an integral processing component. \nAs the price and performance of special-purpose FFT hardware continues \nto improve, we can expect further growth in FFT signal-processing and \nsystem applications. Although it is impossible to enumerate every appli-\ncations area, the fundamentals of FFT signal-processing techniques are ap-\nplicable across a broad range of scientific pursuits. \nBecause every application of the FFT is to sampled waveforms, the \nbasics of signal sampling is of considerable importance to FFT users. For \nthis reason, we first present the details of band-pass- and quadrature-sam-\npling procedures. Then, a broad range of FFT signal-processing and system \nconcepts is presented. An extensive introduction to each field of application \nis not possible; however, sufficient detail is presented to establish a basic \nfoundation on which the reader can easily build. \n14.1 SAMPLING BAND-PASS SIGNALS \nThe FFT is often used in digital signal-processing applications of band-pass \nsignals. Efficient sampling of band-pass signals is of paramount importance \nwhen using the FFT. For this reason, we develop the band-pass sampling \ntheorem, a special case of the Nyquist criteria for sampling baseband wave-\nforms, which was developed in Sec. 5.4. \n320 \nSec. 14.1 \nSampling Band-Pass Signals \n321 \nTo illustrate the concept of sampling a band-pass signal, consider the \ntime-domain waveform shown in Fig. 14.1. The waveform shown by the \nsolid line in Fig. 14.1(a) is an amplitude-modulated band-pass signal. The \ndotted line represents the modulation, or information content, of the signal. \nNote that the modulation waveform is sampled at two times per period, but \nthe carrier \nfrequency is sampled only once per period. As shown, the samples \ncompletely characterize the modulation, or information, waveform even \nthough the sample rate results in aliasing of the band-pass signal. The wave-\nform of Fig. 14.1(a) was sampled in synchronism with the peak of the carrier \nwaveform for clarity of presentation. This is not a requirement for band-\npass sampling, as illustrated in Fig. 14.1(b). Here we show the same sample \nhIt) \nâ¢â¢â¢ sampled values \nâ¢ \n(a) \nhIt) \nâ¢â¢â¢ \nsampled values \n, \n(b) \nFigure 14.1 \nExample of \nbandpass signal sampling: (a) synchronous sampling with \nthe peak of the carrier waveform, and (b) the general case. \n322 \nFFT Signal-Processing and System Applications \nChap. 14 \nrate as before but with a slight time delay. The dashed-line waveform rep-\nresented by the samples is the modulation signal. \nBand-pass waveforms are assumed to have a nonzero spectrum only \nover the frequency interval fl < If I \n< fh' where fh and fl are the highest \nand lowest frequencies that bracket the band-pass signal spectrum, respec-\ntively. The transmission bandwidth of a band-pass signal is defined as B \nT \n= fh - h Using Nyquist criteria, one would sample the band-pass signal \nat a rate of 2f \nh samples per second to ensure that overlap aliasing does not \noccur during sampling. However, recall from Sec. 5.3 that the sampling \nprocess produces spectrum images (aliasing) spaced at harmonics of the \nsampling frequency. We show that aliasing can be used advantageously when \nsampling band-pass signals and that a sampling rate less than 2f \nh can be \ndetermined (BT Â« fl) if we associate the band-pass signal with one of the \naliasing images. The band-pass-sampling theorem states that a band-pass \nsignal can be reproduced from sample values if the sampling frequency f \ns \nsatisfies the relationship \n(14.1) \nand n is integer valued. The condition of Eq. (14.1) ensures that spectrum \noverlap does not occur and only yields acceptable sampling frequencies for \nf s < 2f \nh. Note that if we let n' equal the largest integer that does not exceed \nfhl(fh -\nfl), then the critical (lowest) sampling frequency for a band-pass \nsignal is given by Eq. (14.1) as n = 2fhln'. Also observe that if we choose \nn = fhl(fh -\nfl), then Eq. (14.1) requires fs ~ \n2(fh -\nfl) = 2B T â¢ \nWe illustrate the concept of efficient sampling of band-pass signals in \nFig. 14.2 by means of the convolution theorem. A band-pass time-domain \nwaveform and the corresponding band-pass-frequency spectrum are shown \nin Figs. 14.2(a) and (c), respectively. Note from Fig. 14.2(c) that the center \nfrequency of the band-pass spectrum is 8fo and the transmission bandwidth \nB T is 2f \no. Choose f s = 6f \n0, which satisfies the constraints of the band-pass \nsampling theorem ofEq. (14.1) for n = 3. The time-domain sampling function \nis shown in Fig. 14.2(b) and the corresponding frequency-domain sampling \nfunction is shown in Fig. 14.2(d). \nMultiplication of the band-pass time-domain waveform of Fig. 14.2(a) \nand the sampling function of Fig. 14.2(b) results in the sampled waveform \nillustrated in Fig. 14.2(e). Recall from the convolution theorem that multi-\nplication in the time domain implies convolution in the frequency domain. \nHence, the Fourier transform of the time-sampling function of Fig. 14.2(d) \nis convolved with the band-pass signal spectrum shown in Fig. 14.2(c). The \nresult is the aliased frequency function illustrated in Fig. 14.2(t). \nNote from Fig. 14.2(t) that the sampled frequency function centered \nat frequency Â± 2fo is identical to the original band-pass frequency function \ncentered at frequency Â± 8f \no. Although the function centered at Â± 2f \n0 results \nfrom aliasing, we have not lost information due to spectrum overlap. The \nSec. 14.1 \n1\\ \n\" \n1\\ \nI \n\\ \nSampling Band-Pass Signals \nhIt) \n... \n-810 -610 -410 -210 \n210 410 610 810 lOto \n(e) \nh(I)O(I) \n~ H(I)'6(1) \n... \n1\\ \n\" \n\" ... \n1\\ \n0(1) \n... \nt \n6(1) \nfl = \n6fo \nâ¢â¢â¢ \n-lOto -810 -610 -410 -210 \n210 410 610 810 \nI \n(d) \nFigure 14.2 Nonoverlapped aliased Fourier transform of a band-pass waveform \nthat is sampled at less than twice the highest frequency component. \n323 \nsampled frequency functions centered at Â±4fo \nand Â± 10fo in Fig. 14.2(0 are \nalso the results of aliasing. These terms can be ignored because it can be \nshown that a low-pass fIlter with bandwidth 3fo reconstructs the original \nsignal h(t) with only a shift of the center frequency from 8fo to 2fo. \nThe highest frequency component of the band-pass waveform of Fig. \n14.2(a) is 9fo. Hence, application ofthe baseband Nyquist sampling theorem \nrequires a sampling frequency of 18fo. Because we sampled at a rate of only \n6fo with no loss of information, the waveform is said to have been down \nsampled or decimated. We can down sample with no spectrum overlap as \n324 \nFFT Signal-Processing and System Applications \nChap. 14 \nlong as the sampling frequency fs satisfies the band-pass sampling theorem \nof Eq. (14.1). \nGraphical Development of the Band-Pass Sampling \nTheorem \nWe show in Fig. 14.3 a graphical development of the band-pass sam-\npling theorem. In Fig. 14.3(a), the frequency function of an example band-\npass signal is illustrated. Assume that the signal has center frequency 14fo \nand bandwidth BT < 2fo (i.e., the signal amplitude for frequencies fh and \nfl equals zero). The graphical frequency-convolution procedure is used in \nFigs. 14.3(b) through (I) to illustrate the effect of \nsampling a band-pass signal. \nWe only show the frequency-sampling impulse functions and the convolved \n(aliased) frequency-domain functions. \nBecause B T < 2f \n0, then a natural choice for the sampling frequency \nis fs = 2BT = 4fo, as shown in Fig. 14.3(b). However, we note that this \nchoice of fs produces spectrum overlap. Logically, one increases fs to elimi-\nnate spectrum overlap. In Fig. 14.3(c), we set fs = 4.25fo. Note that there \nis still some spectrum overlap, but if we increase fs to 4.33fo, as shown in \nFig. 14.3(d), we achieve a nonoverlapped sampled frequency spectrum. But \nif \nwe set fs = 4.5fo, as illustrated in Fig. 14.3(e), spectrum overlap is again \nencountered. Using the graphical convolution theorem, we can tediously \ndetermine the range of fs that will produce a nonoverlapped sampled spec-\ntrum. This is the result given by Eq. (14.1). \nConsider Eq. (14.1) for the example band-pass spectrum illustrated in \nFig. 14.3(a). We note that 2:5 n :5 7 because fh = 15fo and h = 13fo. Let \nn = 7. Then, from Eq. (14.1), we obtain 4.29fo :5 is :5 4.33fo. Observe \nfrom Figs. 14.3(c) and (e) that we obtained some spectrum overlap for sam-\npling frequencies 4.25fo and 4.5fo. By a careful graphical analysis, we can \nobtain the range of acceptable sampling frequencies given by Eq. (14.1). The \nsampling frequency fs = 4.33fo illustrated in Fig. 14.3(d) lies at one end of \nthis range and, as shown, spectrum overlap does not occur. \nNow let n = 6 in Eq. (14.1); we obtain a range of acceptable sampling \nfrequencies given by 5fo:5 fs:5 5.2fo. A graphical illustration of the range \nof these sampling frequencies is illustrated in Figs. 14.3(1) through (g). For \nfs = 4.5fo, as shown in Fig. 14.3(e), we obtain an overlapped spectrum; \nbut for fs = 5fo, as shown in Fig. 14.3(f), we note that the sampled spectrum \nis not overlapped. Unacceptable results are obtained for fs = 5.5fo, as \nshown in Fig. 14.3(g). As discussed previously, we can carefully adjust fs \nto graphically obtain the identical range given by Eq. (14.1) for n = 6. \nFrom the results illustrated in Figs. 14.3(h) and (i), we conclude that \n6fo :5 fs :5 6.5fo is an acceptable range for fs. Equation (14.1) yields this \nresult for the choice n = 5. For n = 4, 3, and 2 in Eq. (14.1), we obtain the \nresults 7.5fo :5 fs :5 8.67fo, IOfo :5 fs :5 13fo, and 15fo :5 fs :5 26fo, \nSec. 14.1 \nSampling Band-Pass Signals \nH(I) \n, \n1\\ \n!\\ \nI \nI \n-1510 -1010 \n-510 \n(a) \n510 \n1010 \n1510 \nI. = 4.2510 \nÂ· \n.. t t t \nt t tÂ·Â·Â· \n-1510 \n-1010 \n-510 \n(e) \n510 \n1010 \n1510 \n-1510 \n-1010 \n-510 \n(e) \n510 \nIOta \n1510 \n-1510 \n-1010 \n-510 \n(9) \n510 \n1010 \n1510 \n-1510 \n-lOto \n-510 \n(k) \n510 \nIOta \n1510 \nI \n-1510 \n-1010 \n-510 \n(b) \n-1010 \n-510 \n(d) \n-1510 \n-1010 \n-510 \n(I) \n... \n~ \n~ \n1\\ \n1\\ \n1 \\ \n..â¢ : 1 \n-510 \n(h) \n-1510 \n-1010 \n-510 \n(I) \n325 \nI. = 410 \n510 \n1010 \n1510 I \n510 \n1010 \n1510 I \n510 \n1010 \n1510 \nI \n510 \n1010 \n1510 I \nI. = \n810 \n1.= 1510 , \nTÂ· .â¢ \n510 \n'1010 \n1510 \nFigure 14.3 Aliased Fourier transform of a band-pass waveform that is sampled \nat various frequencies. \nrespectively. Figures 14.3(j) to (I) show acceptable choices of fs within each \nof these ranges. As before, we can refine our graphical analysis to produce \nthe ranges defined by Eq. (14.1). \nNote that the low-pass spectrum results of Figs. 14.3(t), (j), and (I) are \n326 \nFFT Signal-Processing and System Applications \nChap. 14 \nspectrum inverted with respect to Fig. 14.3(a). These results correspond to \nthe frequency ranges determined from Eq. (14.1) for n even. If \nn is chosen \nodd, then the sampled spectrum results are not inverted, as illustrated in \nFigs. 14.3(d), (h), (i), and (k). Also observe that if \nfs is an acceptable sampling \nfrequency, then pfs \n, where p is integer valued, is also an acceptable sampling \nfrequency. For example, if fs = 4.33fo, then fs = 8.66fo and fs = 13fo \nare also acceptable sampling frequencies, as shown in Figs. 14.3(d), G), and \n(k). This follows from the periodicity of the sequence of sampling impulse-\nfrequency functions. We conclude that the determination of sampling-fre-\nquency intervals that do not produce overlapped spectrum results is non-\ntrivial. Both Eq. (14.1) and a graphical analysis are helpful. \nAn alternate way of examining the band-pass sampling theorem is to \nnote that for each case illustrated in Fig. 14.3, the band-pass-frequency func-\ntion with central frequency 14fo is shifted or translated by the down-sam-\npling, or decimation, process. This interpretation of band-pass sampling is \nexplored further in the following example. \nExample 14.1 Down Sampling: A Special Case of Frequency Down Conversion \nRecall from Ex. 3.8 that frequency shifting or down conversion occurs when a time \nfunction h(t) is multiplied by a sinusoidal waveform offrequency fo. From the Four-\nier transform frequency-shifting property, the result of sinusoidal multiplication is \nto shift H(f), the Fourier transform of h(t), such that the original spectrum is now \ncentered at fo Â± fe, where fe is the center frequency of the spectrum H(f). As \nshown in the development of Ex. 3.8, spectrum shifting occurs because time-domain \nmultiplication requires frequency-domain convolution. H(f) is convolved with a pair \nof impulse functions located at Â±fo which is the Fourier transform of a sinusoidal \nwaveform. Down sampling can be interpreted as a special case of frequency down \nconversion. \nThe pair of impulse functions located at frequency Â±6fo in Fig. 14.2(d) can \nbe interpreted as the Fourier transform of a cosine waveform. Hence, from the \nfrequency-shifting theorem, the band-pass spectrum centered at + \n8fo in Fig. 14.2(c) \nis shifted and centered at frequencies (8 -\n6)fo and (8 + 6)fo. Similarly, the band-\npass spectrum centered at -8fo is shifted and centered at frequencies (-8 -\n6)fo \nand (-8 + 6)fo. The result is then the spectrum centered at Â±2fo, as shown in Fig. \n14.2(0, and Â± 14fo. \nSimilar to the arguments above, the pair of impulse functions located at fre-\nquency Â± 12fo, which is not shown in Fig. 14.2(d), result in spectrum being shifted \nto frequencies of Â± (12 - 8)f \n0 and Â± (12 + 8)f \no. The spectrum pair located at Â± 4f \n0 \nare shown in Fig. 14.2(d). Note that this pair is spectrum inverted in that the positive \nfrequency band-pass spectrum is centered at -4fo and the negative frequency band-\npass spectrum is centered at +4fo. \nSummary \nBecause down sampling results in frequency translation of the band-\npass signal, it is possible to position the translated spectrum by an appro-\nSec. 14.2 \nQuadrature Sampling \n327 \npriate choice of sampling frequency fs. Note from Fig. 14.3 that the selection \nof fs satisfying Eq. (14.1) such that nfs = if, where n is integer valued, \ntranslates the band-pass signal spectrum interval, 13fo < f < 15fo, to the \nfrequency interval, 0 < f < 2fo, which is generally referred to as the low-\npass signal equivalent of the band-pass signal. Sampling frequencies fs = \n4.33fo for n = 3, fs = 6.5fo for n = 2, and fs = 13fo for n = 1 satisfy this \ncondition and the graphical results are shown in Figs. 14.3(d), (i), and (k), \nrespectively. For many signal-processing applications, down sampling to the \nlow-pass signal equivalent is the preferred approach. Also note that selection \nof the sampling frequency such that nfs = fh also translates the band-pass \nsignal to a low-pass equivalent, as shown in Figs. 14.3(t) and (i), but the \nspectrum is inverted. \nIt is also possible to select a sampling frequency that results in a fre-\nquency translation to zero center frequency. Note from the graphical de-\nvelopment in Fig. 14.3 that if nfs = (fh -\nfl)/2, that is, the center frequency \nof the band-pass spectrum, then translation to zero center frequency results. \nThis selection of fs always produces spectrum overlap and does not satisfy \nEq. (14.1). In most cases, spectrum overlap is an irreversible operation (see \nProb. 14.3). However, waveforms that are down sampled to zero center \nfrequency are always recoverable if \nthe band-pass signal is quadrature sam-\npled, as is discussed in the following section. \n14.2 QUADRATURE SAMPLING \nApplications ofthe FFT are sometimes limited by the sampling rates achiev-\nable by analog-to-digital converters. For these cases, it is possible to achieve \na lower sampling rate by separating the signal into two waveforms, or chan-\nnels, and sampling each channel. This concept is based on the principle that \na signal can be expressed in terms of two waveforms called quadrature \nfunctions. Each of the two quadrature functions occupies only one-half the \nbandwidth of the original signal. Hence, it is possible to sample each quad-\nrature function at one-half the sample rate required to sample the original \nsignal. We now develop the concept of quadrature functions and quadrature \nsampling. \nQuadrature Functions \nTo demonstrate the concept of quadrature functions, consider Fig. \n14.4. We show in Fig. 14.4(a) an example waveform that is assumed to be \nband-limited with bandwidth fh' as illustrated in Fig. 14.4(c). To derive the \nquadrature functions for this waveform, it is necessary to multiply Fig. \n14.4(a) by both cosine and sine waveforms. \nWe show the required cosine waveform Yi(t) in Fig. 14.4(b). We use \n328 \nFFT Signal-Processing and System Applications \nChap. 14 \nh(l) \nYi(l) = \ncos(2mol) \nâ¢â¢â¢ \nMULTIPLICATION /' \n(e) \nIH(f). VifllLP, \n(f) \nREAL \nH(f) \nREAL \nV,(f) \nA \nV, \nfo=1 \nCONVOLUTION \nÂ·fo \nÂ·fo \nfo \nFigure 14.4 Fourier transform of the in-phase waveform used in quadrature \nsampling. \nthe subscript i to indicate that the cosine waveform has been chosen as the \nreference or in-phase sinusoid. Note that the frequency of this sinusoid is \nf 0 = f h12, the center frequency of the positive frequency range of the band-\nlimited signal, as shown in Fig. 14.4(c). Multiplication in the time domain \nSec. 14.2 \nQuadrature Sampling \n329 \nrequires convolution in the frequency domain and the overlapped spectrum \nof Fig. 14.4(0 results. For ease of discussion, we have constructed h(t) as \nan even function and hence H(f) is real. Therefore, the convolution result \nshown is a real frequency function. Note that we have low-pass filtered the \nconvolution result and have eliminated the convolution terms that are cen-\ntered at frequencies Â± 2f \no. \nThe modulation process just described shifts or translates to zero center \nfrequency the spectrum shown in Fig. 14.4(c). An inspection of Fig. 14.4(0 \nalso reveals that the translated waveform has a bandwidth of fhl2. However, \nspectrum foldover has occurred. As a result, if one samples the waveform \nof Fig. 14.4(e) with sample frequency fh satisfying the Nyquist criteria, the \noriginal time waveform cannot be recovered because of \nthe folded spectrum. \nTo recover the original waveform, it is necessary to define the second of \nthe two quadrature functions. \nIn Fig. 14.5, we repeat the development of Fig. 14.4 with the exception \nthat we multiply by the sine waveform shown in Fig. 14.5(b). If \nwe low-pass \nfilter the product of Figs. 14.5(a) and (b), the resulting waveform of Fig. \n14.5(e) is termed the quadrature function in that it is obtained by mUltipli-\ncation with a sine waveform that is 90Â° out of phase or in quadrature with \nthe cosine waveform of Fig. 14.4(b). \nBy applying the frequency-convolution theorem, we obtain the fre-\nquency function illustrated in Fig. 14.5(0. Note that the Fourier transform \nof the sine waveform is purely imaginary, as illustrated in Fig. 14.5(d). \nHence, convolution with the real frequency function of Fig. 14.5(c) yields \nthe imaginary frequency function illustrated in Fig. 14.5(0. Recall that con-\nvolution requires that one of the functions be flipped prior to shifting and \nmultiplication. The resulting frequency function has a bandwidth f hl2 with \nan overlapped spectrum. Low-pass filtering has eliminated the convolution \nterms that are centered at frequencies Â±2fo. \nThe waveforms of Figs. 14.4(e) and 14.5(e) are termed the in-phase \nand quadrature functions, respectively, because one is obtained by multi-\nplication (translation) with a cosine function and the other is obtained by \nmUltiplication (translation) with a sine function (that is 90Â° out of phase, or \nin quadrature, with the cosine function). The advantage of quadrature-func-\ntion representation can be seen by comparing the frequency functions of \nFigs. 14.4(0 and 14.5(0. Both the in-phase and quadrature waveforms have \na bandwidth of fhl2. Hence, each can be sampled according to the Nyquist \nsampling criteria at a sample rate of fh samples per second. We show in a \nlater development that these sampled results can be appropriately combined \nto eliminate the spectrum overlap that occurs. \nNote that the total number of samples per unit of time that result from \nsampling the quadrature functions is exactly the same as obtained by sam-\npling a function with bandwidth fh. However, with quadrature functions, \nwe have separated the original waveforms into two channels, an in-phase \n330 \nFFT Signal-Processing and System Applications \nChap. 14 \nhIt) \ny.(t) = sin(2mot) \nMULTIPLICATION 7' \nw \n~ \nREAL \nH(f) \n(e) \n(e) \nI \nMAG \nIH(f) â¢ Y.(fllLPF \nA \n2\" \n(f) \n/\" CONVOLUTION \n\"\"'--\nIMAG \ny, \n-fo \n~Y2 \n(d) \nYq(f) \nto \nFigure 14.5 \nFourier transform of the quadrature waveform used in quadrature \nsampling. \nSec. 14.2 \nQuadrature Sampling \n331 \nand a quadrature channel, where the analog-to-digital converter for each \nchannel can operate at one-half the speed required in a single-channel ap-\nproach. If \nanalog-to-digital-converter or digital-processor speed is a limiting \nfunction, a factor of two could be of primary concern. \nRecombination of Quadrature Functions \nA careful recombination of the in-phase and quadrature sampled func-\ntions is necessary to obtain the original waveform and eliminate spectrum \noverlap. In Fig. 14.6, we show a diagram of the quadrature-processing tech-\nnique described previously as well as a procedure for recombining quad-\nrature functions to produce the original real band-limited signal h(t). Note \nthat we recover the original signal by mUltiplying the in-phase channel by a \ncosine waveform with frequency fo, the center frequency of the original \nband-limited signal. The quadrature channel is mUltiplied by a sine waveform \nwith frequency fo. These results are then added and multiplied by a scale \nfactor of \ntwo to recover the original signal. The interpolation function shown \nin Fig. 14.6 is discussed later. \nFigures 14.7 and 14.8 illustrate the rationale underlying the recovery \ntechnique diagrammed in Fig. 14.6. In both illustrations, we use the graphical \nfrequency-convolution procedure, but we only show the frequency-domain \nfunctions. Figures 14.7(b) and (c) are the Fourier transforms of the in-phase \nquadrature function and the cosine waveform, respectively, which are mul-\ntiplied to recover the original waveform. Convolution yields the frequency \nfunction illustrated in Fig. 14.7(a). \nFigure 14.8 depicts the frequency-domain results of mUltiplying the \nquadrature channel waveform determined in Fig. 14.5(e) by a sine waveform \nof frequency f o. The Fourier transforms of the quadrature function and the \nsine waveform are illustrated in Fig. 14.8(b) and (c), respectively. Because \nQUADRATURE SAMPLING \nSIGNAL RECONSTRUCTION \nFigure 14.6 Block diagram of the quadrature-sampling and signal-recombination \nprocesses. \n332 \nFFT Signal-Processing and System Applications \nChap. 14 \nREAL \n[H(I) â¢ Y,(IIlLP' â¢ Y.(f) \n(a) \nREAL \nYi(l) \nREAL \nA \n[H(t). \nYi(IIlLP, \ny, \n'2 \n.10 \n10 \n(b) \n(e) \nFigure 14.7 Frequency function resulting from the cosine modulation of the in-\nphase channel waveform. \nREAL \n[H(t) â¢ Yq(IIlLP' â¢ [Yq(11l \nIMAG \n[H(I) â¢ Y \nq(IIlLPF \nIMAG \nYq(l) \nv, \nto \n.10 \n-Y2 \n(b) \n(e) \nFigure 14.8 Frequency function resulting from the sine modulation of the quad-\nrature channel waveform. \nSec. 14.2 \nQuadrature Sampling \n333 \nboth frequency functions are imaginary, then their convolution is a real \nfunction, as shown in Fig. 14.8(a). To produce the results of Fig. 14.8(a), \nflip one of the functions prior to convolution and take into account the P = \n-1 term. \nNow consider Figs. 14.7(a) and 14.8(a); both frequency functions are \nreal. Addition of the two functions gives the original signal-frequency func-\ntion of Fig. 14.4(c) (except for a scale factor of two). Because we assumed \nthat the band-limited signal spectrum H(f) was real, then no complex terms \noccurred in the example signal-recovery process. The general procedure for \nsignal reconstruction is to multiply the complex sampled signal (i.e., the in-\nphase and quadrature samples, a + jb) by the sampled complex exponential \ne -j2Trfot â¢ The desired waveform is then the real part of \nthis complex product: \nReal {(a + jb)[cos(21Tfot) - j sin(21Tfot)]} \n= Real {[a COS(21Tfot) + b sin(21Tfot)] \n+ j[ -a sin(21Tfot) + b COS(21Tfot)]} \n= a COS(21Tf \not) + b sin(21Tf \no)t \n(14.2) \nThe signal-reconstruction process requires that we translate (frequency \nshift) the quadrature functions up in frequency. As a result, the required \nsampling rate must be increased and interpolation is required because we \nhave sampled at the lower rate. As discussed previously in the development \nof the Nyquist sampling theorem, [sin(t)]/t interpolation yields exact results. \nWe interpolate both the in-phase and quadrature waveforms prior to mul-\ntiplication by the complex exponential. For the example shown, we need \ninterpolate only one sample between each output sample of the analog-to-\ndigital converter. The bandwidth (highest frequency) of the signal after mul-\ntiplication by the complex exponential (i.e., after translation) determines the \nnumber of interpolated samples that are required. \nExample 14.2 Quadrature Sampling of Band-Pass Signals \nWe show in Fig. 14.9 an example band-pass waveform with transmission bandwidth \nBT = 10 and center frequency 510' To derive the quadrature waveforms for the band-\n1 5 \n1.0 \n0.5 \n-0.5 \n-1.0 \n-1.5 \nhIt) = cos[21T(5fo + 1012)\\1 -\n0.5 cos[21T(510 + 1012)11 \n(Q) \n-510 \n-310 \ny, \n';\" \n-10 \n-';\" \nH(I) \n10 \n310 \n510 \nFigure 14.9 Time- and frequency-domain representations of the band-pass signal \nfor Ex. 14.2. \n334 \nFFT Signal-Processing and System Applications \nChap. 14 \npass signal, we multiply by cosine and sine waveforms with frequency 5fo. The in-\nphase component after low-pass fIltering is given by \nh(t)y;(t) = {COS[21T(5fo + f o/2)t] -\n'12 COS[21T(5fo -\nf o/2)t]} cos[2'lT(5fo)t] \n= \n'12 cos[21T(fo/2)t] -\n'14 cos[21T(fo/2)t] \n(14.3) \nThe quadrature component after low-pass fIltering is given by \nh(t)Yq(t) = {COS[21T(5fo + fo/2)t] -\n'12 COS[21T(5fo -\nf o/2)t]} sin[21T(5fo)t] \n= -\n'12 sin[21T(f \n0/2)t] -\n'14 sin[21T(f \n0/2)t] \n(14.4) \nIn Figs. 14.10 and 14.11, we use the graphical frequency-convolution theorem \nto develop the frequency functions corresponding to Eqs. (14.3) and (14.4). Figures \n14.IO(b) and (c) are the Fourier transforms of the band-pass signal h(t) and the cosine \nwaveform with frequency 5fo, respectively. Convolution and low-pass fIltering yield \nthe frequency function illustrated in Fig. 14. lO(a) , the Fourier transform ofEq. (14.3). \nNote that the frequency function has a bandwidth f o/2 and spectrum foldover has \noccurred. \nIn Fig. 14.11, we develop the frequency function corresponding to Eq. (14.4). \nConvolution of the frequency functions of Figs. 14.11(b) and (c) yields the quadrature \nfrequency function illustrated in Fig. 14.11(a). This frequency function has bandwidth \nf o/2 with an overlapped spectrum. Note that both Eqs. (14.3) and (14.4) could be \nREAL \nH(I) â¢ V,(I) \n(a) \n/ \nCONVOLUTION ~ \nREAL \nH(I) \nREAL \nvm \ny, \n'/\" \n-Slo \n-310 \n-10 \n10 \n210 310 410 Sio 610 \n-510 \n51\n0 \n(b) \n(e) \nFigure 14.10 Fourier transform of the quadrature waveform for Ex. 14.2. \nSec. 14.2 \nÂ·5fo \nÂ·3fo \nQuadrature Sampling \nREAL \ny, \nÂ·fo \n-1A \nIMAG \nHlf) â¢ Y.lf) \nÂ·fo \n\"2 \nla) \nHlf) \nCONVOLUTION \nIMAG \nfo \n2fo 3fo 4fo 5fo 6fo \n-5fo \n-v.. \n~ \n~ \nFigure 14.11 \nFourier transform of the in-phase waveform for Ex. 14.2. \n335 \n5fo \nreduced to a single term through addition. This is the mathematical evidence of the \nproblem of spectrum overlap that has been graphically addressed. \nBoth the in-phase spectrum of Fig. l4.l0(a) and the quadrature spectrum of \nFig. 14.11(a) have a bandwidth of f o/2. Hence, each can be sampled at a Nyquist \nrate of fa samples per second as compared to sampling the original band-pass wave-\nform with the Nyquist sampling rate of 2BT = 2fo. To reconstruct a real waveform, \nwe multiply the in-phase and quadrature time-domain samples by the sampled com-\nplex exponential e - j2-rrJ' t, where f' is the desired center frequency of the recon-\nstructed waveforms. Assume f' = fa. Multiplication by the complex exponential \ne - j2-rrfot translates each quadrature spectrum to a center frequency of f o. The highest \nfrequency of the translated waveform is then 3fo/2 and both the in-phase and quad-\nrature functions of Eqs. (14.3) and (14.4) must be interpolated before multiplication \nto obtain a sampling rate three times the original sample rate. \nFigure 14.12(a) illustrates the frequency-domain results o:'tained by multiply-\ning the in-phase waveform of Eq. (14.3) by rj(t) = cos(27rfot). Multiplication of the \nquadrature function of Eq. (14.4) by rq(t) = sin(27rfot) results in the frequency \nfunction shown in Fig. 14.12(b). Addition of the two frequency functions cancels \nthe unwanted overlapped frequency components. The result H'(f) in Fig. 14.12(c) \nis the original signal-frequency spectrum except the spectrum is now centered at \nfrequency fa and must be multiplied by a scale factor of two. \n336 \nFFT Signal-Processing and System Applications \nChap. 14 \nHlf) â¢ Y,(f) â¢ R,(f) \nHlf) â¢ Y.lf) â¢ R.lf) \nYa \n-2fo -fo \nfo 2fo \n-2fo -fo \nfo \n2fo \nla) \nIb) \nH'lf) \nYa \n-fo \nfo \n-3fo \n3fo \n\"2 \n\"2 \n-Va \n(e) \nFigure 14.12 Example frequency function for Ex. 14.2: (a) cosine modulation of \nthe in-phase channel, (b) sine modulation of the quadrature channel, and (c) sum-\nmation of parts (a) and (b). \nSummary \nAs shown, quadrature sampling can be applied to baseband and band-\npass signals. If \na baseband or band-pass signal with bandwidth BT is trans-\nlated in quadrature to zero center frequency, then each quadrature function \ncan be sampled according to the relationship: \n(14.5) \nwithout loss of information. The quadrature sampled waveforms must be \nreconstructed by the technique diagrammed in Fig. 14.6 to recover the orig-\ninal signal. From Eq. (14.5), quadrature sampling allows the analog-to-digital \nconverter to operate at one-half speed. As experimenters continue to press \nthe state of the art in digital signal processing, analog-to-digital-converter \nspeed is often the limiting constraint. \nRecall from Chapter 13 that the real and imaginary FFT outputs are \nin quadrature (i.e., complex). Hence, FFT band-pass filter applications are \na special case of quadrature sampling in that the output waveforms of each \nFFT band-pass filter are in quadrature. It then follows that these quadrature \nwaveforms can be translated, or down sampled, to zero center frequency \nwithout incurring irrecoverable spectrum overlap. This similarity follows in \nthat the processes of quadrature frequency translation to zero center fre-\nSec. 14.3 \nFFT Signal Detection \n337 \nquency and sampling can be interchanged. In quadrature sampling, we de-\nvelop zero-center-frequency waveforms in quadrature prior to sampling. \nConversely, in FFT band-pass filtering, we first obtain samples of the quad-\nrature band-pass waveform and then translate, or down sample, to zero \ncenter frequency. \n14.3 FFT SIGNAL DETECTION \nAn important application of the FFT is in signal detection. The detection of \na narrow-band signal buried in noise is a common signal-processing problem \nin communications, radar, and sonar systems. We describe in this section \nexample experimental results of applying this basic signal-analysis property \nof the FFT. Application of the FFT to digital matched filtering is also \nexplored. \nSignal Extraction Through FFT Resolution \nImprovement \nIn Fig. 14.13(a), we show a sampled time-domain sinusoid buried in \nwhite noise. As illustrated, there is no appearance of a sinusoid, only noise. \nThe signal-to-noise ratio is -12 dB. Obviously, one could never detect the \npresence of the sinusoid by examination of the time-domain samples shown. \nIn the frequency domain, we know that the periodic sinusoid has its \nenergy concentrated in a very narrow frequency band, whereas the noise \npower is spread throughout the frequency domain. Hence, if we take the \nFFT of \nthe waveform illustrated in Fig. 14. 13 (a) , we expect all the sinusoidal \nsignal energy to be concentrated in a few contiguous samples of the FFT \noutput. Recall from Chapter 13 that the N-output FFT samples can be in-\nterpreted as the output of N/2 contiguous band-pass filters. \nFigure 14.13(b) illustrates the FFT of the waveform of Fig. 14. 13(a). \nFor this example, N = 64; the 32 sample points shown in Fig. 14.13(b) \nrepresent the output power of each FFT filter. Power is computed as the \nsum of the square of the real and imaginary component of each filter output. \nThis result is doubled to account for negative frequency results. Although \nthe sample value representing the sinusoidal signal has a larger amplitude \nthan other samples, an experimenter could not be certain that the sample \nrepresents a periodic signal. \nTo firmly establish the presence of a signal, it is necessary to spread \nthe noise over more data points. Hence, we increase N to 512. In Fig. \n14.13(c), we show the resulting 256 FFT sample outputs. The presence of \na periodic signal in the noisy spectrum is very identifiable. \nTo compute the signal-to-noise ratio improvement that is achieved in \nFig. 14.13(c), note that the noise power has been evenly spread throughout \n338 \nFFT Signal-Processing and System Applications \nChap. 14 \n2.5 \n2.0 \n1.5 \n1.0 \n0.5 \n-0.5 \n-1.0 \n-1.5 \n-2.0 \n-2.5 \ny, \nxCkTI \n. \n. \nâ¢ \nâ¢ \nla) \n21 Hlnf'N , \nSINUSOIDAL SIGNAL \n.~ \nâ¢ \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11 \n12 \nIb) \n21 HlnfoM , \n~ \nSINUSOIDAL SIGNAL \n. \n.. ..... . ....-.-...... . .- . \nâ¢ - â¢â¢â¢ - â¢â¢â¢â¢ \n-â¢â¢â¢â¢â¢â¢â¢ : â¢â¢ \n- â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢ : â¢â¢â¢â¢â¢ e. : â¢â¢ \n: â¢â¢ \ne \nâ¢â¢â¢â¢â¢â¢â¢â¢ \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \nIe) \nn \n100 \nn \nFigure 14.13 Example signal buried in noise, where SIN = -12 dB: (a) time-\ndomain presentation, (b) FFT spectrum results for N = 64, and (c) FFT spectrum \nresults for N = 512. \nk \nSec. 14.3 \nFFT Signal Detection \n339 \n256 samples. Equivalently, the wideband noise has been filtered by 256 con-\ntiguous filters and the noise power output in the filter that contains the signal \nhas been reduced by 10 10glO (1/256) = \n- 24 dB. Because the signal power \nis concentrated in a single FFT filter, then the signal power is not reduced \nby the FFT. If \nthe original signal-to-noise level was -18 dB, then the output \nsignal-to-noise ratio of the FFT filter containing the signal is -18 - (- 24) \n= 6 dB. The sample indicating the signal in Fig. 14. 13(c) is clearly visible \nabove the noise. \nFFT Averaging \nSignal-to-noise enchancement as previously discussed cannot be ex-\ntended indefinitely. Sometimes the size of the FFT (i.e., the number of \nfilters) cannot be increased further because of computer memory limitations. \nAnother limiting factor is that the signal itself can spread over several con-\ntiguous FFT filters because of its bandwidth. For these cases, improvement \nin signal detectability can be achieved by averaging successive FFT power \noutputs. The effect of averaging is to smooth and reduce wild amplitude \nvariations that could be interpreted as sinusoidal signal components. \nIn Fig. 14. 14(a) , we show the FFT spectrum (N = 512) of a periodic \nsignal buried more deeply in noise than that previously considered. The \nperiodic component is not detectable. Assume that constraining factors limit \n. \n. \ny, \n. \n. \n. . . \nSINUSOIDAL SIGNAL \n.Yo .. \n. .. ... \n.. .. \n. . . \n. . \n. . . . \n. . \n. . \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n(a) \nSINUSOIDAL SIGNAL \n.~ \n1A :.' \nâ¢â¢â¢â¢â¢â¢ e \nâ¢â¢â¢â¢â¢â¢â¢ â¢â¢â¢â¢â¢ '\" \nâ¢ \nâ¢ \nâ¢ \nâ¢ \nâ¢ \nâ¢â¢ \nâ¢â¢â¢â¢ \nâ¢ \n1A:I. \nâ¢ \nâ¢â¢ â¢ \nâ¢â¢ â¢ â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢ \n: â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢ e â¢â¢â¢â¢â¢ \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n(b) \nFigure 14.14 Example signal buried in noise, where SIN = -24 dB: (a) FFT \nspectrum results for N = 512, and (b) averaged spectrum for 64 successive FFTs \nwith N = 512. \n340 \nFFT Signal-Processing and System Applications \nChap. 14 \nthe experimenter to an FFT of size N = S12. Averaging successive FFTs \nof this size is the appropriate signal-analysis procedure. \nThe resulting averaged spectrum is shown in Fig. 14.14(b) for 64 av-\neraged spectrums. The noise is now smoothed significantly and the signal \nis clearly visible above the smoothed noise level. It is to be noted that the \npower outputs of each FFT filter are averaged for the successive FFTs. \nHence, the phase of the input sinusoid is considered unknown and is not \ntaken into consideration. \nThe mathematics involved in analyzing the signal-to-noise enhance-\nment illustrated in Fig. 14.4(b) is extremely complicated (Refs. [9] and [21]). \nHowever, as a summary, in those cases where the original signal-to-noise \nratio is - 30 dB or less, the described averaging enhances signal detectability \nby approximately I.S log2 Q \ndB, where Q is the number of successive FFTs \nthat are averaged. For original signal-to-noise ratios well above - 30 dB, \nthe detectability gain approaches 3.0 log2 Q dB. \nThe signal illustrated in Fig. 14.14(a) is 24 dB below the noise level. \nThe number of successive FFTs averaged together is 64 = 26 and the FFT \nsize is N = S12. Averaging yields an enhancement of I.Slog2 Q = I.S log2 \n26 = 9 dB. From the results leading to Fig. 14. 13(c) , we know that the S12-\npoint FFT results in a 24 dB gain in signal-to-noise ratio. Hence, the pro-\ncessed signal-to-noise ratio is 24 + 9 - 24 = 9 dB and the signal is clearly \nvisible, as shown in Fig. 14.14(b). \nThe examples presented may appear as a simplistic application of the \nFFT to the signal-detection problem. However, it can be shown that the \noptimum signal-detection receiver for narrow-band signals with random \nphase, unknown frequency, and constant amplitude is a bank of band-pass \nfilters followed by a decision threshold (Ref. [23]). \nFFT Matched Filtering \nA matched filter is the signal processor design that optimizes the peak \nreceived signal-to-noise power ratio in the presence of additive white Gaus-\nsian noise. Mathematically, a matched filter frequency response is given by \nS*(f), where * \nindicates conjugation if the received signal s(t) has a Fourier \ntransform S(f). Practical high-speed matched-filter realizations are easily \nachieved because of the ease of FFT frequency-domain processing. \nFigure 14.1S illustrates the concept of a FFT matched-filter signal pro-\ncessor. The input signal is transformed to the frequency domain using the \nFFT and is multiplied by a stored frequency-domain conjugate replica of the \nreceived signal. At inverse FFT yields the desired matched-filter output \nwaveform. This output waveform is then compared to a threshold to deter-\nmine the presence or absence of the desired signal. The optimum signal \ndetector of a phase-modulated sinusoidal pulse in white noise is a set of \nSec. 14.4 \nFFT Cepstrum Analysis: Echo and Multipath Removal \nSAMPLED~ \nINPUT \nSIGNAL \ns(kT) L ___ \n--' \nFFT \nS(nlo) \nREFERENCE \nWAVEFORM \nFREOUENCY \nDOMAIN \nINVERSE \nFFT \nFigure 14.15 Block diagram of FFT matched-filter implementation. \n341 \nMATCHED \nFILTER \nOUTPUT \nmatched filters to the in-phase and quadrature-phase components of the \nsignal. For this reason, the FFT can be used in radar signal processors. \nPossibly the most important aspect of FFT matched filtering is the \nflexibility allowed the signal designer. Waveform variations are easily pro-\ncessed by simply storing the appropriate FFT coefficients. One can envision \na system where the signal and hence the matched filter can be changed \nrapidly. \n14.4 FFT CEPSTRUM ANALYSIS: ECHO AND MULTIPATH \nREMOVAL \nCepstrum signal-processing techniques (Refs. [2] and [3]) are of considerable \nutility. Specifically, these procedures are based on the premise that when \none examines the frequency transform of the logarithm of the Fourier trans-\nform, certain contaminating components, such as noise, unwanted signals, \netc., can be isolated. Cepstrum analysis is applied to many problem areas, \nincluding noise reduction in speech, sonar echo removal, radio-frequency \nmultipath interference rejection, image processing, and removal of multiple \nreflections in seismology. Because the analysis approach is generally an art-\nscience, we find it more meaningful to examine FFT cepstrum analysis tech-\nniques by means of specific examples. In this section, we describe FFT \ncepstrum analysis as applied to the detection and removal of multi \npath in-\nterference or echos from a desired waveform (Ref. [12]). \nMultipath or Echo-Removal Problem Definition \nAssume that a received signal sr(t) is given by \nSr(t) = s(t) + aos(t + 'l\"/) \n(14.6) \nwhere s(t) is the transmitted or desired signal and s(t + 'l\"/) is a multipath \nor echo component. The constant ao is the relative attenuation between the \ndirect and multipath components of the signal. If we take the Fourier trans-\n342 \nFFT Signal-Processing and System Applications \nChap. 14 \nform of Eq. (14.6) and then the logarithm, we obtain \nlog S \nr(f) = 10g[S(f) + aoS(f)ej21TfTl] \n= 10g[S(f)(1 + aoej21TfTl)] \n= log S(f) + 10g(1 + aoej21TfTl) \n= log S(f) - L (- W(a3In)ej21TfTl \nn=\\ \n(14.7) \nwhere the last term in Eq. (14.7) was obtained by a series expansion. The \nFourier transform of Eq. (14.7) is given by \nC[log Sr(f)] = C[log S(f)] - L (-l)n(a3In)8('J\" -\nn'l\"/) \n(14.8) \nn=\\ \nwhere C[ ] is taken as the Fourier transform. The Fourier transform of the \nlog of the spectrum frequency function is called the cepstrum. Note that the \nfirst term on the right-hand side of Eq. (14.8) is the cepstrum of the trans-\nmitted or desired signal. The remaining term is a sequence of impulse \nfunctions. \nCepstrum analysis then transforms the unwanted echo or multipath \nsignal component into a series of evenly spaced impulse functions. In Fig. \n14.16(a), we show the cepstrum of the signal only, and in Fig. 14.16(b), we \nshow the cepstrum of the signal plus the unwanted multi \npath signals. Theo-\nretically, the echo can be removed by removing the impulse functions and \nthen inverting the whole process to recover the transmitted signal. \nFFT Echo and Multipath Removal Implementation \nWe implement the cepstral analysis procedure by means of the block \ndiagram illustrated in Fig. 14.17. The log operation implies an exponential \nC[log[S(f)]] \n(a) \nCEPSTRUM \nSIGNAL ONLY \nC[log[S~f)]] \nCEPSTRUM SIGNAL \nPLUS MULTI \nPATH \n(b) \nFigure 14.16 Example depicting cepstrum analysis that identifies multi \npath signal \ncomponents as impulse functions. \nSec. 14.4 \nFFT Cepstrum Analysis: Echo and Multipath Removal \nPROCESSED \nSIGNAL \n343 \nFigure 14.17 Block diagram of cepstrum signal-processing procedure for re-\nmoving unwanted echo or multipath signals. \nlogarithm of both the real and imaginary parts of the FFT output. In par-\nticular, because the output of the FFT is of the form [R(nfo) + jI(nfo)], \nthen \n10ge[R(nfo) + jI(fo)] = logeHR2(nfo) + P(nfo)]I/2ejOn} \nwhere \n= loge[R2(nfo) + P(nfoWl2 + loge ejOn \n= loge[R2(nfo) + P(nfo)]112 + jan \n(14.9) \n(14.10) \nThe real part of the loge operation is placed in the real part of the FFT and \nthe imaginary part is placed in the imaginary part of the FFT. The resulting \nFFT yields the quefrequency series of Eq. (14.8). Because FFT use implies \nfinite-duration waveforms, then the impulse-response functions of Eq. (14.4) \nbecome [sin(T)]h functions. Manual or automated procedures can be used \nto identify and supress or filter the unwanted [sin(T)]h functions. \nWe show in Fig. 14.18 the results of the implementation of the FFT \ncepstrum processing procedure defined in Fig. 14.17. In Fig. 14.18(a), we \nshow an example transmitted signal with no multipath or echoes. Figure \n14. 17(b) illustrates the example signal with interfering multipath or echoes \npresent. Cepstrum analysis of the contaminated signal of Eq. (14.8) using \nthe FFT is shown in Fig. 14.18(c). The [sin(T)]h function at T[ is the undesired \necho signal. In Fig. 14.18(d), we set the echo functions to zero. Sequential \ninverse FFT, antilogarithm, and inverse FFT operations result in the re-\ncovered signal shown in Fig. 14.18(e). \n344 \nFFT Signal-Processing and System Applications \nChap. 14 \nS(I) -\nTransmitted Signal \n(a) \nS.(I) -\nReceived Signal \nC[log[S.(f)]) -\nCepslrum \n(e) \nC[log[S.(f))] -\nProcessed Cepslrum \n(d) \n5(1) -\nReconstructed TransmItted SIgnal \n(e) \nFigure 14.18 Example FFT results of \nthe application of the block diagram of Fig. \n14.17. \nSec. 14.5 \nFFT Deconvolution \n345 \n14.5 FFT DECONVOLUTION \nIn the discussion of the FFT digital filter design in Chapter 12, we assumed \nthat we could always compute a finite-duration filter impulse response. De-\nconvolution filter design problems do not normally satisfy this requirement \nand modifications to the techniques described in Chapter 12 are necessary. \nWe develop in this section an FFT procedure for designing digital decon-\nvolution filters. The technique is applicable to a wide variety of problems: \nspectral broadening in spectography, well logging in oil exploration, seismic \nexploration in geophysics, contrast enhancement in optics, and restoration \nof the output waveform of band-limiting filters (Refs. [2], [10], and [22]). \nDeconvolution Problem Definition \nTo define the deconvolution problem, consider Fig. 14.19. If \na signal \nis passed through a filter whose bandwidth is less than that of the signal, \nthe result is a smearing or broadening of the input waveform. Often com-\npensation of the filter itself can be employed to remove this unwanted dis-\ntortion. An alternate or sequential approach is to apply appropriate mathe-\nmatical procedures to the output waveform and thereby restore the input \nwaveform. Because the output of a filter can be written as the convolution \nofthe input waveform and the impulse response of the filter, then the mathe-\nmatical operation for attempting to remove this convolution operation is \ntermed deconvolution. \nMathematically, the deconvolution problem is stated as follows. Recall \nfrom Ex. 4.4 that a linear system is characterized by the convolution integral: \ny(t) = J:\"\" x(T)h(t -\nT) dT \n(14.11) \nwhere x(t) is the input signal, h(t) is the system impulse response, and y(t) \nis the output signal. In this discussion, we assume that the impulse response \nis known. Given h(t) and the output y(t), it is desired to determine the input \nsignal x(t). From the convolution theorem, we can write Eq. (14.11) equiv-\nalently in the transform domain as \nY(f) = X(f)H(f) \n(14.12) \nThe theoretical inverse filter R(f) can be determined by solving for XU) in \nFILTER \n~ \nDECONDITION JL \n.. \nFILTER \nX(I' \nx(I' \nhIt' \ny(t, \nr(t, \nFigure 14.19 Graphical definition of the deconvolution filtering problem. \n346 \nFFT Signal-Processing and System Applications \nChap. 14 \nEq. (14.12): \nX(f) = [11 H(f)] Y(f) = R(f) \nY(f) \nR(f) = II \nH(f) \n(14.13) \nor, equivalently, \nx(t) = ret) * yet) \n(14.14) \nThe meaning of the term deconvolution filter is now apparent. Theoretically, \nwe can recover the signal x(t) perfectly, but as will be seen in the following \ndiscussion, practical considerations force us to determine xct), an estimate \nof x(t). \nFFT Deconvolution Filter Design \nFrom Eq. (14.13), the inverse filter is defined in the frequency domain \nas R(f) = IIH(f); thus, we have a frequency-domain specification FFT \ndigital filter-design problem. However, in general, H(f) tends to zero as \nfrequency increases and, as a result, R(f) tends to infinity as frequency \nincreases. For this reason, it is normally impossible to sample this frequency \nfunction and compute an inverse filter impulse response rCt) of finite \nduration. \nA logical way to approach this issue is to multiply II \nH(f) by a truncation \nfunction W(f). The resulting frequency function is then zero for all fre-\nquencies greater than the truncation frequency f \nco This apparently solves \nthe problem in that it is now feasible to evaluate the inverse discrete trans-\nform of W(f)/H(f). However, we know from previous discussions that trun-\ncation in the frequency domain yields ripples in the time domain. As a result, \nW(f) must be chosen to be a function that gently tapers to zero for some \nfrequency f c and is zero for f > f c. A good compromise is the Hanning \nfunction. Figure 14.20 illustrates the proposed frequency-domain modifi-\ncation concept. \nThe required frequency-domain approximation is obtained by modi-\nfication of Eq. (14.13): \nX(f) = [W(f)/ \nH(f)] Y(f) = R(f) \nY(f) \n(14.15) \nwhere W(f) is the truncation function and \nR(f) = W(f)/ \nH(f) \n(14.16) \nThe approximation of Eq. (14.15) is the inverse filtering equation that we \nimplement by means of the FFT. Note that R(f) is simply a frequency-\ndomain specification of a filter, as discussed in Chapter 12. \nSec. 14.5 \nFFT Deconvolution \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \n,'~ \nI \nH(I) \nI \nI \n\" ~ \nW(I) \n/ \n'\" \n\\Hm \n~~'\" \\ \n\\ \nI \n\\',J \n1', \n'-''- H(I) \n........... ......... \n347 \nFigure 14.20 Weighting-function modification of the inverse filter frequency \nresponse. \nFFT Deconvolution Implementation \nTo illustrate deconvolution filter design, assume that the impulse re-\nsponse of a nonphysically realizable system is given by the function: \nh(t) = 1/2ue-at \n(14.17) \nThis impulse-response function is representative of system responses found \nin many practical signal-restoration problem areas. The Fourier transform \nof Eq. (14.17) is \n(14.18) \nThe analytical expression for the inverse filter frequency response for \na Hanning truncation function is \nR(f) \n1/2 + Y2 cos(7rf/fe) \nU2/[U2 + (27rf)2)] \no \n(14.19) \nf > fe \nThis frequency-response function is sampled and a filter is designed by the \n348 \nFFT Signal-Processing and System Applications \nChap. 14 \nFFT frequency-domain design procedures developed in Chapter 12. Recall \nthat care must be exercised to avoid convolution end effects. \nTo indicate the degree of signal restoration that can be accomplished \nby FFT deconvolution filtering, a simulated input waveform consisting of a \nsum of Gaussian functions is assumed. The input signal and the waveform \n1.0 \n.9 \n.8 \n.7 \nw \ng .6 \n.... \nii! \n.5 \n::;; \nÂ« .4 \n.3 \n.2 \n1 \n1.0 \n.9 \n.8 \n.7 \n~ .6 \n::> \n.... \n.5 \n:::; \n\"-\n~ .4 \n.3 \n.2 \n.1 \n,.. \nI \n\\ \nI \nInput Signal--\n\\ \nI \nI \n\\ \n, \n\\ \" \nI \n; \n\" ' I \nH(t) = \n1 \nI \n!/' \n~ \n1 + (27Tfla)2 \n,/ I'-.... 1// \\ \na=1 \n1 \nI , \n\\ \nIii \n\\ I \nI \n!\\.. \nV \nOutput Signal \n/ \nI \n\" \nV \n,\\ \n-V \nI \n~ \"'-r--\n.1 \n.2 \n.3 \n.4 \n.5 \n.6 \n7 \n.8 \n.9 \n1.01.1 \n1.21.31.41.51.6 \n(a) \nA \n\" \n(1 \nr \nI \nI \nI \nI \n---- fc=50Hz_ \n\\ \n-- fc= 20Hz \nK1 \n-\n.. -\nfc= 15Hz -\n, \n, \nI \n, \nr1 \nI \n~i \n~.j \nI \n'/ \n\\I \n1 \n1/ \n\\t \n.1 \n.2 \n.3 \n.4 \n.5 \n.6 \n.7 \n.8 \n.9 \n1.01.1 \n1.21.31.4 1.51.6 \n(b) \nFigure 14.21 Example deconvolution waveforms: (a) low-pass system input and \noutput waveforms, and (b) deconvolution results as a function of the truncation \nfrequency f c. \nSec. 14.6 \nFFT Antenna Design Analysis \n349 \nresulting from its convolution and the exponential impulse response of Eq. \n(14.17) are shown in Fig. 14.21(a). It \nis this output signal to which the inverse \ndigital filter of Eq. (14.19) is applied. \nFigure 14.21(b) illustrates deconvolved waveforms as a function of the \nparameter fe. Because parameter fe determines the width of the frequency-\ndomain truncation function, we observe that as f \ne is increased, the decon-\nvolved waveform more closely approximates the input signal. Note that for \nall practical purposes, the input signal is completely restored; the degree of \ndeconvolution that is possible is limited principally by the presence of noise. \nIf we assume that the signal and noise cannot be identified with respect \nto the statistics required for the application of sophisticated statistical de-\nconvolution techniques, then the procedure developed here is experimen-\ntally applied. We simply decrease the value of the parameter fe until sat-\nisfactory results are achieved. In general, if \na high level of noise is added \nto either the impUlse response or the output, then reasonably accurate de-\nconvolution is not possible. The deconvolution approach proposed here must \nbe modified if the filterfunction is zero-valued for f < fe, (see Prob. 14.17). \nSilverman [22] describes a theoretically more correct although more com-\nplicated FFT deconvolution procedure. \n14.6 FFT ANTENNA DESIGN ANALYSIS \nThe Fourier transform has long been recognized as a useful tool in the so-\nlution of antenna design problems. However, these analyses were largely \nlimited to those cases for which the Fourier integrals could be evaluated by \nclassical methods. With the FFT, Fourier transform analysis is considerably \nmore effective. \nIn this section we develop the fundamentals for applying the FFT to \nantenna design analysis. Our approach is limited to a consideration of one-\ndimensional apertures. This may appear inadequate in that antennas are \ngenerally considered in two dimensions. However, the treatment is adequate \nfor a great many antennas whose directivity is separable into a product of \ndirectivities of one-dimensional apertures and where spacial patterns are \nsurfaces of revolution of the two-dimensional pattern that is produced by \nthe one-dimensional aperture. Further, the one-dimensional case develops \nthe analogy of antenna patterns and the Fourier transform. Our results are \nreadily extendable to two dimensions. \nFourier Transform Relationship Between Antenna \nAperture Distribution and Far-Field Pattern \nConsider the electric field distribution over the aperture of length a, \nas shown in Fig. 14.22. This electric field aperture distribution model rep-\n350 \n--\nÂ·a \n2\" \nFFT Signal-Processing and System Applications \nChap. 14 \nE(x)-ELECTRIC FIELD APERTURE \nDISTRIBUTION \na \n2\" \nT \nr \nx \nFigure 14.22 One-dimensional antenna \nelectric field aperture distribution. \nresents a conventional electromagnetic horn antenna or a simple dipole an-\ntenna. As shown, the electric field is zero over the part of \nthe plane occupied \nby the conductor but has a uniform electric field distribution over the horn \nopening (or dipole dimension). \nThe far-field pattern, as a function of 6, where 6 is measured from the \nperpendicular to the aperture distribution, is given by (Refs. [1] and [13]): \nE(6) = f:oo E(x)e - j21Tx[sin(9)]/>. dx \n(14.20) \nwhere E(x) = electric field aperture distribution, volts/meter; \nE(6) = far-field radiation pattern, volts; \n6 = direction of antenna field pattern measured from perpendic-\nular to aperture dimension, degrees. \nEquation (14.20) is a Fourier transform, where the aperture dimension x is \nanalogous to time t and the direction function [sin(6)]!A is analogous to fre-\nquency f in the conventional Fourier transform relationship. \nNote that the analogous relationship between frequency f and [sin(6)]/ \nA \nmust be interpreted correctly in that the variable f is defined from -\n00 to \n+ \n00, whereas 6 is periodic over the interval 0 to 211\". As a result, the Fourier \ntransform relation of Eq. (14.20) is uniquely defined over a finite range of \nthe variable 6. We further explore this antenna pattern Fourier transform \ninterpretation problem in the following example. \nExample 14.3 Antenna Far-Field Pattern Fourier Transform Computation \nAssume that the electric field aperture distribution E(x) is as shown in Fig. 14.22. \nDetermine the far-field pattern from the Fourier transform relationship ofEq. (14.20) \nand compare with the conventional Fourier transforms results if Fig. 14.22 is con-\nsidered a time-domain waveform, that is, if \nthe length dimension x is interpreted as \na time dimension t. \nFirst, let us compute the conventional Fourier transform of the waveform il-\nSec. 14.6 \nFFT Antenna Design Analysis \n351 \nlustrated in Fig. 14.22: \nE(f) = fO\", E(t)e -j2T,jt dt = L\"'\"\" Eoe -j2-rrjt dt \n(14.21) \n= Eo [sin(1Taf)]/1Taf \n(14.22) \nAs expected, the pulse waveform yields the [sin(f)]lf function of Eq. (14.22). This \nresult is plotted in Fig. 14.23(a) for parameter a = 1. \nTo determine the far-field antenna pattern, we use Eq. (14.20) and Fig. 14.22: \nE(6) = f-\"'\", Eoe -j2-rrx[sin(8)]/~ dx \n(14.23) \n= Eo sin{1Ta[sin(6)]/A} \n1Ta[sin(6)]/A \n(14.24) \nTo plot the antenna pattern of Eq. (14.24), we must relate the antenna aperture \ndimension a to the frequency at which the antenna is to be used. Assume that a = \n2A. For this case, Eq. (14.24) yields the antenna pattern shown in Fig. 14.23(b). \nNow let us compare the results ofEqs. (14.22) and (14.24), that is, Figs. 14.22(a) \nand (b). As shown in Fig. 14.23(a), the frequency function is defined for all frequency \nvalues from +00 to -00. (The negative frequency function is a mirror image of the \npositive frequency function of Fig. 14.23(a) and is not shown for clarity.) In contrast, \nthe antenna pattern in Fig. 14.23(b) is periodic over the interval -90Â° to + \n90Â°. Hence, \nwhen one attempts to compare the two results, it is readily apparent that the con-\nventional Fourier transform results of Fig. 14.23(a) must be truncated if we are to \nconvert these results to those of Fig. 14.23(b). \nTo determine the appropriate conversion factor and the truncation value, com-\npare the defining relationships of Eqs. (14.21) and (14.23). We note the following \nequalities: \nx = t \n(14.25) \n[sin(6)]/A = f \nHence, we convert Fig. 14.23(a) to Fig. 14.23(b). We determine 6 from the \nrelationship \n(14.26) \nBecause the maximum nonperiodic value of 6 is 90Â°, then the maximum value of f \n(i.e., the truncation value) occurs for fA = 1. Recall that A was chosen as al2 in \nFig. 14.23(b) and a was chosen as 1 in Fig. 14.23(a). Hence, A = V2 and the \ntruncation value of f is 2 Hz. As a result, to convert Fig. 14.23(a) to Fig. 14.23(b), \nwe use only the main lobe and first side lobe of Fig. 14.23(a) and determine the \nabscissa axis 6 from Eq. (14.26). Note that we have illustrated by means of symbols \non Figs. 12.22(a) and (b) several conversion values. As shown, we truncate the \nconventional Fourier transform results of Fig. 14.23(a) at f = 2 Hz. \nThe results of Fig. 14.23(b) are plotted in conventional polar-coordinate form \nin Fig. 14.23(c). Observe that the results are symmetrical for angles greater than \nÂ± 90Â°. This follows from our electric field aperture distribution assumption in that \n352 \n1.0 \n9 \n.8 \n.7 \n.6 \n.5 \n.4 \n.3 \n.2 \n1 \n-.1 \n-.2 \n-.3 \n-.4 \n1.0 \n.9 \n8 \n.7 \n6 \n.5 \n.4 \n.3 \n.2 \n.1 \n-.1 \n-.2 \n- 3 \n-.4 \nE(I) \n0.5 \nE(0) \n150 \n_90Â° \nFFT Signal-Processing and System Applications \nEo Sin (ITal) \nITat \n(a) \nEo sin[ITa sin(0)/ \nAl \nITasin(El)IA \n(b) \na = 1 \n0Â° \n(e) \nEl \n90Â° \nChap. 14 \nFigure 14.23 (a) Conventional Fourier transform of the time function of Fig. \n14.22, (b) far-field antenna pattern for the aperture distribution of Fig. 14.22, and \n(c) polar coordinate graphical presentation of part (b). \nSec. 14.6 \nFFT Antenna Design Analysis \n353 \nFig. 14.22 can be interpreted as the aperture distribution in any plane revolved around \nthe abscissa axis. Hence, the antenna pattern is expected to be symmetrical. \nFFT Antenna-Pattern Computation \nTo apply the FFT to the computation of antenna patterns, we simply \nimplement the basic principles previously established. That is, we consider \nthe aperture electric field distribution as a time-domain waveform; compute \nthe FFT of this waveform and then apply the appropriate abscissa scale-\nconversion factor of Eq. (14.26). \nIn Fig. 14.24(a), we show an example electric field aperture distribution \nthat alternates in phase and has constant amplitude. Note that the aperture \ndistribution function is symmetrical about the origin. We must be careful to \npreserve this relationship when applying the FFT. This is accomplished by \nsampling the aperture distribution function, as shown in Fig. 14.24(b). We \nuse the fact that the sampled function to which the FFT is to be applied \nmust be periodic. The number of \nzeros that one introduces is strictly a matter \nof choice as to the desired FFT frequency spacing to allow one to easily \ntrace the side-lobe structure of the antenna pattern. \nFigure 14.24(c) illustrates the FFT of the sampled aperture distribution \nof Fig. 14.12(b). This result must be converted or transformed, as is de-\nscribed in Ex. 14.3. Let us assume that the distances shown in Fig. 14.24(a) \nare in meters and that we wish to determine the antenna pattern for a wave-\nlength A = V2 m. Then, from Eq. (14.26), the truncation frequency value \nis 2 Hz. As a result, we transform or convert the FFT results of Fig. 14.12(c) \nto those of Fig. 14.24(d) by means of Eq. (14.26). Only the results for the \nfrequencies 0 ~ \nf ~ \n2 Hz are converted. As before, the antenna pattern for \nangles greater than Â± 90Â° is a replica of the pattern for angles less than Â± 90Â°. \nThe corresponding polar plot is shown in Fig. 14.24(e). \nRecall that as the wavelength of the antenna becomes small with re-\nspect to the aperture dimension, then the main lobe of the antenna becomes \nnarrow and the number of side lobes is increased. To see this effect, let us \nconvert or transform the FFT results of Fig. 14.24(c) for a wavelength A = \nVs m. From Eq. (14.26), the truncation frequency is now 5 Hz. We illus-\ntrate the resulting converted polar plot in Fig. 14.24(t). \nWe have developed a simplified application of the FFT to antenna-\npattern analysis. Our approach requires a conversion ofthe far-field radiation \nintegral of Eq. (14.20) to a Fourier Integral. A more detailed application of \nour approach is given in Ref. [25]. Results presented here can be extended \nto the two-dimensional analysis of antenna apertures. The radiation pattern \nof reflector antennas is determined in Refs. [5] and [14] by the FFT and a \n[sin(u)]/u sampling approach. Incorporation of the FFT with the conjugate \ngradient method is used to solve for the aperture fields and the induced \n354 \nFFT Signal-Processing and System Applications \nChap. 14 \nE(d/'A, \nEld/'A, \nÂ·1.0 \n1.0 \nn \nd/'A \nÂ·1 \n'-!---N \n= 32'--\"'''-\nla' \n14 \nIElnlol1 \n0 \n0 \nIEf9Il \n12 \n1\\ . \nIf \n14 \n10 \n12 \n8 \n10 \n6 \n8 \n4 \n6 \n2 \n-0 ,t'v:~~L1, .. \n1\\ \n{' \nI 0- 1 10~ \n6 2 4 6 8 10 12 14 16 18 2022 2426 2830 \nn \nI I \nI \nI \nI \nI \nI \nI I'll I \nI \nI -\n-0 \nI \n\"\" \n.625 1.8753.125 4.375 \n-3.75 -2.5 \n-1.25 \nI \n10 20 30 40 50 60 70 80 \n1.25 \n2.5 \n3.75 \n5 \nIe' \nId, \nIe' \nII, \nFigure 14.24 (a) Example one-dimensional antenna aperture distribution, (b) \nsampled aperture distribution for FFT computation, (c) FFT of the sampled ap-\nerture distribution of part (b), (d) angle transformation of the FFT results of part \n(c) for 'A = 0.5 meters, (e) polar plot of part (d) for 'A = 0.5 meters, and (t) polar \nplot of part (d) for 'A = 0.2 meters. \nI \n.. \n90 \ne \nSec. 14.7 \nFFT Phase-Interferometer Measurement System \n355 \ncurrent densities for wire, wire mesh, and rectangular plate antennas in Refs. \n[7] and [20]. \n14.7 FFT PHASE-INTERFEROMETER MEASUREMENT \nSYSTEM \nThe FFT can implement a phase-measurement system based on the inter-\nferometer principle. Recall that the phase difference between waveforms \nreceived at two spatially separated sensors (antennas) separated by a dis-\ntance d can be used to determine the angle of arrival of the waveform from \nthe relationship (see Fig. 14.25): \no = sin - I (A<I>/2'TTd) \n(14.27) \nwhere 0 = angle of arrival \nA = signal wavelength \n<I> = phase difference \nd = antenna separation \nEquation (14.27) is the classical phase-interferometer equation for computing \nthe direction of arrival of a plane wavefront. We now show the procedures \nfor applying the FFT to phase-interferometer measurement systems. \nFFT Phase Interferometer \nThe block diagram of \nan FFT interferometer direction-of-arrival system \nis illustrated in Fig. 14.25. As shown, the output of each sensor or antenna! \nreceiver is sampled by an analog-to-digital (AID) converter and the FFT of \neach sensor output is computed. Because each resolution element of the \nFFT consists of a real and an imaginary term, then the phase On of each \nFFT filter output can be computed as \nlin = tan-1 [Real Output (Rn)] \nu \nn = 0, 1, ... , NI2 \n(14.28) \nImag Output (In) \nEquation (14.28) is computed for the FFT outputs for each of the two chan-\nnels. Phase difference <l>n is then determined by simple subtraction for each \nFFT resolution cell (filter output). \nThe next process step, system phase correction, is the single most \npractical consideration in considering the application of the FFT to inter-\nferometer signal processing. A limiting factor in the accuracy of a direction \nfinding system is the differential phase error between the two channels. \nSystem designers attempt to perfectly match the two channels from sensor \n356 \nFFT Signal-Processing and System Applications \nChap. 14 \nPHASE DIFFERENCE \nCOMPUTATION -\ncl>n \nANGLE OF ARRIVAL \n8 = \nsino, A(cI>n â¢ B n) \n21Td \nFigure 14.25 Block diagram of an FFf phase-interferometer measurement \nsystem. \n(or antenna) to receiver output, but in practice phase error exists. Calibration \nis often necessary to achieve sufficient accuracy. \nWith an FFT implementation, it is possible to calibrate the system for \nall frequencies within the passband of the receiver. For example, a broad-\nband signal can be injected perpendicular to the sensor array. The phase \ndifference between the two channels should be zero. If the phase difference \nbetween corresponding FFT cells differs from zero, this differential is due \nto system inaccuracies for that frequency cell and the error can be stored \n(by cell) as a system phase correction. System calibration can be repeated \nas often as required. \nThe angle of arrival can be determined for each frequency cell of the \nFFT. If \nthe signal bandwidth is greater than the FFT bandwidth, adjacent \nFFT cells should give near-identical results. Note that the proposed imple-\nmentation concept also yields the angle of arrival of multiple signals if their \nfrequencies do not overlap. \nSec. 14.8 \nFFT Time-Oifference-of-Arrival Measurement System \n357 \nMeasurements in the Presence of Interference \nAnother distinct advantage of an FFT interferometer system is the \ncapability to cope with interfering signals. A conventional phase-measure-\nment system normally is reasonably well-matched to the signal bandwidth \nand if an interfering signal overlaps any portion of this bandwidth, then the \nresulting phase measurement is in error. With the FFT approach, the re-\nceiver output is divided by the FFT into a band of narrow-band filter outputs. \nThe phase difference is computed simultaneously for each of the filter out-\nputs across the receiver bandwidth. In most instances, the interfering signal \ndiffers in angle of arrival from the signal of interest. Hence, on an angle-\nvs.-frequency plot, one will see two straight-line segments, one for the signal \nof interest and one for the interfering signal. Unless the interfering signal \ncompletely overlaps in frequency the desired signal, then an angle-of-arrival \nmeasurement can be made. \nFFT Monopulse Direction-Finding System \nThe FFT can also be applied to the development of an amplitude-\ncomparison monopulse direction-finding system. An amplitude comparison \nof each filter output provides the appropriate measurement. Note that, as \nin the interferometer case, it is straightforward to develop a calibration pro-\ncedure for each FFT resolution cell. \n14.8 FFT TIME-DIFFERENCE-OF-ARRIVAL \nMEASUREMENT SYSTEM \nThe accurate measurement of the time difference of arrival for narrow-band \nsignals arriving at spacially separated sensors is an excellent application of \nthe FFT. Analog correlators can be used, but system inaccuracies severely \nlimit the fields of application. In this section, we address the basics in ap-\nplying the FFT to time-difference-of-arrival measurements. \nProblem Definition \nThe FFT is applied to time-difference-of-arrival measurements by im-\nplementing classical correlation techniques. From Chapter 4, the correlation \nfunction for a waveform St(l) that arrives at a sensor at some time to and a \nreplica of that same waveform, S2(t), arriving at a different sensor at some \nlater time to + T is given by \n(14.29) \n358 \nFFT Signal-Processing and System Applications \nChap. 14 \nBy definition, the correlation function measures the degree of match or cor-\nrelation between a waveform and a shifted replica of the waveform. Hence, \nEq. (14.29) reaches a maximum for that shift 'T that corresponds to the time \ndifference of arrival of \nthe waveforms at the sensors. We determine the value \nOf'Tmax at the correlation peak by using the FFT to compute the discrete \ncorrelation theorem discussed in Sec. 7.4. \nFFT Time-Difference-of-Arrival Measurement \nThe basic computational procedure for FFT application to the time-\ndifference-of-arrival measurement is illustrated in Fig. 14.26. As shown, \nwaveforms s \\ \n(t) and its replica S2(t) arrive at spacially separated sensors at \ntime difference 'T. Each sensor output is sampled by an analog-to-digital \nconverter and input to an FFT. Recall from Chapter 7 that zeros must be \nappended to the sampled waveforms to avoid end effects. From the FFT \noutputs, we form the correlation theorem product S \\(f)si(f), where \nSi(f) is the complex conjugate of S2(f). The resulting complex function is \ntermed the cross spectrum. The cross spectrum can be viewed as an am-\nplitude and phase spectrum, as shown in Fig. 14.26. The inverse FFT of the \ncross spectrum is the desired cross-correlation function. The cross-corre-\nlation function peaks at the value of 'T max corresponding to the desired time-\ndifference-of-arrival measurement. \nAlthough the procedure described appears straightforward, there is one \nshortcoming. Examine the output cross-correlation in terms of an accurate \ndetermination ofthe peak value. The time resolution ofthe cross-correlation \nfunction is determined by the sampling interval of the waveforms s \\ \n(t) and \nS2(t). If \nthe sampling interval is Ts, then the cross-correlation time resolution \nis T s, which is not sufficiently accurate for practical applications. As a result, \nwe must interpolate between the samples of the cross-correlation function \nto determine 'Tmax. As long as the Nyquist sampling rate for the input signals \nis observed, then, theoretically, the continuous cross-correlation waveform \ncan be reconstructed. \nFFT Phase-Domain Time-Difference-of-Arrival \nMeasurement \nAn alternate approach to measure the time difference of arrival is to \ncompute the slope of the phase-domain function. Note from Fig. 14.26 that \nthe phase slope is equal to 2'lT'l\"max. This follows from the time-shifting theo-\nrem (Sec. 3.4). Hence, rather than implementing an interpolation procedure \nto accurately estimate 'Tmax , we simply estimate the slope of the phase \nspectrum. \nSeveral alternatives for slope estimation should be explored based on \nthe specifics of the problem. A weighted least-square approach based on \n51 (t) \n52(t) \nFFT \nFFT \nSI(1) \nMULTIPLICATION \nSI (I) S;(t) \n*conjugate \nS2(t) \nAMPLITUDE \n--7----. \n. \nTIME-DIFFERENCE-\nOF-ARRIVAL -I \nâ¢ â¢â¢â¢â¢ \nPROCESSING \n~-- - -.; \n-\n-\nTHRESHOLD \n' .. \nsa \nCROSS-SPECTRUM \n\". \n~/.. \nr,)\"-\n<\".~ \n.,,( â¢ SLOPE = 2\"1mu \nCROSS-COR RE LA \nTION \nI-\n1mu \nDELAY-1 \nINVERSE \nFFT \nCROSS-\nCORRELATION \nFigure 14.26 Computation procedure for an FFT time-difference-of-arrival measurement \nsystem_ \n(J) \n(l) \np \n.j:. \na, \n01 \n~ \n--i \n3-\n(l) \n9. \nCD \niil \n::J \n(') \ncp \no \n~ \n<-\n~ \ns:: \n(l) \nIII \n(J) \nc: \nm \n3 \n(l) \na \n(J) \n'< \n(J) \nCD \n3 \nw \nen \n10 \n360 \nFFT Signal-Processing and System Applications \nChap. 14 \ncross-spectrum amplitude (shown in Fig. 14.26) appears the most practical. \nThis procedure allows one to give the most weight to the frequency resolution \ncells where the signal-to-noise ratio is greatest. In fact, a procedure to elimi-\nnate from the slope estimation those data from low cross spectrum amplitUde \ncells is probably most advantageous. One can also average consecutive \nphase-spectrum data in order to increase the signal-to-noise ratio. \nAn FFT time-difference-of-arrival system can be calibrated, as dis-\ncussed in the previous section. Note that if the input waveforms have no \ndelay between them, then the cross-phase spectrum should be zero. Any \ndeviations from zero are due to system phase-differential errors. This cal-\nibration data can be stored and the phase-spectrum data can be appropriately \ncorrected for each measurement. \n14.9 FFT SYSTEM SIMULATION \nAccurate prediction of system performance often requires the development \nof simulation techniques to verify the design criteria. Radar, communica-\ntions, sonar, and imaging system designers use the FFT for digital simulation \nanalysis to reduce hardware design cost. \nThe general class of systems for which FFT simulation is applicable \nare those that can be characterized by open-loop transfer functions. This \nfollows because the FFT requires that a block of data is processed simul-\ntaneously. Systems whose nonlinearities are characterized in the time do-\nmain can also be easily simulated. FFT simulation is appealing to a system's \nanalyst because of the simplicity of designing the simulation. As we saw in \nChapter 12, either a time- or frequency-domain specification of system func-\ntions can be implemented by the FFT. This implies that equations familiar \nto the system's hardware engineer are used directly in the simulation. \nTo explore the potential of FFT simulation techniques, we describe in \nthis section the application of \nthe FFT to the prediction of \nradar performance \nin a specified environment. This problem is in general nontractable by con-\nventional analysis techniques and is characteristic of classical background \nclutter, electronic countermeasure (ECM) and electronic counter-counter-\nmeasure (ECCM) problems. The simulation can be extended to more so-\nphisticated radar signal-processing techniques, including matched receivers, \ndoppler filtering, optimum signal design, chirp waveforms, and phased \narrays. \nFFT RadarÂ·System Simulation \nThe block diagram of a simplified radar receiver is shown in Fig. 14.27. \nThe mixer and the local oscillator convert the radio-frequency (RF) signal \nto an intermediate frequency (IF), where the converted signal is amplified \nSec. 14.9 \nFFT System Simulation \n361 \nIFAMPUFIER \nFigure 14.27 Block diagram of a simplified radar receiver. \nand filtered. The (RF) pulse modulation is extracted by the detector and \namplified by the video amplifier. Target range information is extracted by \nthe video processor. The key to radar performance in a clutter or jamming \nenvironment is generally determined by the capability of \nthe video processor. \nThe classical radar-analysis problem is to determine the degradation \nof range extraction by the video processor as a function of input-noise char-\nacteristics. System degradation is normally measured in terms of probability \nof detection and probability offalse alarm. If \nthe received noise is Gaussian, \nthen closed-form solutions for system performance can be obtained. How-\never, it is necessary to resort to a simulation in order to evaluate system \ndegradation if the noise is an interfering signal with specified modulation \ncharacteristics. A digital FFT simulation of the block diagram illustrated in \nFig. 14.27 is a cost-effective method for evaluating system performance in \nthese cases. \nFigure 14.28 illustrates the radar model chosen for simulation and the \ncorresponding FFT simulation block diagram. The received waveform is \nsimulated by generating samples of the additive combination of the pulsed \nIF waveform and the interfering signal. Both the IF and video amplifiers \n(filters) are simulated by sampling their respective transfer functions in the \nfrequency domain, as discussed in Chapter 12. \nThe IF amplifier is assumed to be a cascade combination of \nButterworth \nfilters whose center and cutoff frequencies are chosen to enhance the rolloff \nIFAMPUFIER \nSAMPLED \nIF FllJER \nTRANSFER FUNCTION \nSAMPLED \nLOW PASS \nTRANSFER FUNCTION \nFigure 14.28 FFT Simulation model of a radar receiver. \n362 \nFFT Signal-Processing and System Applications \nChap. 14 \nof the skirts of the resulting transfer function. Conventional analog filter \ndesign equations are sampled in the frequency domain. Filtering at IF is \naccomplished by forming the product of the FFT of the sampled input and \nthe sampled filter function; this product is then inversely transformed to \nobtain the time-domain output of the filter. \nNonlinear square-law detection is simulated by squaring the IF time-\ndomain waveform. Simulation of the video amplifier (filter) is accomplished \nin the same manner as the lF amplifier. The video amplifier is assumed to \nbe a three-pole Butterworth low-pass filter that is simulated by frequency-\ndomain sampling. The resulting output video waveform is representative of \nthe receiver performance in the presence of the simulated interference. \nFFT Radar-System Simulation Results \nTo illustrate the waveforms that can be obtained by this simulation \nmethod, Gaussian noise is added to the input waveform shown in Fig. 14.29. \nAn estimate of the power spectrum of the input waveform obtained by com-\nputation of a Hanning weighted FFT is shown in Fig. 14.30(a). A similarly \ncomputed IF output power spectrum is shown in Fig. 14.30(b). The detected \nvideo output is illustrated in Fig. 14.30(c). If a sequence of system video \noutputs is generated by the simulation, each with independent noise samples, \nstatistical parameters such as probability of detection, probability of false \nalarm, error rate, etc. can be evaluated as a function of the characteristics \nof the interfering signal and the parameters of the video processor. \n.2 \n.4 \n1.2 \n1.4 \n1.6 \n1.8 \n2.0 \nTI ME (IlSEC) \nÂ·1 \nFigure 14.29 FFT radar-system simulation input waveform. \nSec. 14.9 \nw \nQ \n1.0 \n.8 \n~ .6 \n:::i \n0.. \n::;; \n<{ \n::;; \n:J \ncc \nt; \n.4 \nw \n0.. \n(J) \n.2 \nFFT System Simulation \nFREQUENCY (MHz) \n(a) \nFigure 14.30 FFT radar-system simulation results: (a) power spectrum of input \nsignal plus noise, (b) IF output power spectrum, and (c) detected video output \nsignal. \nCommunication-System Simulation \n363 \nFFT simulation techniques are also readily adaptable to communica-\ntion systems. In digital data systems, a common problem encountered is the \nestimation of intersymbol intererence as a function of noise level, data rate, \ntransmitter bandwidth, transmitter filter roll \noff characteristics, and system \nsynchronization parameters. Analogous to the radar problem, an FFT simu-\nlation can be implemented to evaluate the probability of correctly decoding \na transmitted message as a function of each parameter degrading system \nperformance. An FFT simulation approach to communication-system anal-\nysis allows one to include real-world constraints that are normally unwieldly \nin closed-form analysis. \n364 \nFFT Signal-Processing and System Applications \nChap. 14 \n1.0 \n.8 \nw \nC \n.6 \n:::l \nI-\n::::; \n11. \n::;: \nÂ« \n::;: \n:::l \na: \nI-\n.4 \nu \nw \n11. \nUl \n.2 \no+-____ \n~~wu~--~--------~------+_------~----~ \nw 3 \nc. \n:::l \nI-\n::::; \n~ 2 \nÂ« \no \nw \nC \n> 1 \no \n50 \n.2 \n.4 \n.6 \n100 \n150 \n(b) \n.8 \n1.0 \n1.2 \n1.3 \n(e) \nFigure 14.30 (continued) \n200 \n1.6 \n250 \nFREOUENCY (MHz) \n1.8 \n2.0 \nTIME (\"SEC) \nSec. 14.10 \nFFT Power-Spectrum Analysis \n365 \n14.10 FFT POWER-SPECTRUM ANALYSIS \nThe measurement of power spectra is a difficult and often misunderstood \ntopic. Because the FFT readily yields frequency and amplitude information, \nmany investigators proceed to estimate the magnitude of a sampled wave-\nform by applying the FFT.lfthe waveform is periodic or deterministic, then \na correct interpretation of FFT results is likely. However, when waveforms \nare random processes, it is necessary to develop a statistical approach to \namplitude estimation. We describe in this section the fundamentals of \npower-\nspectrum estimation, introduce the terminology, and provide FFT proce-\ndures for computing the power spectrum. As is shown, the FFT computa-\ntional procedures are straightforward; however, the statistical interpretation \nof the results is difficult. A detailed development of statistical estimation is \nbeyond the scope of this discussion. \nCorrelation Spectrum Estimation \nLet xCt) be a random function of time. In constrast to a deterministic \nfunction, future values of a random function cannot be predicted exactly. \nHowever, it is possible that the value of the random function at time t1 \ninfluences the value at time t2' We express this statistical characteristic by \nmeans of the autocorrelation function, which is given by \nI\nLl2 \n<1>(1') = 1~ \nIlL \n-L/2 xCt)[x(t + 1')] dt \n(14.30) \nThe power-spectral-density function <I>(f) and the autocorrelation function \n<1>(1') are defined as a Fourier transform pair: \n<1>(1') = I:oo <I>(f)ej271'fT df ~ \n<I>(f) = I:oc <1>(1')e -j271'fT d1' \n(14.31) \nFunction <I>(f) is called by many terms including the power-spectrum, the \npower-density, the spectral-density, and the power-spectral-density func-\ntion. We use these terms interchangeably, as does the literature. Note that \nif we set l' = 0 in Eqs. (14.30) and (14.31), we obtain \nI:= <I>(f) df = <1>(0) = I_oo\"\" x 2(t) dt \n(14.32) \nThe right-hand of Eq. (14.32) is the total energy or power of the random \nfunction (see Sec. 2.4). Because the integral of <I>(f) is equal to the total \nsignal power, then the terminology power, or spectral density, has been \nadopted. \nIf the autocorrelation function is known, then the calculation of the \npower spectrum is determined directly from the Fourier transform. How-\never, the general case is that we must determine <1>(1'). Equation (14.30) is \n366 \nFFT Signal-Processing and System Applications \nChap. 14 \nappealing but the relationship requires a knowledge of \nx(t) for -\n00 < t < 00. \nIn practice, x(t) is known only over a finite interval, and we must estimate \n<!>(-r) based on only this finite duration of data. The estimator for <!>(-r) that \nis generally used is \nA \n1 \n(L-ITI \n<!>(-r) = L _ I \nT I \nJo \nx(t)x[(t + I \nT I \n)] dt \nI \nT 1< L \n(14.33) \nwhere x(t) is assumed to be known only over the finite duration L. \nBecause <!>(-r) is not defined for T > L, then, as shown in Fig. 14.31, \nwe multiply Eq. (14.33) by a window function that is nonzero where Eq. \n(14.33) is defined and is zero elsewhere. Function W(T) is termed a lagged \nwindow because we can visually describe our observation of <!>(T) as looking \nUndefined \nUndefined \n-L \nL \nT \n(a) \nW(T) \n-L \nL \nT \n(b) \n-L \nL \nT \n(c) \nFigure 14.31 Graphical illustration of the window function used in correlation-\nspectrum estimation. \nSec. 14.10 \nFFT Power-Spectru \nm Analysis \n367 \nthrough the window WeT). The modified autocorrelation function W(T)<!>(T) \nexists for all T and hence its Fourier transform exists. We can then obtain \nan estimate of the power spectrum using the relationship of Eq. (14.31): \n(14.34) \nwhere WeT) = 1 \nfor I \nT I \n< L and is zero elsewhere. <l>c(f) is normally defined \nas the correlation or lagged-product estimator for the power spectrum. This \napproach to spectral analysis is commonly referred to in the literature as \nthe Blackman-Tukey procedure [27]. \nPeriodogram Power-Spectrum Estimation \nAn alternate approach to the correlation spectrum procedure is to es-\ntimate the spectrum directly by means of the periodogram. Let \n<l>p(f) = (1IL) 1 LL x(t)e-J27Tfldt 12 \n(14.35) \nSubscript p indicates that the power-spectrum estimate is obtained by means \nof the periodogram. Because Eq. (14.35) is in the form of a Fourier transform \nover a finite interval, we can then use the FFT to compute the spectrum \nestimate. \nAlthough the periodogram and the correlation spectrum-estimation \nprocedures appear quite different, they are theoretically equivalent under \ncertain conditions. It can be shown (Ref. [28]) that \nJ\nLl2 \n<l> (f) = \n(1 - IT IIL)<!>(T)e-J27TfT dT \np \n-L/2 \n(14.36) \nThe inverse Fourier transform of Eq. (14.36) yields \n<!>p(T) = (1 - IT IIL)<!>(T) \nI \nT \nI \n< L \n(14.37) \nHence, if we modify the lagged-product spectrum-estimation technique by \nsimply using a triangular (Bartlett) window rather than a rectangular lag \nwindow, then the two procedures are equivalent. Using the convolution \ntheorem, we can rewrite Eq. (14.36) as \n(14.38) \nwhere WB(f) is the Bartlett frequency-domain window function. Hence, the \nperiodogram spectrum estimate is equal to the lagged-product spectrum es-\ntimate convolved with the Bartlett window frequency function. \nCorrelation spectrum estimation theoretically employs the rectangular \nlag window, and the periodogram spectrum-estimation procedure can be \ninterpreted as employing the triangular lag window. In practice, we employ \nneither of these two windows, as will now be described. \n368 \nFFT Signal-Processing and System Applications \nChap. 14 \nSpectral Windows \nIn the previous discussion, we showed that the correlation and peri-\nodogram estimation procedures can both be interpreted as using frequency-\ndomain window or weighting functions. In estimation problems, one strives \nto achieve an estimator whose mean value (the average of multiple estimates) \nis the parameter being estimated. It can be shown (Ref. [28]) that the mean \nvalue of both the correlation and periodogram estimation procedures is the \ntrue spectrum <p(f) convolved with the frequency-domain window function: \n(14.39) \nHence, the mean value of the power-spectrum estimate equals the true spec-\ntrum only if the frequency-domain window function is an impulse function \n(i.e., the data record length is infinite in duration). If \nthe mean ofthe estimate \nis not equal to the true value, then we say that the estimate is biased. \nFrom our previous discussion of FFT data-weighting functions (Sec. \n9.2), we know that detail is lost if we smooth (convolve) with a broad spectral \n(frequency-domain) window. Said differently, amplitude values adjacent to \na true peak in the spectrum become biased due to the smoothing that occurs \nin the convolution operation with the spectral frequency window function. \nHence, one could conclude that a narrow spectral window is desirable. How-\never, this is not a valid conclusion because the more narrow the spectral \nwindow, the larger the variance of the estimate (Refs. [27] and [28]). This \nstatement follows intuitively because the variance of the estimate of several \nrandom variables that are summed has a smaller variance than that of a \nsingle random variable. Hence, to reduce the variance of the estimate, we \nmust broaden the spectral window that averages or smoothes adjacent es-\ntimates due to the convolution operation of Eq. (14.39). \nThe normal method for characterizing the width of the frequency-do-\nmain window is to define its bandwidth. In spectral analysis, bandwidth is \ndefined as \nBandwidth (BW) = 1/ \n{J:oo W2(f) df} \n(14.40) \nSpectral window bandwidth determines the resolution of the spectrum es-\ntimate as well as the variance of the estimate. A compromise between small \nvariance and high fidelity (resolution) is the crux of the power-spectrum \nestimation problem. We follow the conclusion of Jenkins [28] that any a \npriori optimality criteria that sets too rigid a mathematical formulation for \nthis trade-off is not practical. A more useful and flexible approach is to use \nan experimental spectrum-estimation approach that allows one to learn the \nappropriate bandwidth of the spectral window from the data. After defining \na FFT procedure for computing the power spectrum, we develop such an \nexperimental technique. \nSec. 14.10 \nFFT Power-Spectrum Analysis \n369 \nSmoothed Periodogram FFT Spectrum Estimation \nThe spectral window for the periodogram is of the form {[sin(f)]/f}2. \nThis follows from the developments leading to Eq. (14.38), where it was \nshown that the periodogram spectral estimate was equivalent to a correlation \nestimate using the triangular or Bartlett window. Recall from Sec. 9.2 that \nthe Bartlett frequency window has relatively high side lobes with respect to \nother window functions. However, Jones [29] has shown that very good \nperiodogram spectral estimates can be obtained from the ([sin(f)]1 \nf}2 spectral \nwindow by averaging (smoothing) adjacent spectrum estimates. The \nsmoothed periodogram (sp) estimate is given by \n(14.41) \nwhere W \nD(f) is the rectangular frequency window first suggested by Daniel \n[28]; \n- f3f \n0/2 =5 f =5 f3f \n0/2 \n(14.42) \n= 0 \notherwise \nNote that parameter f3fo specifies the frequency range over which the peri-\nodogram is averaged (fo = lIL). Hence, the smoothed periodogram window \nis that obtained by averaging the appropriate number of {[sin(f)]/f}2 spectral \nwindows that are spaced at intervals of fo = 1IL. We show in Fig. 14.32 \nthe ([sin(f)]/f}2 periodogram spectral window and the smoothed periodogram \nspectral window for f3 = 10. That is, we have averaged 10 adjacent periodo-\ngram windows. A comparison of \nthe smoothed periodogram spectral window \nwith the Hanning and Parzen windows is shown in Fig. 14.33 under the \nconstraint of equal bandwidths. Spectral windows with equal bandwidths, \nW.(I) \n-1 \n1 \nL \nL \n(a) \n-5 \nL \nWo(I)\"W.(I) \no \n(b) \n5 \nL \nf3 = 10 \nFigure 14.32 (a) Periodogram spectral window, and (b) the smoothed periodo-\ngram spectral window for 13 = 10. \n370 \nFFT Signal-Processing and System Applications \nChap. 14 \no \nI \nI \n... ... \nSmoothed Periodogram window \n\\, \n---\n~ \n------\nHanning \n~ \n---\nParzen \n\\c \nI . \nLowpass Bandwidth = 0 5Hz \n, I \n\\ \n,,\\~ ~ \n: \n\\ \n\\ -\n, , \n: \\ I' \n\\.......' \n\" \n/ \nI \n'. \nI , \nI \n, / \nI \n' I' ~-\n, \\ \nI \nI \nV \n' \nI \n, \n, \n, I \nI \n~ \nII \n\\ r ___ \nI \nI \nI \n1-, \\ \nI \nI I \nI \n:, \n~ \n/ \nI \n\" \n\\ \nI \n, \n\" \nI \nI \n, \n, \n\\ \nI \nt \n\\ \nI \nI I \nI \nI \nI / \nI \nI \n:t \nI \n, \nif \nI \nI \nI \nI \n\\ \nI \n, \nI ~ \nI \nI \n\\ \n'/ \n, \n-20 \n'0 \n~ -40 \n~ \n'\" \n0 \n--' \n0 \nN \n-60 \n-80 \n2 \n3 \nFREQUENCY \nFigure 14.33 Comparison of smoothed periodogram, Hanning, and Parzen spec-\ntral windows under the equal-bandwidth constraint. \nas determined from Eq. (14.43), produce a spectrum estimate with equal \nvariances. The bandwidth or resolution of the smoothed periodogram is \ngiven by ~/L. We show in Fig. 14.34 a smoothed periodogram computing \nprocedure using the FFT. As shown, we average the FFT computed esti-\nmates in groups of ~, except that the first group contains only ~/2 terms. \nExperimental Procedure for FFT Spectral Analysis \nA practical procedure for power-spectrum estimation is to progres-\nsively reduce the spectral analysis bandwidth. This approach allows one to \nlearn significant features of the spectrum during the course of the analysis. \nThe initial choice of a wide bandwidth masks fine detail in the spectrum. \nHowever, a wide bandwidth produces a stable (low-variance) estimate. If \nwe allow the analysis bandwidth to become smaller, then additional detail \ncan be explored. The practicality ofthis approach is limited by interpretation \nproblems that result from the instability (large variance) of the estimates. \nTo illustrate the concept of spectral bandwidth closing, we generate \nsamples of a random process (T = 0.1 s) with a power spectrum that for \nthe present we assume is unknown. Our objective is to deduce from the data \nthe true form of the spectrum. \nFFT spectrum estimates according to the procedure of Fig. 14.34 are \ncomputed in Fig. 14.35 for N = 64 and BW = 0.8,0.4, and 0.2 Hz. We note \nSec. 14.10 \nFFT Power-Spectrum Analysis \n1. Sample x(t) for 0 :S t:S L: \nx(kT) = x(t) IkT \nk = 0, 1, ... , N -\n1 \n2. Compute the FFT of x(kT): \nN-1 \nX(nfo) = \n~ \nx(kt)e- j2-rrnkiN \nk=O \nfo = 1/NT \n3. Compute the periodogram of X(nfo): \ncl>p(nfo) = (TI N){Re2[X(nfo) + Im2[X(nfo)]} \n4. Compute the smoothed periodogram: \n1'1/2-1 \ncl>sp(O) = 2/[3 ~ cl>p(nfo) \nn=O \n313/2-1 \ncl>sp([3fo/2) = 1/[3 \n~ cl>p(nfo) \nn=13/2 \n51312-1 \ncl>sp(3[3fo/2) = 1/[3 \n~ cl>p(nfo) \nn=31'1/2 \nFigure 14.34 FFT computational procedure for smoothed periodogram spectrum \nestimation. \n371 \nin Fig. 14.35 that as we close the bandwidth from 0.8 to 0.4 Hz, the estimated \nspectrum contains several spectrum peaks. As we further close the band-\nwidth to 0.2 Hz, these peaks become even more pronounced. Before reach-\ning a conclusion that these peaks are real, it is necessary to establish that \nthe peaks are not the result of variability or instability of our estimate. We \nuse the concept of confidence intervals to make this assessment. \nWe also show in Fig. 14.35 the 90-percent confidence limits (amplitude \nrange) for the estimate produced for each bandwidth selection. Because a \nlog amplitude scale is used, then the confidence interval is valid for any \nfrequency estimate of the power spectrum. The confidence limit, or ampli-\ntude range, is interpreted in that the true power spectrum for any frequency \nfalls within the noted interval with probability 0.9. Hence, the confidence \nlimit is a measure of the statistical variance of the estimate if \nwe assume \nthat there is no bias in the spectral estimate. For wide spectral bandwidth, \nwe know that bias is possible. To determine the confidence limit for each \n372 \nFFT Signal-Processing and System Applications \nChap. 14 \n10.0 r---,------r---,..---,------r---r-----, \n9.01----+----t----+----_+_----,--t----1------1 \nN=64 \nB.O I---+-----+---t---+--'-'-----=--:...--+-----..:---I------I \n7.01----+-----+---t-----+ -- - --\n<I>(f); BW = 0.2 Hz_ \n,-\n--- 4.(f);BW=0.4Hz \n6.0 I---+--~/f-'r-=+~\"\\.---+---+ \n-\n- -\n~(f); BW = O.B Hz -\n5.0 I----+--H\"<--+-V' \nI\\---_t_-----t---+---_t_-----1 \nj' 't \nBW=0.4 \n3.0 r \n\\\\'\" \n1\\ ' \n~ \n~ \\ \n\\. \nBW=OB \n<i 2.01----+-----+--+~\\~~~~--+-----+-t--+-r-----1 \n:;; \n\\ \\ \n' \nc \n\\ \n\\ \n~ \n\\ \n'-\n\\ \n\"1\\ \n~ \n\\ \\i-\\ \\ \n~ 1.0 I---+-----+---+,+-:', \n'---n-\\\",,\\:---+-+--+-+---I \nb 0.91---+-----+---\\j.l---~II+--\n\\+--+-+--+-+---1 \n~ O.BI---+-----+---t----l-l-\\----'I--\n\\--+-+-+-t-----I \n~ 0.71----+-----+---t----~r-~--+_t_-+-t__----1 \n, \n\" \\ \n0.61----+-----+---t-----lt-+__,--'--t-_t_-+-t__----1 \n\\ \n\\ \n0.51----+-----+---t-----+++--++_t_ \n90% CONFIDENCE \n~y~ \\ \nLIMITS \n0.41----+---+---+---f---J\\1-\\+~\\-+1 \n,---+------l \n0.3 f----+---+--+---+---+\\\\-----H\\c----t-----l \n',\\ \n\\ \nh \\ \n0.21-----+---+---+----+-----1-\\\\ \n;+--1\\,---+-------1 \n\\\\ \n\\ \n~I\\~\\ \\ \n0.1 '--__ \n-'-__ \n.....J... ___ \nI--__ \n-'-__ \n.....J......L:...._-'-I--_~ \n0.2 \n0.4 \n0.6 \nO.B \n1.0 \n1.2 \nFREQUENCY (Hz) \nFigure 14.35 Spectrum estimate for N = 64. \nbandwidth, we use the graphs shown in Fig. 14.36. To use the graph, we \nmust compute the parameter 'T] = 2L(BW), where L is the data record length, \nL = NT. Parameter 'T] is referred to as the number of \ndegrees of \nfreedom \nand can be interpreted as the number of squared random variables that have \nbeen summed. Intuitively, we expect the variance of summed random vari-\nables to decrease as the number of variables summed is increased. Hence, \nthe larger the number of degrees of freedom, the smaller the variance of the \nspectrum estimate. \nSec. 14.10 \nFFT Power-Spectrum Analysis \n5.0 \n4.0 \n3.5 \n3.0 \n2.5 \nw 2.0 \nc 1.8 \n~ 1.6 \n~ 14 \n~ 1.2 \n1.0 \n0.8 \n06 \n0.5 \n04 \n\"-\"-\n~ \n~ \n== \nr;::-\n----\nSo \n99% \n95% \n9 \n90% \n95% \n99% \n~ \nf...... \nUPPER CONFIDENCE LIMIT \n- ....-\n- -\n~ \n..... \n---\n;;;;0-\nLOWER CONFIDENCE LIMIT \n10 \n15 \n20 25 30 \n40 50 60 \n80 100 \n150 200 \n300 400 \n600 800 1000 \nn \n-number of degrees of freedom \nFigure 14.36 Plot of confidence limits as a function of the number of degrees of \nfreedom 1]. \n373 \nFor the spectrum estimate determined with a bandwidth of 0.8 Hz, we \ncompute 1] = (2)(0.1)(64)(0.8) = 10.24. From Fig. 14.36, we obtain the values \n2.2 and 0.58 from the upper-limit and lower-limit graphs, respectively. These \nlimits are plotted as a vertical line, as shown in Fig. 14.35. Because the \nconfidence interval is valid for any frequency estimate of the power spec-\ntrum, we slide this vertical line along our estimate to the peak value of 0.4 \nHz in Fig. 14.36. We conclude that the 90-percent confidence amplitude \nrange is so large with respect to the peak variation in our spectrum estimate \nthat we cannot conclude if the results are statistically significant. To ensure \nthat the peak is real, we must reduce the amplitude range of the confidence \ninterval. \nTo improve the confidence of our estimate, we increase the number \nof data points N, which increases 1], the number of degrees of freedom of \nthe estimate. In Fig. 14.37, we show spectrum estimates for N = 512 and \nBW = 0.8, 0.4, and 0.2 Hz. Note that the spectrum peaks that were observed \npreviously are significantly different. This gives evidence that the previously \ndefined peaks were due to statistical instability. Observe in the lower-fre-\nquency region that the spectral estimate decreases in magnitude as the spec-\ntral bandwidth is decreased. This observation leads one to the conclusion \nthat there is estimation bias for the wider spectral windows. This same effect \nis observed in the upper-frequency region (0.8 to 1.2 Hz). In the frequency \nregion of \n0.3 to 0.6 Hz, the opposite effect occurs and the amplitude increases \nas the spectral window bandwidth is reduced. This trend gives evidence that \nthere may be a peak in this region. However, we note that the range of the \n374 \n<~ \nN \n10.0 \n9.0 \n8.0 \n70 \n6.0 \n5.0 \n40 \n3.0 \nW \n2.0 \no \n:::l \n..... \n::::; \na. \n::; \nc( \n::; \n:::l \na: \nt; \n1.0 \n~ 0.9 \n<fl \n0.8 \n0.7 \n06 \n0.5 \n0.4 \n0.3 \n02 \nO. 1 \nFFT Signal-Processing and System Applications \nChap. 14 \nN = 512 \nA \n----- '!'(f);8W=0.2Hz_ \n/-\\ \n~, \n-\n-\n-\n4>(f); BW = 0.4 Hz \n'1-\\-\n.... , \n--- $(f);BW=0.8Hz-\nI \n\\ \\ \nI \n\\ \n-/-1 \n--\\- / \n\\ \n,.// \nFi--, \\\\ \n,\" \n/ \n\"\" \n\\\\ \n_/ \n~\\~ \nBW=02 \ni \\ \n\\ \n. \\\\ \nBW = 0.4 \n\\ \\ \n\\ \\ \nBW=08 \nI \\ 1\\ \nI \n\\ \\ \n\\ \nI \\ \n\\ \n'\\ \n\\ \nI \n\\ \n\\ \\ \n90% CONFIDENCE-\n\\ \nLIMITS \nI \n'. \n\\ \nI \n\\ \n~\\ \\ 1\\ \n\" \nI , \nI \\ \n\\ \n\\ \n\\ \n, \nI \n1...\" \\ \n\\ \nI \\ \n\\ \nI \nI \\ \nI \n\\, \n\\ \n\\ \n\\ \n\\ \n2 \n4 \n6 \n8 \n10 \n12 \nFREQUENCY (Hz) \nFigure 14.37 Spectrum estimate for N = 512. \n90-percent confidence limit is still larger than the excursion of the peak we \nare trying to validate. \nIn Fig. 14.38, we repeat the spectrum estimates for the case N = 2048. \nWe observe a definitive trend toward a peak in the spectrum at approxi-\nmately 0.5 Hz. The spectrum estimate for BW = 0.4 is relatively smooth, \nwhich gives credibility to the estimated peak. The estimate for BW = 0.2 \nHz is questionable because of noticeable variability. Note that for BW = \nSec. 14.10 \n100 \n9.0 \n8.0 \n70 \n60 \n5.0 \n40 \n30 \n<0& \n20 \nN \nW \no \n:J \n>-\n:J \n0.. \n:2 \n< \n~ 10 \n~ 0.9 \n~ 0.8 \nCfJ \n07 \n06 \n0.5 \n04 \n03 \n02 \no \n1 \nFFT Power-Spectrum Analysis \n375 \n,-, \nN = 2048 \nVI \\ \n<1>(11 \nA \n_ \n-.F -\"\"~ \n'-- - --\n<I>{f); BW = 0.2 Hz \nA \n---\n<I>{f); BW=0.4Hz-\n// \n~, \nA \n---\n<I>(f); BW=0.8Hz \n? \n1\"-- __ \n1' \n.... \n,~ \nIs;..--\nI \n',\\ \n\"-\nr~ \n~\\ \n~\\ \nBW=02 \nBW = 0.4 \n: \n\\ \n\\ \nBWt08 \n'-\n\\ \n\\ , \n\\ \n1-. \n\\~ \n\\ \n\\\\ \n\\ \n90% CONFIDENCE -\n, \n\\ \n\\ \nLIMITS \n\\ \\ \n\\ \n~ \\ '\\ \n\"\\ \\ \n1\\ \n, \\ \\ \n1\\ \\ \n\\ \nr \n\\ \n\" \\ \n\\ \n\" \\ \n\\ \n\\ \\ \n2 \n4 \n6 \n8 \n10 \n1.2 \nFREQUENCY (Hz) \nFigure 14.38 Spectrum estimate for N = 2048. \n0.4 and 0.2 Hz, there is essentially no change in the estimate in the range \n0.0 to 0.2 Hz and only a small change in the range 0.3 to 0.6 Hz. We conclude \nthat the spectral estimate (BW = 0.4 Hz) has minimum bias in the lower-\nfrequency range. The same arguments and conclusion can be reached for \nthe upper-frequency region. This estimate also has a reasonably small con-\nfidence interval. Recall that the confidence interval assumes that there is no \nbias in the estimate. Therefore, we cannot conclude that the estimate for \nBW = 0.8 Hz is the best estimate. Based on these observations, we conclude \n376 \nFFT Signal-Processing and System Applications \nChap. 14 \nthat the spectrum estimate for BW = 0.4 Hz yields a true peak in the spec-\ntrum at approximately 0.5 Hz. In Fig. 14.38, we also show the true spectrum. \nWe have estimated reasonably well the true shape of the spectrum. \nSummary \nIn the literature, there are mUltiple methods described for computing \nthe power spectrum. The Blackman-Tukey procedure is the most popular, \nbut there is no particular reason other than tradition for its use. Spectrum-\nestimation procedures based on the periodogram yield results that are as \ngood or better than other methods and is computationally more efficient. \nAs discussed, the application of the FFT to spectrum analysis is complex \nand is primarily a statistical estimation problem. As long as we can continue \nto increase the data record length, then estimates with reduced variability \ncan be obtained. However the practical problem normally encountered is \none of insufficient data and the power-spectrum analysis problem quickly \nenters the realm of art-science. With the FFT, it is quite easy to produce \nspectrum estimates and for this reason the reader is cautioned to use this \nsection as only an introduction to spectrum estimation. The interpretation \nof these FFT results is the key to the power-spectrum estimation problem. \nReaders should beware that there is considerable discussion in the literature \nconcerning the selection of optimal window functions. In most practical \nspectrum-analysis applications, window selection is of minor importance \ncompared to the problem of spectrum interpretation. The literature also de-\nscribes the use of data-weighting functions in the periodogram computation \nof the power spectrum. This approach is used to reduce the side lobes of \nthe Bartlett spectral window that, as discussed, is inherent in the compu-\ntation of the periodogram. From a statistical viewpoint, this approach is not \nsound unless the random background noise is of minor importance to the \ndeterministic signal being evaluated. \nWelch [32] describes a spectrum-analysis procedure where the data is \nsectioned into subintervals and a periodogram is computed for each section \nof data. These periodograms are then averaged to improve the variability \nof the spectrum estimate for each frequency. This computation approach is \nlimited by the leakage properties of the Bartlett spectral window and data \nwindows are generally used to improve the window characteristics. Addi-\ntional spectrum analysis applications of the FFT are given in Refs. [30], [31], \nand [33]. \n14.11 FFT BEAMFORMING \nThe conventional delay-and-sum technique for array beamforming in radar, \ncommunications, sonar, and seismic applications is illustrated in Fig. 14.39. \nSec. 14.11 \nFFT Beamforming \n377 \nM-l \nv(t) 0 L Xm(t-mT) \nm=Q \nFigure 14.39 Conventional delay-and-sum technique for array beamforming. \nAs shown, a plane wavefront arriving at an angle 6 to a sensor array with \nspacing d between elements is delayed by an amount 'T between each adjacent \nsensor pair. If we are to recombine, in the proper phase, the output of each \nsensor, then we must compensate for these time delays. The relationship \nbetween the sensor spacing d and the delay 'T is given by \n'T = (die) cos(6) \n(14.43) \nwhere e is the velocity of propagation of the wavefront. To recombine the \nsensor outputs for an incoming wavefront at an angle 6, we must delay the \noutput of sensor m by \nm'T = (mdle) cos(6) \n(14.44) \nwhere the wavefront angle 6 and the sensor m are defined in Fig. 14.39. \nSignal recombination, or spatial array beamforming, is then achieved by the \ncoherent (in-phase) addition of the delayed sensor outputs: \nM-\\ \ny(t) = ~ \nxm(t -\nm'T) \n(14.45) \nm=O \nwhere we have assumed M sensors. \nBeamforming by means of delay lines becomes quite cumbersome from \na hardware viewpoint as the number of array sensors increase. If digital \ndelay lines are used, the sensor outputs must be sampled at a rate much \nhigher than the Nyquist rate to minimize side-lobe degradation in narrow-\nband linear-array beam patterns. However, with the FFT, it is possible to \n378 \nFFT Signal-Processing and System Applications \nChap. 14 \nperform the equivalent of a time delay in the frequency domain. We develop \nin this section the procedures for applying the FFT to spatial-array beam-\nforming. Our approach follows that of Refs. [19] and [34]. \nFrequency-Domain Single-Beam Relationships \nRecall from the time-shifting property (Sec. 3.5) that a shift in time by \nan amount T is equivalent to mUltiplication by e -j2-rrfT in the frequency do-\nmain. Equation (14.45) can then be written as \nM-l \nM-l \ny(t) = L xm(t -\nmT) ~ \nY(f) = L Xm(f)e -j2-rrfmT \n(14.46) \nm=O \nm=O \nThe term on the right-hand side of Eq. (14.46) is the appropriate fre-\nquency-domain relationship for combining the M sensor outputs with the \nappropriate delays. Hence, we take the Fourier transform of the output for \neach sensor, multiply by the complex exponential e-j2-rrfmT, and add the \nresults according to the right-hand side of Eq. (14.46). The inverse Fourier \ntransform yields y(t). \nNote that Eq. (14.46) is valid only for one specific value of the param-\neter T, that is, the array has been pointed in the direction 6 defined in Eq. \n(14.43). \nFrequency-Domain Multiple-Beam Relationships \nLet us assume that we desire to simultaneously implement the correct \ndelays in order to combine the M sensor outputs for various azimuth angles \n6;. In particular, we must compute the frequency-domain summation of Eq. \n(14.46) for each value T; associated with a beam direction 6;. To form M \nazimuth beams, we define increments of T; as \nT; = i(d/M) \ni = 0, 1, ... , M -\n1 \n(14.47) \nwhere d is the distance between sensors and M is the number of sensors. \nThe beam, or azimuth, angle 6; for each delay value T; can be determined \nfrom Eq. (14.43): \n6; = cOS-1(CT;/d) = cos-1(iC/M) \nThe right-hand side of Eq. (14.46) then becomes \nM-l \nY;(f) = L X m(f)e-j2-rrf(m;dIM) \nm=O \n(14.48) \n(14.49) \nEquation (14.49) requires that we compute the Fourier transforms of \neach sensor output, multiply the transform for each sensor by the exponential \ne-j2-rrf(m;dIM) for each desired beam direction 6;, and then perform the in-\nChap. 14 \nProblems \n379 \ndicated summation. The inverse Fourier transform yields the time-domain \nfunction associated with a beam direction 6i . The computation of \nXm(f) and \nYi(f) are easily formulated in terms of a two-dimensional FFT. \nTwo-Dimensional FFT Array Processing \nAssume that we sample each sensor output xm(t) with sample interval \nT to form xm(kt), where k = 0, 1, ... , N -\n1. Then we use the FFT to \ncompute the frequency function for each sensor m: \nN-I \nXm(nfo) = L xm(kt)e -j2-rr(kD(nfo) \nk=O \nn =0,1, .. . ,N-l \n(14.50) \nfo = liNT \nIf we replace the continuous variable f in Eq. (14.49) with the discrete \nfrequencies nfo obtained by substituting Eq. (14.50), then Eq. (14.49) \nbecomes \nY(i,nfo) \nM-I{N-I \n} \nL \nL x m(kt)e- j2-rr(kD(nfo) \ne-j2-rrf (midIM) \nm=O \nk=O \n(14.51) \nThe two-dimensional FFT relationship of Eq. (14.51) is a function of the \nbeam number i and frequency nf \no. The two-dimensional inverse FFT yields \nthe appropriate time-domain waveform associated with each beam pointed \nin the direction 6i â¢ If \nthe number of sensors is large, then digital beamforming \nusing the FFT is attractive. \nSummary \nThe preceding derivation has been for a linear array of M elements. \nOur approach can be extended to cases of circular, cylindrical, and spherical \narrays. Beam-pattern side lobes can be minimized by using a weighting func-\ntion. It \nis also possible to design adaptive methods in which each array output \nis weighted from a calculated expression based on actual received data. \nPROBLEMS \n14.1 Consider the band-pass waveforms illustrated in Fig. 14.40. Analogous to Fig. \n14.3, graphically and analytically develop the range of acceptable sampling \nfrequencies that produce nonoverlapped aliased images of the band-pass spec-\ntrum. What do you conclude concerning spectrum inversion? \n14.2 Assume a band-pass waveform with a frequency spectrum as shown in Fig. \n14.41. Show graphically a sampling frequency that results in down sampling \n380 \n/\\ \nI \n\\ \n-710 \nFFT Signal-Processing and System Applications \nChap. 14 \nr, \nI \n, \nI \n, \nI \n, \nI \n, \n-2Oto \n-1510 \n-lOto \n-510 \n1 \n\" \n, I \nI \n, \n, \n-1510 -1110 -810 -510 \nH(I) \n510 \nlOto \n1510 \n2Oto \n(a) \nH(I) \n510 \n810 1110 \n1510 \n(b) \nFigure 14.40 Band-pass waveforms for Prob. 14.1. \nH(t) \n8T 0 210 \n710 \nFigure 14.41 Band-pass waveform for \nProb. 14.2. \nto a center frequency of fo. Are there additional sampling frequencies that \nproduce the same result? \n14.3 For the example frequency function shown in Fig. 14.3(c), choose a sampling \nfrequency that down samples the spectrum to a zero center frequency. Show \ngraphically your results. What are your conclusions? Hint: Consider double \nside-band amplitude modulation. \n14.4 Assume that the frequency function shown in Fig. 14.42 results from a single \nside-band modulation of a voice signal that inverts the voice spectrum as \nshown. Develop graphically a down-sampling procedure that simultaneously \ndemodulates the signal and inverts the spectrum. Your results should be iden-\ntical to those of Fig. 13. 14(e). \n-15kHz \nl IH(I)' \n1 I \nBT 0 4kHz \n15kHz \nFigure 14.42 Frequency function for \nProb. 14.4. \n14.5 Repeat the graphical and analytical developments of Ex. 14.2 if \nh(t) is given \nas \nh(t) \ncos[2'lT(5fo + f o/2)1] -\nY2 sin[2'lT(5fo + f o/2)1] \nChap. 14 \nProblems \n381 \n14.6 Assume a bandpass signal has a center frequency of 16fo and a bandwidth BT \n= 3fo. If quadrature sampling is used, what is the minimum sample rate for \nthe in-phase and quadrature channels? If the signal is to be reconstructed with \na center frequency of 5fo, determine the sample interpolation requirements. \n14.7 Assume that the frequency functions illustrated in Fig. 14.43 are narrow-band \nsignals to be sampled by quadrature-sampling techniques. Use a graphical \nanalysis procedure analogous to Figs. 14.4 and 14.5 to develop the Fourier \ntransform of the in-phase and quadrature functions resulting from quadrature \nsampling. Discuss for each case the required sampling rate to prevent aliasing. \nShow by a graphical analysis analogous to Figs. 14.7 and 14.8 that no infor-\nmation is lost even though quadrature sampling results in overlapped fre-\nquency functions. Also comment on the increase in sample rate that is required \nfor each case if \nthe sampled waveforms are recombined at a center frequency \nof 2fo. \n, \n/I' \nI \\ \n, \\ \nJ \n\\ \n,... \n\\ \n-5'0 \n-3'0 \n-fa \n(a) \nH(I)- REAL \n,1 \n, I \n, \nI \n/ \nI \n, \nI \n10 \n310 \n510 \nI \n-710 \n-510 \n-310 \n-10 \n-4'0 -3'0' ,,2to .fo/ \n, \nI \n\" \n(e) \nH(I) - IMAGINARY \nfo \n2to 3'0 4'0 \n-, \nH(I)- REAL \n10 \n310 \n510 \n(b) \nFigure 14.43 Frequency functions for Prob. 14.7. \n710 I \n14.8 Assume that extremely fine resolution is desired in the narrow frequency band \nB z shown in Fig. 14.44. Also, assume that the desired resolution cannot be \nachieved across the total signal bandwidth due to computer memory limita-\ntions. Use the concept of frequency translation followed by low-pass filtering \nto develop a procedure for increased FFT resolution (Zoom FFT, Ref. [24]). \nFigure 14.44 Frequency function for \nProb. 14.8. \n14.9 Assume that a single frequency sinusoid is buried in noise with a SIN = -20 \ndB. If \na SIN = + \n10 dB is required to clearly establish the presence of the \nsignal, determine the appropriate FFT parameters. \n382 \nFFT Signal-Processing and System Applications \nChap. 14 \n14.10 A narrow-band signal of bandwidth BWs is buried in wide-band noise of band-\nwidth BWn. Determine the maximum processing gain which can be achieved \nby FFT signal processing. \n14.11 Assume that hardware limits FFT processing to a size N = 512. If a single \nfrequency sinusoid is buried in noise with a SIN = -35 dB, determine the \nnumber of successive FFTs which must be added to yield a processed signal-\nto-noise ratio greater than + \n6 dB. \n14.12 Explain how you would process the signal of Prob. 14.11 if SIN = -55 dB \nand the frequency of the sinusoid is known. \n14.13 Use the FFT to implement the block diagram matched filter processor shown \nin Fig. 14.15. Experiment with different waveforms and compute the matched \nfilter output. If your waveforms are to be used as a radar, discuss each from \nthe perspective of range resolution and probability of detection. \n14.14 Let a received signal sr(t) be given by \nsr(t) = s(t) + a s(t + Tl) \nwhere \ns(t) = cos(27rfot) \na = -0.9 \nTl = 0.75 \nfo = 1 Hz \nOur objective is to use cepstrum processing to remove the echo a[s(t + Tl)]. \n(a) Compute the cepstrum of s(t) using the FFT. \n(b) Compute the cepstrum of sr(t) using the FFT. \n(c) Implement the block diagram of Fig. 14.17 and compare your results with \nthe waveform s(t) computed in part (a). \n14.15 In deconvolution, the inverse filter impulse response is r(t), as determined \nfrom Eq. (14.13). What is the theoretical result if r(t) and h(t) are convolved? \nWhat is the result if a Hanning weighting function is employed according to \nEq. (14.16)? \n14.16 Theoretically, deconvolution can be accomplished with no error. Describe \nseveral practical limitations to obtaining theoretical results. \n14.17 Develop an approach to design a deconvolution filter for the case where H(f) \nis zero-valued, for example, H(f) = [sin(f)]lf. Hint: Use the Hanning weight-\ning function, where the truncation frequency fe is the first zero value of H(f). \nUse a second Hanning weighting function between the first and second zero \nvalues of H(f). Repeat as necessary to achieve signal restoration. \n14.18 Repeat Ex. 14.3 for the cases a = 4>.. and a = >../2. What do you conclude \nconcerning the relationship between parameters a and >... \n14.19 In Fig. 14.24(b), what is the effect of increasing the number of zero-valued \nsamples? \n14.20 Using the procedures developed in Sec. 14.6, compute and plot the far-field \nantenna pattern for each of the electric field aperture distributions shown in \nFig. 14.45. \nChap. 14 \nProblems \n383 \nE(x) \nE(x) \nE(x) \nRAISED \nCOSINE \nFUNCTION \nÂ·a \n2\" \na \n2\" \nx \n-a \n-3 \n\"2 \na \na \n2\" \n(a) \n(b) \n(e) \nFigure 14.45 Electric field aperture distributions for Prob. 14.20. \n14.21 Refer to Eq. (14.27), which determines the angle of arrival of a waveform. \nFor a fixed d, what is the effect of setting A = Ao, 2Ao, 4Ao, ... ? What do \nyou conclude concerning the relationship between parameters d and A? \n14.22 Practical implementation of Eq. (14.27) involves measurement of the param-\neter <l> in the presence of noise. If \na measurement is made in the presence of \nnoise, what is the effect of: \n(a) increasing d for a fixed A \n(b) decreasing A \nfor a fixed d \nWhat do you conclude is the optimum relationship between the parameters d \nand A \nif noise is considered? \n14.23 Assume that an FFT phase-interferometer direction-finding system is to per-\nform measurements over the wavelength range Ao to IOAo in the presence of \nnoise. In view of Probs. 14.21 and 14.22, propose a system solution which \nwill insure accurate phase difference measurement over the wavelength range. \n(Hint: Consider multiple antennas.) \n14.24 Modify Fig. 14.25 so that only FFT cells with a signal-to-noise ratio that ex-\nceeds a preset threshold enter into the phase difference computation. \n14.25 In an FFT time-difference-of-arrival system, how does one determine which \nsignal, S \nI (t) or S2(t), is the first to arrive? \n14.26 Prove that the phase slope of the cross-spectrum (Fig. 14.26) is equal to the \ntime-difference-of-arrival mUltiplied by 2'1T. \n14.27 A weighted least-squares procedure to estimate the phase slope in Fig. 14.26 \nhas been suggested. On what parameters should the weights be based? \n14.28 Propose an FFT simulation to evaluate the performance of a radar system \nemploying a specially designed transmitted waveform and a matched filter \nsignal processor. How does your simulation change if the radar is operating \nin the presence of a known interference Gamming)? \n14.29 Let N = 512, BW = 0.1,0.3, and 0.9 Hz. If \nT = 0.1 secs, determine the 90% \nand 95% confidence limits for each case. \n14.30 Compute and graph on a log scale the smoothed periodogram spectral window \nfor ~ = 5, 10, 20, and 50. Show that the windows are roughly rectangular in \nshape and that the sidelobes falloff at 6 dB per octave. Observe that the initial \nfall-off of the sidelobes is a function of ~. \n384 \nFFT Signal-Processing and System Applications \nChap. 14 \nREFERENCES \n1. BALANIS, A. C. Antenna Theory, Analysis, and Design. New York: Harper & \nRow, 1982. \n2. KEMERAIT, R. C., AND D. G. CHILDERS. \"Signal Detection and Extraction by \nCepstrum Techniques.\" IEEE Trans. Info. Theory (November 1972), Vol. IT-\n18, No.6, pp. 745-759. \n3. BOGERT, B. P., M. J. HEALY, AND J. W. TUKEY. \"The Quefrequency Analysis \nof Time Series for Echos; Cepstrum, Pseudo Autocovariance, Cross Cepstrum \nand Saphe Cracking.\" In M. Rosenblatt, ed., Time Series Symposium, pp. 201-\n243. New York: Wiley, 1963. \n4. BROSTE, N. A. \"Digital Generation of Random Sequences.\" IEEE Trans. Auto. \nCont. (April 1971), Vol. AC-16, No.2, pp. 213-214. \n5. BUCCI, O. M., AND D. M. GUISEPPE. \"Exact Sampling Approach for Reflector \nAntenna Analysis.\" IEEE Trans. Ant. Prop. (November 1984), Vol. AP-32, No. \nll, pp. 1259-1262. \n6. CARSON, C. T. \"The Numerical Solution of \nWaveguide Problems by Fast Fourier \nTransform.\" IEEE Trans. Micro. Theory Tech. (November 1968), Vol. 16, No. \n11, pp. 955-958. \n7. CHRISTODOULOU, C. G., AND J. F. KAUFFMAN. \"On the Electromagnetic Scat-\ntering from Infinite Rectangular Grids with Finite Conductivity.\" IEEE Trans. \nAnt. Prop. (February 1986), Vol. AP-34, No.2, pp. 144-154. \n8. CROCHIERE, R. E., AND L. R. RABINER. Multirate Digital Signal Processing. En-\nglewood Cliffs, NJ: Prentice-Hall, 1983. \n9. HELSTROM, C. W. Statistical Theory of \nSignal Detection. New York: Pergamon, \n1960. \n10. HUNT, B. R. \"Application of Constrained Least Squares Estimation to Image \nRestoration By Digital Computers.\" IEEE Trans. Comput. (September 1973), \nVol. C-22, No.9, pp. 805-812. \n11. JERRI, A. J., \"The Shannon Sampling Theorem-Its Various Extensions and \nApplications: A Tutorial Review.\" Proc. IEEE (November 1977), Vol. 65, No. \n11, pp. 1565-1569. \n12. JONES, W. R. \"Precision FFT Correlation Techniques for Nondeterministic \nWaveforms.\" IEEE EASCON Cony. Rec. (October 1974), pp. 375-380. \n13. KRAUS, J. D. Antennas. New York: McGraw-Hill, 1950. \n14. LAM, P. T., S. LEE, C. C. HUNG, AND R. ACOSTA. \"Stategy for Reflector Pattern \nCalculation: Let the Computer Do the Work.\" IEEE Trans. Ant. Prop. (April \n1986), Vol. AP-34, No.4, pp. 592-595. \n15. LINDEN, D. A. \"A Discussion of Sampling Theorems.\" Proc. IRE (July 1959), \nVol. 47, No.7, pp. 1219-1226. \n16. NAGAI, K. \"Measurement of Time Delay Using the Time Shift Property of the \nDiscrete Fourier Transform (DFT).\" IEEE Trans. Acoust. Speech Sig. Proc. \n(August 1986), Vol. ASSP-34, No.4, pp. 1006-1008. \n17. OPPENHEIM, A. V. Application of \nDigital Signal Processing. Englewood Cliffs, \nNJ: Prentice-Hall, 1978. \nChap. 14 \nReferences \n385 \n18. RABINER, L. R., AND B. GOLD. Theory and Application of \nDigital Signal Pro-\ncessing. Englewood Cliffs, NJ: Prentice-Hall, 1975. \n19. RUDNICK, P. \"Digital Bearnforming in the Frequency Domain.\" J. Acoust. Soc. \nAmerica (November 1969), Vol. 46, No.5, pp. 1089-1090. \n20. SARKAR, T. K., E. ARVAS, AND S. M. RAo. \"Application of FFT and the Con-\njugate Gradient Method for the Solution of Electromagnetic Radiation from Elec-\ntrically Large and Small Conducting Bodies.\" IEEE Trans. Ant. Prop. (May \n1986), Vol. AP-34, No.5, pp. 635-640. \n21. SCHWARTZ, MISCHA, AND L. SHAW. Signal Processing. New York: McGraw-Hill, \n1975. \n22. SILVERMAN, H. F., AND A. E. PEARSON. \"On Deconvolution Using the Discrete \nFourier Transform.\" IEEE Trans. Audio and Electroacoust. (April 1973), Vol. \nAU-21, No.2, pp. 112-118. \n23. WILLIAMS, J. R., AND G. G. RICKER. \"Signal Detectability Performance of Op-\ntimum Fourier Receivers.\" IEEE Trans. Audio and Electroacoust. (October \n1972), Vol. AU-20, No.4, pp. 264-270. \n24. YIP, P. C. Y. \"Some Aspects of the Zoom Transform.\" IEEE Trans. Comput. \n(March 1976), Vol. C-25, No.3, pp. 287-296. \n25. McDoUGAL, J. R., L. C. SURRATT, AND J. F. STOOPS. \"Computer Aided Design \nof \nSmall Superdirective Antennas Using Fourier Integral and Fast Fourier Trans-\nform Techniques.\" SWIEECO Rec. (1970), pp. 421-425. \n26. BRAULT, J. W., AND O. R. WHITE. \"The Analysis and Restoration of Astronom-\nical Data via the Fast Fourier Transform.\" Astronomy and Astrophysics (July \n1971), Vol. 13, No.2, pp. 169-189. \n27. BLACKMAN, R. B., ANDJ. W. TUKEY. Measurement of \nPower Spectra. New York: \nDover, 1959. \n28. JENKINS, G. M., AND D. G. WATTS. Spectral Analysis and Its Applications. San \nFrancisco: Holden Day, 1968. \n29. JONES, R. H. \"A Reappraisal of the Periodogram in Spectral Analysis.\" Tech-\nnometrics (November 1965), Vol. 7, No.4, pp. 531-542. \n30. BINGHAM, C., M. D. GODFREY, ANDJ. W. TUKEY. \"Modem Techniques of \nPower \nSpectrum Estimation.\" IEEE Trans. Audio Electroacoust. (June 1967), Vol. AU-\n15, No.2, pp. 55-66. \n31. HINICH, M. J., AND C. S. CLAY. \"The Application of the Discrete Fourier Trans-\nform in the Estimation of Power Spectra, Coherence, and Bispectra of Geo-\nphysical Data.\" Rev. Geophysics (August 1968), Vol. 6, No. c, pp. 347-362. \n32. WELCH, P. D. \"The Use of Fast Fourier Transform for the Estimation of Power \nSpectrum: A Method Based on Time Averaging Over Short, Modified Periodo-\ngrams.\" IEEE Trans. Audio Electroacoust. (June 1967), Vol. AU-15, No.2, pp. \n70-74. \n33. CHILDERS, D. G., (ed.). Modern Spectrum Analysis. New York: IEEE Press, \n1978. \n34. WILLIAMS, J. R. \"Fast Beam-Forming Algorithm.\" J. Acoust. Soc. America \n(1968), Vol. 44, No.5, pp. 1454-1455. \nA \nTHE IMPULSE FUNCTION: \nA DISTRIBUTION \nThe impulse function 8(t) is a very important mathematical tool in continuous \nand discrete Fourier transform analysis. Its usage simplifies many deriva-\ntions that would otherwise require lengthy, complicated arguments. Even \nthough the concept of \nthe impulse function is correctly applied in the solution \nof many problems, the basis or definition of the impulse is normally mathe-\nmatically meaningless. To ensure that the impulse function is well-defined, \nwe must interpret the impulse not as a normal function but as a concept in \nthe theory of distributions. \nFollowing the discussions by Papoulis [1, Appendix I] and Gupta [2, \nChapter 2], we describe a simple but adequate theory of distributions. Based \non this general theory, we develop those specific properties of the impulse \nfunction that are necessary to support the developments of Chapter 2. \nIMPULSE FUNCTION DEFINITIONS \nNormally, the impulse function (8-function) is defined as \n8(t -\nto) = 0 \nJ:\"\" 8(t -\nto) dt = 1 \nt Â¥ to \n(A. I) \n(A.2) \nThat is, we define the 8-function as having undefined magnitude at the time \nof occurrence and zero elsewhere, with the additional property that the area \nunder the function is unity. Obviously, it is very difficult to relate an impulse \n386 \nImpulse Function Definitions \n387 \nto a physical signal. However, we can think of an impulse as a pulse wave-\nform of very large magnitude and infinitely small duration such that the area \nof the pulse is unity. \nWe note that with this interpretation, we are, in fact, constructing a \n-8 \nfit) \n-a \na \n\"2 \n2 \n1 \n2a \nla) \nfit) \nIb) \nfit) \nIe) \nfit) \nd \nT \n! \na \nFigure A.I Representations of the 8-function. \n388 \nThe Impulse Function: A Distribution \nApp. A \nseries of functions (i.e., pulses) that progressively increase in amplitude, \ndecrease in duration, and have a constant area of unity. This is simply an \nalternate method for defining a 8-function. Consider the pulse waveform \nillustrated in Fig. A.l(a). Note that the area is unity and, hence, we can \nwrite mathematically the 8-function as \n8(t) = lim f(t, a) \n(A.3) \n...... \n0 \nIn the same manner, the functions illustrated in Figs. A.l(b) to (d) satisfy \nEqs. (A. I) and (A.2) and can be used to represent an impulse function. \nThe various properties of impulse functions can be determined directly \nfrom these definitions. However, in a strict mathematical sense, these defi-\nnitions are meaningless if we view 8(t) as an ordinary function. If \nthe impulse \nfunction is introduced as a generalized function or distribution, then these \nmathematical problems are eliminated. \nDISTRIBUTION CONCEPTS \nThe theory of distributions is vague and, in general, meaningless to the \napplied scientist who is reluctant to accept the description of a physical \nquantity by a concept that is not an ordinary function. However, we can \nargue that the reliance on representation of physical quantities by ordinary \nfunctions is only a useful idealization and, in fact, is subject to question. To \nbe specific, let us consider the example illustrated in Fig. A.2. \nAs shown, the physical quantity V is a voltage source. We normally \nassume that the voltage v(t) is a well-defined function of time and that a \nmeasurement merely reveals its values. But we know in fact that there does \nnot exist a voltmeter that can measure exactly v(t). However, we still insist \non defining the physical quantity V by a well-defined function v(t) even \nthough we cannot measure v(t) accurately. The point is that because we \ncannot measure the quantity V exactly, then on what basis do we require \nthe voltage source to be represented by a well-defined function v(t)? \nA more meaningful interpretation of \nthe physical quantity V is to define \nit in terms of the effects it produces. To illustrate this interpretation, note \nthat in the previous example, the quantity V causes the voltmeter to display \n~ \nI \nVOLTAGE \nV(outpUtl \nVOLT-METER \nI \n1 \n+ 11Â·14121 \nSOURCE \nv(tl \nI \nI \nDISPLAY \nI \nI \nCAUSE \nTESTING FUNCTION \nRESPONSE \nFigure A.2 Physical interpretation of a distribution. \nDistribution Concepts \n389 \nor assign a number as a response. For each change in V, another number \nis displayed or assigned as a response. We never measure v(t) but only the \nresponse; therefore, the source can be specified only by the totality of the \nresponses that it causes. It is conceivable that there is not an ordinary func-\ntion v(t) that represents the voltage parameter V. But because the responses \nor numbers are still valid, then we must assume that there is a source V \ncausing them and the only way to characterize the source is by the responses \nor numbers. We now show that these numbers in fact describe V as a \ndistribution. \nA distribution, or generalized function, is a process of assigning to an \narbitrary function <!>(t) a response or number \nR[<!>(t)] \n(A.4) \nFunction <!>(t) is termed a testing function and is continuous, is zero outside \na finite interval, and has continuous derivatives of all orders. The number \nassigned to the testing function <!>(t) by the distribution g(t) is given by \nJ:= g(t)<!>(t) dt = R[<!>(t)] \n(A.5) \nThe left-hand side of Eq. (A.5) has no meaning in the conventional sense of \nintegration, but rather is defined by the number R[<!>(t)] assigned by the \ndistribution g(t). Let us now cast these mathematical statements in light of \nthe previous example. \nWith reference to Fig. A.2, we note that if the voltmeter is modeled \nas a linear system, then the output at time to is given by the convolution \nintegral \nJ:= v(t)h(to -\nt) dt \nwhere h(t) is the time-domain response of the measuring instrument. If \nwe \nconsider h(t) as a testing function (that is, each particular voltmeter has \ndifferent internal characteristics and as a result yields a different response \nfor the same input, we thus say that the meter tests or senses the distribution \nv(t), then the convolution integral takes the form \nJ:= v(t)<!>(t,to) dt = R[<!>(t,to)] \n(A.6) \nThus, for a fixed input V, the response R is a number depending on the \nsystem function <!>(t,to). \nIf \nwe interpret Eq. (A.6) as a conventional integral and if this integral \nequation is well-defined, then we say that the voltage source is defined by \nthe ordinary function v(t). But, as stated previously, it is possible that there \ndoes not exist an ordinary function satisfying Eq. (A.6). Because the re-\nsponse R[<!>(t,to)] still exists, we must assume that there is a voltage source \n390 \nThe Impulse Function: A Distribution \nApp. A \nV that causes this response and that a method of characterizing the source \nis by means of the distribution of Eq. (A.6). \nThe preceding discussion casts the theory of distributions in the form \nof physical measurements for ease of interpretation. Based on the defining \nrelationship of Eq. (A.5), we now investigate the properties of a particular \ndistribution: the 8-function. \nPROPERTIES OF IMPULSE FUNCTIONS \nThe impulse function 8(t) is a distribution assigning to the testing function \n<\\>(t) the number <\\>(0): \nJ:oo 8(t)<\\>(t) dt = <\\>(0) \n(A.7) \nIt should be repeated that the relationship of Eq. (A.7) has no meaning as \nan integral, but the integral and the function 8(t) are defined by the number \n<\\>(0) assigned to the function <\\>(t). \nWe now describe the useful properties of impulse functions. \nSifting Property \nThe function 8(t -\nto) is defined by \nJ:oo 8(t -\nto)<\\>(t) dt = <\\>(to) \n(A.8) \nThis property implies that the 8-function takes on the value of the function \n<\\>(t) at the time the 8-function is applied. The term sifting property arises \nin that if we let to continuously vary, we can sift out each value of \nthe function \n<\\>(t). This is the most important property of the 8-function. \nScaling Property \nThe distribution 8(at) is defined by \nJ:oo 8(at)<\\>(t) dt = I! I \nJ:oo 8(t)<\\>(~) dt \n(A.9) \nwhere the equality results from a change in the independent variable. Thus, \n8(at) is given by \n1 \n8(at) = r;l8(t) \n(A. to) \nProperties of Impulse Functions \n391 \nProduct of a 8-function by an Ordinary Function \nThe product of a 8-function by an ordinary function h(t) is defined by \nJ:\", [8(t)h(t)]<!>(t) dt = J:\", 8(t)[h(t)<!>(t)] dt \n(A. II) \nIf h(t) is continuous at t = to, then \n8(to)h(t) = h(to)8(to) \n(A.I2) \nIn general, the product of two distributions is undefined. \nConvolution Property \nThe convolution of two impulse functions is given by \nJ:\", [J:\", 8\\ \n(T)8 2(t -\nT) dT ] <!>(t) dt \n= J:\",8\\(T) [J:\",82(t -\nT)<!>(t)dt] dT \n(A. 13) \nHence, \n(A.14) \n8-functions as Generalized Limits \nConsider the sequence gn(t) of distributions. If there exists a distri-\nbution g(t) such that for every test function <!>(t), we have \n~ \nJ:\", gn(t)<!>(t) dt = J:\", g(t)<!>(t) dt \nthen we say that g(t) is the limit of gn(t) \ng(t) = lim g n(t) \nn-+'\" \n(A.I5) \n(A.16) \nWe can also define a distribution as a generalized limit of a sequence \nf n(t) of ordinary functions. Assume that f n(t) is such that the limit \n~ \nJ:\", fn(t)<!>(t) dt \nexists for every test function. This limit then is a number that depends on \n<!>(t) and thus defines a distribution g(t), where \ng(t) = lim f n(t) \n(A.17) \nn-+'\" \n392 \nThe Impulse Function: A Distribution \nApp. A \nand the limiting operation is to be interpreted in the sense of Eq. (A.I5). If \nEq. (A.I7) exists as an ordinary limit, then it defines an equivalent function \nif we assume that we can interchange the order of limit and integration in \nEq. (A.I5). It is from these arguments that the conventional limiting argu-\nments, although awkward, are mathematically correct. \nThe 8-function can then be defined as a generalized limit of a sequence \nof ordinary functions satisfying \n(A.I8) \nIf Eq. (A.I8) holds, then \n8(t) = lim fn(t) \n(A. 19) \nEach of these functions illustrated in Fig. A.I satisfy Eq. (A.I8) and define \nthe 8-function in the sense of Eq. (A.I9). \nAnother functional form of importance that defines the 8-function is \nl:>( ) \nl' \nsin at \nu t \n= Im--\na-+oo \n'ITt \nUsing Eq. (A.20), we can prove [Papoulis] that \n(A.20) \n(A.21) \nwhich is of considerable importance in evaluating particular Fourier \ntransforms. \nTWO-DIMENSIONAL IMPULSE FUNCTIONS \nThe two-dimensional impulse function 8(x,y) is a distribution assigning to \nthe testing function <l>(x,y) the number <l>(0,0): \n(A.22) \nFrom this definition, the useful properties of two-dimensional impulse func-\ntions can be derived. In particular, the shifting property, which is key to \ndeveloping the two-dimensional sampling theorem, is as follows: \n(A.23) \nApp.A \nReferences \n393 \nREFERENCES \n1. PAPOULIS, A. The Fourier Integral and Its Applications, 2d ed. New York: \nMcGraw-Hill, 1984. \n2. GUPTA, s. C. Transform and State Variable Methods in Linear Systems. New \nYork: Wiley, 1966. \n3. BRACEWELL, R. M. The Fourier Transform and Its Applications, 2d rev. ed. New \nYork: McGraw-Hill, 1986. \n4. LIGHTHILL, M. J. An Introduction to Fourier Analysis and Generalized Function. \nNew York: Cambridge University Press, 1959. \n5. ARSAC, J. Fourier Transforms and the Theory of \nDistributions. Englewood Cliffs, \nNJ: Prentice-Hall, 1%6. \n6. ZEMANIAN, A. H. Distribution Theory and Transform Analysis. New York: \nMcGraw-Hill, 1965. \nBIBLIOGRAPHY \nThis bibliography of FFT applications is categorized for ease of reference. \nClassifications are as follows: \nA. Biomedical Engineering \nB. Array Processing \nC. Deconvolution \nD. Spectroscopy, Microscopy, Electrochemical Analysis \nE. Antenna and Wave Propagation \nF. Numerical Methods \nG. FFT Hardware \nH. Radar \nI. Instrumentation \nJ. General and Miscellaneous Applications \nK. Spectral Estimation, Frequency Analysis \nL. Time Domain Reflectometry, Delay Estimation \nM. Network Analysis \nN. Two-Dimensional Algorithms, Multidimensional Analysis \nO. Sonics, Ultrasonics, Acoustics \nP. Filter Design, Digital Filtering, Convolution, Correlation \nQ. Communications \nR. Speech Processing \nS. Multichannel Filtering, Transmultiplexers \n394 \nBibliography \nT. FFT Algorithms \nU. Geophysical Processing \nv. Mechanical Analysis, Fluid Mechanics, Structural Analysis \nw. FFT Error Analysis \nx. Image Processing, Optics \nY. Electrical Power Systems \nz. Simulation \n395 \nABBOUD, S., I. BRUDERMAN, AND D. SARDEH, \"Frequency and Time Domain \nA \nAnalysis of Airflow Breath Patterns in Patients with Chronic Obstructive \nAirway Disease,\" Comput. and Biomed. Res., Vol. 19, No.3, pp. 266-\n73, June 1986. \nABDELMALEK, N. N., T. KASVAND, ANDJ. P. CROTEAU, \"Image Restoration \nX \nFor Space Invariant Pointspread Functions,\" Appl. Opt., Vol. 19, No. \n7, pp. 1184-89, April 1980. \nABUSHAGUR, M. A. G., AND N. GEORGE, \"Measurement of Optical Fiber \nX \nDiameter Using The Fast Fourier Transform,\" Applied Optics, Vol. 19, \nNo. 12, pp. 2031-33, June 1980. \nACOSTA, R. J., AND RICHARD Q. LEE, \"Case Study of Sample-Spacing in \nE \nPlanar Near-Field Measurement of High Gain Antennas,\" NASA Tech \nMemo 86872, p. 9. \nACOSTA, R. J., \"Compensation of Reflector Surface Distortions Using Con-\nE \njugate Field Matching,\" NASA Tech Memo 87198, p. 9, June 1986. \nACOSTA, R. J., \"Secondary Pattern Computation Of An Offset Reflector \nE \nAntenna,\" NASA Tech Memo 87160, p. 19, November 1985. \nAGARWAL, R. C., AND J. W. COOLEY, \"New Algorithms for Digital Con-\nP \nvolution,\" IEEE Trans. Acous. Speech and Signal Processing, Vol. \nASSP-25, No.5, pp. 392-410, October 1977. \nAGRAWAL, Y. C. \"CCD Chirp-Z FFT Doppler Signal Processor For Laser \nH \nVe1ocimetry,\" Journal of \nPhysics E: Scientific Instruments, Vol. 17, No. \n6, pp. 458-61, June 1984. \nAGULLO, J., AND A. BARJAU, \"Reflection Function r(t): A Matrix Approach \n0 \nVersus FFT** Minus ** 1,\" Journal of Sound and Vibration, Vol. 106, \nNo.2, pp. 193-201, April 1986. \nAKIYAMA, TAKAO, \"Pressure Estimation From Oscillatory Signals Obtained \n0 \nThrough BWR's Instrument Lines,\" Journal of \nDynamic Systems, Mea-\nsurement and Control, Transactions ASME, Vol. 108, No.1, pp. 80-85, \nMarch 1986. \nALBA, D., AND G. R. MEIRA, \"Instrumental Broadening Correction In Size \nC \nExclusion Chromatography Through Fast Fourier Transform Tech-\nniques,\" Journal of \nLiquid Chromatography, Vol. 6, No. 13, pp. 2411-\n31, 1983. \n396 \nBibliography \nALLAN, R. N., A. M. LEITE DA SILVA, A. A. ABU-NASSER, AND R. C. BURÂ· \nY \nCHETI, \"Discrete Convolution in Power System Reliability,\" IEEE Trans. \nReliab., Vol. R-30, No.5, pp. 452-56, December 1981. \nALLEN, G. H., \"Programming an Efficient Radix-Four FFT Algorithm,\" \nT \nSignal Processing, Vol. 6, No.4, pp. 325-29, August 1984. \nALLEN, J. B. \"Short-term Spectral Analysis, Synthesis, AND Modification \nK \nBy Discrete Fourier Transform,\" IEEE Trans. Acoust. Speech Signal \nProcess. ASSP-25, pp. 235-38, 1977, corrections in ASSP-25, p. 589, \n1977. \nALLEN, J. B., \"Estimation of Transfer Functions Using the Fourier Trans-\nC \nform Ratio Method,\" American Institute of \nAstronautics and Aeronautics \nJournal, Vol. 8, pp. 414-23, March 1970. \nALSOP, L. E., AND A. A. NOWROOZI, \"Fast Fourier Analysis,\" J. Geophys. \nU \nRes., Vol. 70, No. 22, pp. 5482-83, November 1966. \nALTES, R. A., AND W. J. FAUST, \"Unified Method of Broad-Band Echo \nA \nCharacterization For Diagnostic Ultrasound,\" IEEE Transactions on \nBiomedical Engineering, Vol. BME-27, No.9, pp. 500-508, September \n1980. \nANDERSON, G. L., \"A Stepwise Approach To Computing The Multidimen-\nN \nsional Fast Fourier Transform of Large Arrays,\" IEEE Transactions on \nAcoustics, Speech, and Signal Processing, Vol. ASSP-28, No.3, pp. 280-\n84, June 1980. \nANDREWS, H. C., AND B. R. HUNT, Digitial Image Restoration. Englewood \nX \nCliffs, New Jersey: Prentice Hall, 1977. \nANDREWS, H. c., AND K. L. CASPAR!, \"A Generalized Technique for Spec-\nK \ntral Analysis,\" IEEE Trans. on Computers, Vol. C-19, No. I, pp. 16-25, \nJanuary 1970. \nANON, \"Battery Powered FFT Analysis,\" Noise & Vibration Control \nG \nWorldwide, Vol. 17, No.2, pp. 60-63, February 1986. \nANSARI, R., \"An Extension of the Discrete Fourier Transform,\" IEEE \nT \nTrans. Circuits & Syst., Vol. CAS-32, No.6, pp. 618-19, June 1985. \nAOKI, Y., AND A. BOIVIN, \"Computer Reconstruction of Images from a \nX \nMicrowave Hologram,\" Proceedings of the IEEE, Vol. 58, pp. 821-22, \nMay 1970. \nAOKI, Y., \"Optical and Numerical Reconstructions of Images from Sound-\nX \nwave Holograms,\" IEEE Trans. Audio and Electroacoustics, Vol. AU-\n18, pp. 258-67, September 1970. \nARAMBEPOLA, B., AND P. J. W. RAYNER, \"Discrete Transforms Over Po-\nP \nlynomial Rings With Applications in Computing Multidimensional Con-\nvolutions,\" IEEE Trans. Acoust., Speech and Signal Proces., Vol. \nASSP-28, No.4, pp. 407-14, August 1980. \nARIDGIDES, A., AND D. R. MORGAN, \"Effects ofInput Quantization in Float-\nW \ning-Point Digital Pulse Compression,\" IEEE Trans. Acoust., Speech & \nSignal Process., Vol. ASSP-33, No.2, pp. 434-35, April 1985. \nARNOT, N. R., G. G. WILKINSON, AND R. E. BURGE, \"Applications of the \nN \nBibliography \nICL Dap For Two-Dimensional Image Processing,\" Computer Physics \nCommunications, Vol. 26, No.3 & 4, June 1982. \n397 \nARNon, P. J., G. W. PFEIFFER, AND M. E. TAVEL, \"Spectral Analysis of \nA \nHeart Sounds: Relationships Between Some Physical Characteristics and \nFrequency Spectra of First and Second Heart Sounds In Normals and \nHypertensives,\" Journal of \nBiomedical Engineering, Vol. 6, No.2, pp. \n121-28, April 1984. \nARONSON, E. A., \"Fast Fourier Integration of Piecewise Polynomial Func-\nF \ntions,\" Proceedings of \nthe IEEE, Vol. 57, pp. 691-92, April 1969. \nARRIDGE, R. G. c., \"Improvement of the High Frequency Content of Fast \nK \nFourier Transforms,\" Journal of Physics D: Applied Physics, Vol. 17, \nNo.6, pp. 1101-05, June 1984. \nAZIMI-SADJADI, M. R., AND R. A. KING, \"Two-Dimensional Block Proces-\nN \nsors-Structures and Implementations,\" IEEE Trans. Circuits & Syst., \nVol. CAS-33, No.1, pp. 42-50, January 1986. \nBABIC, H., AND G. C. TEMES, \"Optimum Low-Order Windows For Discrete \nK \nFourier Transform Systems,\" IEEE Trans. Acoust. Speech Signal Pro-\ncess., Vol. ASSP-24, pp. 512-17, 1976. \nBABU, B. N. S., \"Performance of an FFT-Based Voice Coding System in \nR \nQuiet and Noisy Environments,\" IEEE Transactions on Acoustics, \nSpeech and Signal Processing, Vol. ASSP-31, No.5, pp. 1323-27, Oc-\ntober 1983. \nBAKHRAKH, L. D., AND O. S. LITVINOV, \"On an Effective Method of Com-\nE \nputing the Coefficients in the Fourier Series of Functions of Sharply Di-\nrected Antenna Diagrams,\" Trans. In: SOy. Phys.-Dokl., Vol. 25, No.9, \npp. 702-4, September 1980. \nBALCOU, Y., \"A Method To Increase The Accuracy of Fast Fourier Trans-\nK \nform Calculations For Rapidly Varying Functions,\" IEEE Trans. In-\nstrum. and Meas., Vol. IM-30, No.1, pp. 38-40, March 1981. \nBARGER, H. A., \"Evaluation of Discrete Transforms For Use In Digital \nR \nSpeech Recognition,\" Comput. and Electr. Eng., Vol. 6, No.3, pp. 183-\n97, September 1979. \nBASANO, L., AND P. OnoNELLO, \"Real-Time FFT To Monitor Muscle Fa-\nA \ntigue,\" IEEE Transactions on Biomedical Engineering, Vol. 33, No. 11, \npp. 1049-51, 1986. \nBECKER, W., R. JANSSEN, A. MUELLER-HELLMANN, AND H-C. SKUDELNY, \nJ \n\"Analysis of Power Converters For AC-Fed Traction Drives and Micro-\ncomputer-Aided On-Line Optimization of Their Line Response,\" IEEE \nTransactions on Industry Applications, Vol. IA-20, No.3, pp. 605-14, \nMay-June 1984. \nBELLAHSENE, B. E., J. W. HAMILTON, J. G. WEBSTER, AND P. BASS, \"Im-\nA \nproved Method For Recording And Analyzing The Electrical Activity Of \nThe Human Stomach,\" IEEE Transactions on Biomedical Engineering, \nVol. T-BME-32, No. 11, pp. 911-15, November 1985. \nBELLANGER, M. G., AND J. L. DAGUET, \"TDM-FDM Transmultiplexer: Dig-\nQ \n398 \nBibliography \nital Polyphase and FFT,\" IEEE Trans. Commun., Vol. COM-22, No.9, \npp. 1199-1205, September 1974. \nBELOV, Y. I., AND D. E. EMEL'YANOV, \"Properties of Fourier Transform of \nE \nNear-zone Antenna Field Measured at a Nonspherical Surface,\" Radi-\nophys. & Quantum Electron., Vol. 29, No.2, pp. 147-58, February 1986. \nBELYI, A. A., A. I. BOVBEL, AND V. I. MIKULOVICH, \"Investigation of AI-\nT \ngorithms of the Fast Fourier Transform, Base 4, with a Constant Struc-\nture,\" Radio Engineering and Electronic Physics, Vol. 25, No.8, pp. \n43-50, August 1980. \nBENIGNUS, V. A., \"Estimation of Coherence Spectrum of Non-Gaussian \nK \nTime Series Populations,\" IEEE Trans. Audio and Electroacoustics, Vol. \nAU-17, pp. 198-201, September 1969. \nBENIGNUS, V. A., \"Estimation of the Coherence Spectrum and its Confi-\nK \ndence Interval Using the Fast Fourier Transforms,\" IEEE Trans. Audio \nand Electroacoustics, Vol. AU-17, pp. 145-50, June 1969; Correction, \nVol. 18, p. 320, September 1970. \nBENNETT, J. c., \"Fast Algorithm for Calculation of Radiation Integral and \nE \nits Application to Plane-polar Near Field and Far Field Transformation,\" \nElectron. Lett. (GB), Vol. 21, No.8, pp. 343-44, April 1985. \nBERGLAND, G. D., AND H. W. HALE, \"Digital Real-time Spectral Analysis,\" \nK \nIEEE Trans. Electronic Computers, Vol. EC-16, No.2, pp. 180-85, April \n1967. \nBERGLAND, G. D., \"A Fast Fourier Transform Algorithm for Real-valued \nT \nSeries,\" Commun. ACM, Vol. 11, No. 10, pp. 703-10, October 1968. \nBERGLAND, G. D., \"A Guided Tour Of The Fast Fourier Transform,\" IEEE \nJ \nSpectrum, Vol. 6, pp. 41-51, July 1969. \nBERGLAND, G. D., \"A Radix-eight Fast Fourier Transform Subroutine for \nT \nReal-valued Series,\" IEEE Trans. Audio and Electroacoustics, Vol. 17, \nNo.2, pp. 138-44, June 1969. \nBERGLAND, G. D., \"The Fast Fourier Transform Recursive Equations for \nT \nArbitrary Length Records,\" Math. Computation, Vol. 21, pp. 236-38, \nApril 1967. \nBERGLAND, G. D., A Fast Fourier Transform Algorithm Using Base 8 It-\nT \nerations,\" Math. Comput., Vol. 22, pp. 275-79, April 1968. \nBERNARD, R., D. VIDAL-MADJAR, F. BAUDIN, AND G. LAURENT, \"Data Pro-\nH \ncessing & Calibration For An Airborne Scatterometer,\" IEEE Trans. On \nGeoscience & Remote Sensing, Vol. GE-24, No.5, September 1986. \nBERROU, J.-L, AND R. A. WAGSTAFF, \"Virtual Beams From An FFT Beam-\nB \nformer and Their Use to Access the Quality of a Towed-Array System,\" \nProceedings of \nICASSP 82. IEEE International Conference on Acoustics, \nSpeech and Signal Processing, Vol. 2, pp. 811-14, Paris, France, May \n1982. \nBERRUT, J. P., \"Fredholm Integral Equation Of \nThe Second Kind For Con-\nF \nformal Mapping,\" J. Comput Appl. Math, Vol. 14, No. 1-2, pp. 99-110, \nFebruary 1986. \nBibliography \n399 \nBERTRAM, S., \"Frequency Analysis Using The Discrete Fourier Trans-\nK \nform,\" IEEE Trans. Audio Electroacoust. AU-18, pp. 495-500, 1970. \nBERTRAM, S., \"On The Derivation Of The Fast Fourier Transform,\" IEEE \nT \nTrans. Audio Electroacoust., Vol. AU-18, pp. 55-58, March 1970. \nBHUYAN, L. N., AND D. P. AGRAWAL, \"Performance Analysis of FFT AI-\nG \ngorithms and Multiprocessor Systems,\" IEEE Trans. Software Eng., Vol. \nSE-9, No.4, pp. 512-21, July 1983. \nBINGHAM, c., M. D. GODFREY, AND J. W. TUKEY, \"Modern Techniques of \nK \nPower Spectral Estimation,\" IEEE Trans. Audio and Electroacoustics, \nVol. AU-18, pp. 439-42, June 1967. \nBINGHAM, C., M. D. GODFREY, AND J. W. TUKEY, \"Modern Techniques Of \nK \nPower Spectrum Estimation,\" IEEE Trans. Audio Electroacoust., Vol. \nAU-17, pp. 56-66, June 1967. \nBLANKEN, J. D., AND P. L. RUSTAN, \"Selection Criteria For Efficient Im-\nT \nplementation of FFT Algorithms,\" IEEE Transactions on Acoustics, \nSpeech and Signal Processing, Vol. ASSP-30, No. I, pp. 107-9, February \n1982. \nBLANKENSHIP, P. E., AND E. M. HOFSTETTER, \"Digital Pulse Compression \nH \nVia Fast Convolution,\" IEEE Trans. Acoust., Speech and Signal Pro-\ncess., Vol. ASSP-23, No.2, pp. 189-201, April 1975. \nBLUESTEIN, L. I., \"A Linear Filter Approach to the Computation of the \nT \nDiscrete Fourier Transform,\" Nerem Record, pp. 218-19, 1968. \nBOGART, B. P., AND E. PARZEN, \"Informal Comments on the Uses of Power \nK \nSpectrum Analysis,\" IEEE Trans. Audio and Electroacoustics, Vol. AU-\n15, pp. 74-76, June 1967. \nBOlTE, R., H. LEICH, AND J. HANCQ, \"An Efficient Computation Method \nF \nfor the Bayard-Bode Relations with Applications,\" 1982 International \nSymposium on Circuits and Systems, Vol. 3, pp. 872-75, May 1982. \nBOKHARI, S. A., AND N. BALAKRISHNAN, \"Analysis of \nCylindrical Antennas-\nE \nA Spectral Iteration Technique,\" IEEE Trans. Antennas & Propag., Vol. \nAP-33, No.3, pp. 251-58, March 1985. \nBOLD, GARY E. J., \"Comparison Of The Time Involved In Computing Fast \nT \nHartley And Fast Fourier Transforms,\" IEEE Proceedings, Vol. 73, No. \n12, pp. 1863-64, December 1985. \nBOLLINGER, K. E., AND R. GILCHRIST, \"Voltage Regulator Models Using \nY \nAutomated Frequency Response Equipment,\" IEEE Transactions on \nPower Apparatus and Systems, Vol. PAS-IOI, No.8, pp. 2899-2905, \nAugust 1982. \nBOMAR, L. c., W. J. STEINWAY, S. A. FAULKNER, AND L. L. HARKNESS, \nH \n\"CW Multi-Tone Radar Ranging Using DFT Techniques,\" Conference \nProceedings of \nthe 13th European Microwave Conference, pp. 127-32, \nNurenberg, Ger., September 1983. \nBONGIOVANNI, G., \"VLSI Network For Variable Size FFT's,\" IEEE Trans-\nG \nactions on Computers, Vol. C-32, No.8, pp. 756-60, August 1983. \n400 \nBibliography \nBONGIOVANNI, G., P. CORSINI, AND G. FROSINI, \"One Dimensional And Two \nN \nDimensional Generalized Discrete Fourier Transform,\" IEEE Trans. \nAcoust. Speech Signal Process., Vol. ASSP-24, pp. 97-99, 1976. \nBONNEROT, G., AND M. BELLANGER, \"Odd-time Odd-frequency Discrete \nT \nFourier Transform For Symmetric Real-valued Series,\" Proc. IEEE, Vol. \n64, pp. 392-93, 1976. \nBORGIOLI, R. C., \"Fast Fourier Transform Correlation Versus Direct Dis-\nP \ncrete Time Correlation,\" Proceedings of the IEEE, Vol. 56, pp. 1602-\n1604, September 1968. \nBORUP, D. T., AND O. P. GANDHI, \"Calculation Of High-Resolution SAR \nE \nDistributions In Biological Bodies Using The FFT Algorithm And Con-\njugate Gradient Method,\" IEEE Transactions on Microwave Theory and \nTechniques, Vol. MTT-33, No.5, pp. 417-19, May 1985. \nBORUP, D. T., AND O. P. GANDHI, \"Fast-Fourier-Transform Method for \nE \nCalculation of SAR Distributions in Finely Discretized Inhomogeneous \nModels of Biological Bodies,\" IEEE Trans. Microwave Theory and \nTech., Vol. MTT-32, No.4, pp. 355-60, April 1984. \nBOURQUIN, J. J., \"Simulation by the Fast Fourier transform,\" Conference \nZ \nProceedings. 28th Midwest Symposium on Circuits and Systems, pp. 278-\n82, August 1985. \nBOVBEL, E. I., A. M. ZAITSEVA, AND V. I. MIKULOVICH, \"Effective Algo-\nT \nrithms for the Fast Fourier Transform With a Mixed Base,\" Radio Eng. \nand Electron. Phys., Vol. 27, No.5, pp. 99-104, May 1982. \nBOYER, A. L., AND E. C. MOK, \"Brachytherapy Seed Dose Distribution \nA \nCalculation Employing the Fast Fourier Transform,\" Med. Phys., Vol. \n13, No.4, pp. 525-29, August 1986. \nBOYES, J. D., \"Noise and Vibration Analysis of Reciprocating Machines,\" \nV \nNoise and Vih. Control Worldwide (GB), Vol. 12, No.3, pp. 90-92, April \n1981. \nBRASS, A., AND G. S. PAWLEY, \"Two And Three Dimensional FFTs On \nN \nHighly Parallel Computers,\" Parallel Comput, Netherlands, Vol. 3, No. \n2, pp. 167-84, May 1986. \nBRAULT, J. W., AND O. R. WHITE, \"The Analysis and Restoration of As-\nC \ntronomical Data via the Fast Fourier Transform,\" Astronomy and As-\ntrophysics, Vol. 13, No.2, pp. 169-89, July 1971. \nBRENNER, N. M., \"Fast Fourier Transform of Externally Stored Data,\" \nT \nIEEE Trans. Audio and Electroacoustics, Vol. AU-17, No.2, pp. 128-\n32, June 1969. \nBRIGGS, W. L., L. B. HART, R. A. SWEET, AND A. O'GALLAGHER, \"Multi-\nT \nprocessor FFT Methods,\" SIAM J. Sci. & Stat. Comput., Vol. 8, No. \n1, pp. S27-42, January 1987. \nBRIGHAM, E. 0., AND R. E. MORROW, \"The Fast Fourier Transform,\" IEEE \nJ \nSpectrum, Vol. 4, No. 12, pp. 63-70, December 1967. \nBRIGHAM, E. 0., The Fast Fourier Transform. Englewood Cliffs, New Jer-\nJ \nsey: Prentice Hall, 1974. \nBibliography \n401 \nBRONZINO, J. D., \"Quantitative Analysis Of The EEG-General Concepts \nA \nand Animal Studies,\" IEEE Transactions On Biomedical Engineering, \nVol. BME-31, No. 12, pp. 850-56, December 1984. \nBROWN, B. L., W. J. STRONG, AND A. C. RENCHER, \"Fifty Four Voices From \nR \nTwo: The Effects of Simultaneous Manipulations of Rate, Mean Fun-\ndamental Frequency, and Variance of Fundamental Frequency on Rat-\nings of Personality From Speech,\" J. Acoust. Soc. Am., Vol. 55, No.2, \npp. 313-18, February 1974. \nBROWN, T. D., R. H. GABEL, D. R. PEDERSEN, L. D. BELL, AND W. F. BLAIR, \nA \n\"Some Characteristics Of Laminar Flow Velocity Spectra Detected By \nA 20 MHZ Pulsed Ultrasound Doppler,\" J. Biomech, Vol. 18, No. 12, \npp. 927-38, 1985. \nBRUCE, J. D., \"Discrete Fourier Transforms, Linear Filters and Spectrum \nP \nWeighting,\" IEEE Trans. Audio and Electroacoustics, Vol. AU-16, No. \n4, pp. 495-99, December 1%8. \nBUCCI, O. M., AND G. DIMASSA, \"Exact Sampling Approach For Reflector \nE \nAntennas Analysis,\" IEEE Trans. Antennas and Propag., Vol. AP-32, \nNo. 11, pp. 1259-62, November 1984. \nBUCCi, O. M., AND G. DIMASSA, \"Sampling Approach for Fast Computation \nE \nof Scattered Fields,\" Electron. Lett. (GB), Vol. 19, No. I, pp. 15-17, \nJanuary 1983. \nBUCCI, O. M., G. D'ELIA, G. FRANCESCHETTI, AND R. PIERRI, \"Efficient \nE \nComputation of the Far Field of Parabolic Reflectors by Pseudo-sampling \nAlgorithm,\" IEEE Trans. Antennas and Propag., Vol. AP-31, No.6, pp. \n931-37, November 1983. \nBUIJs, H. L., \"Fast Fourier Transformation of Large Arrays of Data,\" \nT \nApplied Optics., Vol. 8, No. I, pp. 211-12, January 1969. \nBUIJs, H. L., \"Implementation of a Fast Fourier Transform (FFT) For \nX \nImage Processing Applications,\" IEEE Trans. Acoust. Speech Signal \nProcess., Vol. ASSP-22, pp. 420-24, 1974. \nBUIJs, H. L., A. POMERLEAU, M. FOURNIER, AND W. G. TAM, \"Implemen-\nX \ntation of a Fast Fourier Transform (FFT) For Image Processing Appli-\ncations,\" IEEE Trans. Acoust., Speech and Signal Process., Vol. ASSP-\n22, No.6, pp. 420-24, December 1974. \nBURRUS, C. S., AND P. W. ESCHENBACHER, \"An In-Place, In-Order Prime \nT \nFactor FFT Algorithm,\" IEEE Transactions on Acoustics, Speech, and \nSignal Processing, Vol. ASSP-29, No.4, pp. 806-17, August 1981. \nBURRUS, C. S., \"Comments on 'Selection Criteria For Efficient Implemen-\nT \ntation of FFT Algorithms',\" IEEE Trans. Acoust., Speech and Signal \nProcess., Vol. ASSP-31, No.1, Pt. 1, p. 206, February 1983. \nBURRUS, C. S., \"Index Mappings For Multidimensional Formulation of the \nT \nFFT and Convolution,\" IEEE Trans. Acoust. Speech Signal Process., \nVol. ASSP-25, pp. 239-42, June 1977. \nBURRUS, C. S., \"New Prime Factor FFT Algorithm,\" Proceedings-\nT \nICASSP, IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing, Vol. 1, pp. 335-38, Atlanta, Georgia, March 1981. \n402 \nBibliography \nBUSIGIN, A., \"FFT Software For The IBM PC,\" IEEE Micro, Vol. 6, No. \nT \nI, p. 6, 1986. \nBUTCHER, W. E., AND G. E. COOK, \"Comparison of \nTwo Impulse Response \nC \nIdentification Techniques,\" IEEE Trans. Automatic Control, Vol. AC-\n15, pp. 129-30, February 1970. \nBUTZ, A. R., \"FFT Length in Digital Filtering,\" IEEE Trans. Comput., \nP \nVol. C-28, No.8, pp. 577-80, August 1979. \nCAIN, A. B., J. H. FERZIGER, AND W. C. REYNOLDS, \"Discrete Orthogonal \nF \nFunction Expansions for Non-Uniform Grids Using the Fast Fourier \nTransform,\" J. Comput. Phys., Vol. 56, No.2, pp. 272-86, November \n1984. \nCANDEL. S. M., \"Dual Algorithms for Fast Calculation of \nthe Fourier-Bessel \nF \nTransform,\" IEEE Trans. Acoust., Speech and Signal Process., Vol. \nASSP-29, No.5, pp. 963-72, October 1981. \nCANDEL, S. M., \"Fast Computation of Fourier-Bessel Transforms,\" Pro-\nF \nceedings of ICASSP 82. IEEE International Conference on Acoustics, \nSpeech and Signal Processing, pp. 2076-79, Paris, France, May 1982. \nCARAISCOS, C., AND B. LIU, \"Two Dimensional FFT Using Mixed Time and \nT \nFrequency Decimations,\" Proceedings of ICASSP 82. IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing, Vol. I, \npp. 24-27, Paris, France, May 1982. \nCARLSON, D. A., \"Time-Space Tradeoffs on Back-to-Back FFT Algo-\nT \nrithms,\" IEEE Trans. Comput., Vol. C-32, No.6, pp. 585-9, June 1983. \nCARSON, C. T., \"The Numerical Solution of Waveguide Problems by Fast \nE \nFourier Transforms,\" IEEE Trans. Microwave Theory, Vol. MTT-16, \nNo. 11, pp. 955-58, November 1968. \nCARTER, G. c., \"Coherence And Its Estimation Via The Partitioned Mod-\nK \nified Chirp-Z Transform,\" IEEE Trans. Acoust. Speech Signal Process., \nVol. ASSP-23, pp. 257-63, 1975. \nCARTER, G. c., \"Receiver Operating Characteristics For A Linearly Thresh-\nQ \nolded Coherence Estimation Detector,\" IEEE Trans. Acoust. Speech Sig-\nnal Process., Vol. ASSP-25, pp. 90-92, 1977. \nCARTER, G. C., C. H. KNAPP, AND A. H. NUTTALL, \"Estimation Of The \nK \nMagnitude-Squared Coherence Function Via Overlapped Fast Fourier \nTransform Processing,\" IEEE Trans. Audio Electracoust., Vol. AU-21, \npp. 337-44, 1973. \nCATEDRA, M. F., \"Solution to Some Electromagnetic Problems Using Fast \nE \nFourier Transform with Conjugate Gradient Method,\" Electronic Letters \n(OB), Vol. 22, No. 20, pp. 1049-51, September 1986. \nCAWLEY, P., \"Accuracy Of Frequency Response Function Measurements \nI \nUsing FFT-Based Analyzers With Transient Excitation,\" Journal ofVi-\nbration, Acoustics, Stress, and Reliability In Design, Vol. 108, No.1, \npp. 49-49, January 1986. \nCAWLEY, P., \"Reduction of Bias Error In Transfer Function Estimates \nJ \nUsing FFT-Based Analyzers,\" Journal of Vibration, Acoustics, Stress, \nand Reliability in Design, Vol. 106, No.1, pp. 29-35, January 1984. \nBibliography \n403 \nCETIN, A. E., AND R. ANSARI, \"Iterative Procedure for Designing Two-\nP \ndimensional FIR Filters,\" Electron. Lett. (GB), Vol. 23, No.3, pp. 131-\n33, January 1987. \nCHANDER, V., V. K. AATRE, AND K. RAMAKRISHMA, \"2's Complement Error \nW \nand Scaling in FFT Structures,\" 1984 IEEE International Symposium on \nCircuits and Systems Proceedings, Vol. 1, pp. 280-83, May 1984. \nCHANG, T. L., AND D. R. ELLIOTT, \"Communication Channel Demulti-\nS \nplexing Via An FFT,\" IEEE Conference Record of the Thirteenth Asi-\nlomar Conference on Circuits, Systems and Computers, pp. 162-66, Pa-\ncific Grove, Ca., November 1979. \nCHANOUS, D., \"Synthesis of Recursive Digital Filters Using the FFT,\" \nP \nIEEE Trans. Audio and Electroacoustics, Vol. 18, pp. 211-12, June 1970. \nCHAUDHRY, F. I., S. C. DENNIS, AND J. B. HARNESS, \"Spectral Analysis of \nA \nElectrocardiogram Signals Of The Isolated Guinea Pig Heart For The \nDetection Of Arrhythmias,\" Journal of \nBiomedical Engineering, Vol. 4, \nNo.4, pp. 289-93, October 1982. \nCHEN, W. H., C. H. SMITH, AND S. C. FRALICK, \"A Fast Computational \nT \nAlgorithm For The Discrete Cosine Transform, IEEE Trans. Commun., \nVol. COM-25, pp. 1004-1009, 1977. \nCHIANG, Y. C., AND A. A. SEIREG, \"Acoustic Design Of Variably Segmented \n0 \nPipes,\" Comput Mech Eng., Vol. 3, No.6, pp. 57-59, May 1985. \nCHILDERS, D. G., Modern Spectrum Analysis. New York: IEEE Press, 1978. \nK \nCHORNIK, B., H. E. BISHOP, A. LE MOEL, AND C. LE GRESSUS, \"Decon-\nC \nvolution in Auger Electron Spectroscopy,\" Scanning Electron Microsc, \nPt. 1, pp. 77-88, 1986. \nCHOWDARY, N., AND W. STEENAART, \"Accumulation Of Product Roundoff \nT \nErrors In Modified FFT's,\" IEEE Transactions On Circuits and Systems, \nVol. CAS-33, No.1, pp. 103-7, January 1986. \nCHOWDARY, N. U., AND W. STEENAART, \"A High Speed Two-dimensional \nN \nFFT Processor,\" ICASSP 84. Proceedings of the IEEE International \nConference on Acoustics, Speech and Signal Processing, 4.1111-4 Vol. \n1, San Diego, Ca., March 1984. \nCHOWDHURY, S. K., AND A. K. MAJUMDER, \"Digital Spectrum Analysis of \nA \nRespiratory Sound,\" IEEE Transactions On Biomedical Engineering, \nVol. BME-28, No. 11, pp. 784-88, November 1981. \nCHOY, F. K., AND W. H. LI, \"Frequency Component and Modal Synthesis \nV \nAnalysis of Large Rotor-bearing Systems with Base Motion Induced Ex-\ncitations,\" J. Franklin Inst., Vol. 323, No.2, pp. 145-68, 1987. \nCHRISTODOULOU, C. G., AND J. F. KAUFFMAN, \"On the Electromagnetic \nE \nScattering From Infinite Rectangular Grids With Finite Conductivity,\" \nIEEE Transactions on Antennas and Propagation, Vol. AP-34, No.2, \npp. 144-54, February 1986. \nCHU, S., AND C. S. BURRUS, \"Prime Factor FTT Algorithm Using Distrib-\nT \nuted Arithmetic,\" IEEE Transactions On Acoustics, Speech, and Signal \nProcessing, Vol. ASSP-30, No.2, pp. 217-27, 1982. \n404 \nBibliography \nCHU, W. T., \"Architectural Acoustic Measurements Using Periodic Pseu-\n0 \ndorandom Sequences and FFT,\" Journal of the Acoustical Society of \nAmerica, Vol. 76, No.2, pp. 475-78, August 1984. \nCLARK, c., R. J. DOOLING, AND T. BUNNELL, \"Analysis and Synthesis of \nJ \nBird Vocalizations: An FFT-Based Software System,\" Behav. Res. \nMethods and Instrum., Vol. 15, No.2, pp. 251-53, April 1983. \nCLAYTON, C., J. A. MCCLEAN, AND G. J. MCCARRA, \"FFT Performance \nTesting of Data Acquisition Systems,\" IEEE Transactions on Instru-\nmentation and Measurement, Vol. IM-35, No.2, pp. 212-15, June 1986. \nCLINE, C. L., AND H. J. SIEGEL, \"Augmenting ADA for SIMD Parallel Pro-\nT \ncessing,\" IEEE Trans. On Software Engineering, Vol. SE-Il, No.9, pp. \n970-77, September 1985. \nCOBB, R. F., \"Compute Lowpass Filter Responses For Special Driving \nZ \nFunctions,\" EDN, Vol. 29, No.9, pp. 265-72, May 1984. \nCOBB, R. F., \"Extend Basic FFT Capabilities to Handle Many Filter \nZ \nTypes,\" EDN, Vol. 29, No. 12, pp. 183-93, June 1984. \nCOBB, R. F., \"Format Driving Functions To Simplify FFT Routines,\" EDN, \nZ \nVol. 29, No.7, pp. 237-46, April 1984. \nCOBB, R. F., \"Use Fast Fourier Transform Programs to Simplify, Enhance \nZ \nFilter Analysis,\" EDN, Vol. 29, No.5, pp. 209-18, March 1984. \nCOCHRAN, W. T., et aI., \"What is the Fast Fourier Transform?,\" Proc. \nJ \nIEEE, Vol. 55, pp. 1664-73, October 1967. \nCOOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH, \"The Application of \nK \nthe Fast Fourier Transform Algorithm to the Estimation of Spectra and \nCross-spectra,\" Journal of Sound Vibration, Vol. 12, pp. 339-52, July \n1970. \nCOOLEY, J. W., AND J. W. TUKEY, \"An Algorithm For The Machine Cal-\nT \nculation Of Complex Fourier Series,\" Math. of Com \nput., Vol. 19, No. \n90, pp. 297-301, 1965. \nCOOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH, \"Historical Notes On \nJ \nThe Fast Fourier Transform,\" Proc. IEEE, Vol. 55, pp. 1675-77, October \n1967. \nCOOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH, \"The Fast FOUl;er \nF \nTransform Algorithm. Programming Considerations In The Calculation \nOf Sine, Cosine and Laplace Transforms,\" J. Sound Vib., Vol. 12, pp. \n315-37, July 1970. \nCOOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH, \"The Finite Fourier \nJ \nTransform,\" IEEE Trans. Audio and Electroacoustics., Vol. 17, No.2, \npp. 77-85, June 1969. \nCOOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH, \"Application of the \nP \nFast Fourier Transforms to Computation of Fourier Integrals, Fourier \nSeries, and Convolution Integrals,\" IEEE Trans. Audio and Electro-\ncoustics, Vol. AU-15, No.2, pp. 79-84, June 1967. \nCOOLEY, J. W., P. A. W. LEWIS, AND P. D. WELCH, \"The Fast Fourier \nJ \nBibliography \nTransform And Its Applications,\" IEEE Trans. Educ., Vol. E-12, No.1, \npp. 27-34, March 1969. \n405 \nCOOPER, D. E., AND S. C. Moss, \"Picosecond Optoelectronic Measurement \nJ \nOf The High-Frequency Scattering Parameters Of A GaAs FET,\" IEEE \nJ. Quantum Electron, Vol. QE-22, No.1, pp. 94-100, January 1986. \nCOREY, L. E., J. C. WEED, AND T. C. SPEAKE, \"Modeling Triangularly \nE \nPacked Array Antennas Using a Hexagonal FFT,\" 1984 International \nSymposium Digest, Antennas and Propagation, Vol. 2, pp. 507-10, June \n1984. \nCORINTHIOS, M. J., K. C. SMITH, AND J. L. YEN, \"A Parallel Radix-4 Fast \nG \nFourier Transform Computer,\" IEEE Trans. Comput., Vol. C-24, No.1, \npp. 80-92, January 1975. \nCORSINI, P., AND G. FROSINI, \"Properties of the Multidimensional Gener-\nT \nalized Discrete Fourier Transform,\" IEEE Trans. Comput., Vol. C-28, \nNo. 11, pp. 819-20, November 1979. \nCOZZENS, J. H., AND L. A. FINKELSTEIN, \"Computing the Discrete Fourier \nT \nTransform Using Residue Number Systems In A Ring Of Algebraic In-\ntegers,\" IEEE Transactions On Information Theory, Vol. 1T-31, No.5, \npp. 580-88, September 1985. \nCRAIG, A. D., AND P. D. SIMS, \"Fast Integration Techniques For Reflector \nE \nAntenna Pattern Analysis,\" Electron. Lett. (GB), Vol. 18, No.2, pp. 60-\n62, January 1982. \nCRITINA, S., AND M. D'AMORE, \"DFT-based Procedure for Transmission-\nY \nline Transient Computation,\" IEEE Proc. C (GB), Vol. 134, No.2, pp. \n138-43, March 1987. \nCUSHMAN, R. H., \"Add the FFT to Your Box of Design Tools,\" EDN, Vol. \nM \n26, No. 18, pp. 83-88, September 16, 1981. \nD'ALESSIO, T., \"'Objective' Algorithm For Maximum Frequency Esti-\nA \nmation In Doppler Spectral Analysers,\" Medical & Biological Engi-\nneering & Computing, Vol. 23, No.1, pp. 63-68, January 1985. \nD'ELIA, G., G. LEONE, AND R. PIERRI, \"Analysis of Fresnel Field Of A \nE \nSubreflector By An Optimal Use OfFFT,\" Electronics Letters, Vol. 20, \nNo.8, pp. 332-34, April 12, 1984. \nD'INCAU, S., AND R. G. GARNER, \"Application of Two-Channel FFT Anal-\nJ \nysis to Paper Machines,\" Journal of Pulp and Paper Science, Vol. 9, \nNo.2, pp. 38-42, June 1983. \nDAISHIDO, T., K. ASUMA, H. OHARA, S. KOMATSU, AND K. NAGANE, \"FFT \nL \nProcessor as a Digital Lens for Radio Patrol Camera in Astrophysics,\" \nICASSP 86 Proceedings, International Con! on Acoust. Speech and Sig-\nnal Processing, Vol. 4, pp. 2855-57, April 1986. \nDALY, R. H., AND J. M. GLASS, \"Digital Signal Processing for RADAR,\" \nH \nElectron Prog, Vol. 17, No.1, pp. 24-30, Spring 1975. \nDAMASKOS, N. J., R. T. BROWN, J. R. JAMESON, AND P. L. E. USLENGHI, \nE \n\"Transient Scattering By Resistive Cylinders,\" IEEE Transactions on \nAntennas and Propagation, Vol. AP-33, No.1, pp. 21-25, January 1985. \n406 \nBibliography \nDAME, L., AND F. VAKILI, \"Ultraviolet Resolution of Large Mirrors Via \nX \nHartmann Tests and Two-Dimensional Fast Fourier Transform Analy-\nsis,\" Opt. Eng., Vol. 23, No.6, pp. 759-65, November-December 1984. \nDANIELL, G. J., \"Improvement In The Evaluation Of Convolutions Of Real \nJ \nSymmetric Signals Using The Fast Fourier Transform,\" Signal Pro-\ncessing, Vol. 10, No.3, pp. 311-13, April 1986. \nDANILENKO, A. I., AND M. O. KISCHENKOV, \"A Method For The Rapid \nE \nCalculation Of The Radiation Patterns Of Nonequidistant Systems Of \nRadiators,\" Radioelectron Commun Syst., Vol. 27, No.2, pp. 77-79, \n1984. \nDASH, P. K., AND D. K. PANDA, \"Spectral Observations of Power Network \nX \nSignals for Digital Signal Processing,\" Microprocess. and Microsyst. \n(GB), Vol. 8, No.9, pp. 475-80, November 1984. \ndEAMEIDA, R. A. R., AND J. MAZZUCCO, JR., \"Fast Calculation of Far-Field \nE \nPatterns of \nReflector Antennas With Fast Fourier Transform,\" Con! \nProc \nEur Microwave Con! 10th, Warsaw, Pol, pp. 61-66, September 1980. \nDEEL, J. c., \"Use of Modal Analysis and Structural Modification Software \nV \nFor The Measurement and Prediction of Machinery Dynamic Charac-\nteristics,\" Noise & Vibration Control Worldwide, Vol. 14, No.1, pp. 7-\n10, February 1983. \nDEL RE, E., F. RONCONI, P. SALVI, AND P. SEMENZATO, \"Comparison of \nQ \nNon-FFT Methods of TDM-FDM Transmultiplexing,\" Alta Freq. (Italy), \nVol. 51, No.1, pp. 9-21, January-February 1982. \nDELL' AGUILA, A., L. SALVATORE, \"A Minicomputer Based System for Anal-\nysis oflnverter-fed Induction Motors,\" Microcomput. Appl., Vol. 4, No. \n1, pp. 22-28, 1985. \nDELOTTO, I., AND D. DOTTI, \"Two-dimensional Transforms by Mini-com-\nN \nputers Without Matrix Transposing,\" Computer Graphics and Image \nProcessing, Vol. 4, No.3, pp. 271-78, September 1975. \nDESPAIN, A. M., A. M. PETERSON, O. S. ROTHAUS, AND E. H. WOLD, \"Fast \nT \nFourier Transform Processors Using Gaussian Residue Arithmetic,\" J. \nParallel Distrib Comput, Vol. 2, No.3, pp. 219-37, August 1985. \nDESPAIN, A., \"Very Fast Fourier Transform Algorithms Hardware For Im-\nG \nplementation,\" IEEE Trans. Comput., Vol. C-28, pp. 333-41, 1979. \nDIDERICH, R., \"Calculating Chebyshev Shading Coefficients Via the Dis-\nP \ncrete Fourier Transform,\" IEEE Proceedings Letters, Vol. 62, No. 10, \npp. 1395-96, October 1974. \nDILLARD, G. M., \"Recursive Computation Of The Discrete Fourier Trans-\nH \nform With Applications To A Pulse-Doppler Radar System,\" Comput. \nElectr. Eng., Vol. 1, pp. 143-52, 1973. \nDILTS, G. A., \"Computation of Spherical Harmonic Expansion Coefficients \nF \nVia FFTs,\" J. Comput. Phys., Vol. 57, No.3, pp. 439-53, February 1985. \n\"Discrete Fourier Transform Applied to Time Domain Signal Processing,\" \nJ \nIEEE Communications Magazine, Vol. 20, No.3, pp. 13-22, May 1982. \nBibliography \n407 \nDOERNBERG, J., H.-S., LEE, AND D. A. HODGES, \"Full-Speed Testing of AI \nJ \nD Converters,\" IEEE J. Solid-State Circuits, Vol. SC-19, No.6, pp. 820-\n27, December 1984. \nDOUGHTY, S., \"Steady-State Torsional Response With Viscous Damping,\" \nV \nJournal of Vibration, Acoustics, Stress, and Reliability in Design, Vol. \n107, No. I, pp. 123-27, January 1985. \nDOYLE, J. F., AND S. KAMLE, \"Experimental Study Of The Reflection And \nV \nTransmission Of Flexural Waves At Discontinuities,\" Journal of \nApplied \nMechanics, Transactions ASME, Vol. 52, No.3, pp. 669-73, September \n1985. \nDOYLE, J. F., \"Further Developments In Determining The Dynamic Contact \nV \nLaw,\" Experimental Mechanics, Vol. 24, No.4, pp. 265-70, December \n1984. \nDRUBIN, M., \"Computation of the Fast Fourier Transform Data Stored in \nT \nExternal Auxiliary Memory for Any General Radix (r = 2 expo n, n \ngreater than or equal to I.),\" IEEE Trans. on Computers., Vol. C-20, \nNo. 12, pp. 1552-58, December 1971. \nDUBNER, H., AND J. ABATE, \"Numerical Inversion of Laplace Transforms \nF \nBy Relating Them to the Finite Fourier Cosine Transforms,\" Journal of \nthe Association for Computing Machinery, Vol. 15, No. I, pp. 115-23, \nJanuary 1968. \nDUCHOVIC, R. J., AND G. C. SCHATZ, \"The FFT Method for Determining \nF \nSemiclassical Eigenvalues-application to Asymmetric Top Rigid Rotors,\" \nJ. Chem. Phys., Vol. 84, No.4, pp. 2239-46, February 1986. \nDUHAMEL, P., AND H. HOLLMANN, '''Split Radix' FFT Algorithm,\" Elec-\nT \ntronics Letters, Vol. 20, No. I, pp. 14-16, January 5, 1984. \nDUHAMEL, P., AND H. HOLLMANN, \"Existence of a 2**n FFT Algorithm \nT \nwith A Number of Multiplications Lower Than 2**n** Plus **1,\" Elec-\ntronics Letters, Vol. 20, No. 17, pp. 690-92, August 16, 1984. \nDUHAMEL, P., \"Implementation of 'Split-Radix' FFT Algorithms For Com-\nT \nplex, Real, AND Real-Symmetric Data,\" IEEE Trans. Acoust., Speech & \nSignal Process., Vol. ASSP-34, No.2, pp. 285-95, April 1986. \nDUMERMUTH, G. P., P. J. HUBER, B. KLEINER, AND T. GASSER, \"Numerical \nJ \nAnalysis of Electroencephalographic Data,\" IEEE Trans. Audio and \nElectroacoustics, Vol. AU-18, pp. 404-11, December 1970. \nDUMOULIN, C. L., G. C. LEVY, AND F. A. L. ANET, \"FFT Algorithm For \nT \nVirtually Stored Data,\" Computers & Chemistry (GB), Vol. 5, No. 2-3, \npp. 125-38, 1981. \nDUNHAM, W. R., J. A. FEE, L. J. HARDING, AND H. J. GRANDE, \"Application \nJ \nof \nFast Fourier Transforms to EPR Spectra of \nFree Radicals In Solution,\" \nJ. Magn. Resonance, Vol. 40, No.2, pp. 351-59, August 1980. \nDURAND, L. G., J. DE GUISE, G., CLOUTIER, R. GUARDO, AND M. BRAIS, \nA \n\"Evaluation Of FFT-Based and Modem Parametric Methods For The \nSpectral Analysis Of Bioprosthetic Valve Sounds,\" IEEE Transactions \non Biomedical Engineering, Vol. BME-33, No.6, pp. 572-78, June 1986. \n408 \nBibliography \nDURRANI, T. S., AND E. GOUTlS, \"Optimisation Techniques for Digital Image \nX \nReconstruction From Their Projections,\" lEE Proc. E (GB), Vol. 127, \nNo.5, pp. 161-69, September 1980. \nDURVASULA, S., B. PANDA, AND R. K. KAPANIA, \"Impulse Excitation Tech-\nV \nnique For Determining Vibration Characteristics Of Aeroelastic Models \nof Tall Structures,\" J Inst Eng India Part ND 2, Vol. 61, May 1981. \nDWYER, R. F., \"Essential Limitations to Signal Detection and Estimation: \nP \nAn application to the arctic under ice environmental noise problem,\" \nProc. IEEE, Vol. 72, No. 11, pp. 1657-60, November 1984. \nEAKER, C. W., AND G. C. SHATZ, \"Fourier Transform Methods for Cal-\nF \nculating Action Variables and Semiclassical Eigen Values for Coupled \nOscillator Systems,\" J. Chem. Phys., Vol. 81, No. 12, Pt. 2, pp. 5913-\n19, December 1985. \nEAST, K. A., AND T. D. EAST, \"Computerized Acoustic Detection of Ob-\nA \nstructive Apnea,\" Comput Methods Prog Biomed, Vol. 21, No.3, pp. \n213-20, December 1985. \nEDDY, D. R., AND F. J. BREMNER, \"Computer Packages That Contain the \nJ \nFFT,\" Behav. Res. Methods and Instrum., Vol. 15, No.2, pp. 254-57, \nApril 1983. \nEKLUNDH, J.~., \"A Fast Computer Method for Matrix Transposing,\" IEEE \nN \nTransactions Computers, Vol. C-21, No.7, pp. 801-3, July 1972. \nEL BANNA, M., AND M. EL NOKALI, \"On The Transient Response Of A \n0 \nCircular Transducer,\" Wave Motion, Vol. 8, No.3, pp. 235-41, May \n1986. \nEpSTEIN, G., \"Recursive Fast Fourier Transforms,\" AFIPS Proc. 1968 Fall \nT \nJoint Computer Con/., Vol. 33, Part 1, Washington D.C., pp. 141-43, \n1968. \nERICKSON, B. K., \"Programmed Calculation of Discrete Fourier Trans-\nT \nforms,\" IEEE Trans. Consum. Electron., Vol. CE-25, No.5, pp. 772-\n76, November 1979. \nERSOY, 0., \"On Relating Discrete Fourier, Sine, and Symmetric Cosine \nT \nTransforms,\" IEEE Trans. Acoust., Speech & Signal Process., Vol. \nASSP-33, No.1, pp. 219-22, February 1985. \nESHLEMAN, R. L., \"Machinery Diagnostics And Your FFT,\" S. V. Sound \nV \nand Vibration, Vol. 17, No.4, pp. 12-18, April 1983. \nEVANS, J. C., AND P. H. MORGAN, \"Simulation of Electron Spin Resonance \nD \nSpectra By Fast Fourier Transform,\" J. Magn. Resonance, Vol. 52, No. \n3, pp. 529-31, May 1983. \nFABER, A. S., AND C. E. Ho, \"Wide-band Network Characterization by \nM \nFourier Transformation of Time Domain Measurements,\" IEEE Journal \nof \nSolid State Circuits,\" Vol. SC-4, No.4, pp. 231-35, August 1969. \nFARMER, R. G., AND B. L. AGRAWAL, \"State-of-the-art Techniques for \nY \nPower System Stabilizer Tuning,\" IEEE Trans. on Power Apparatus and \nSystems, Vol. PAS-I02, No.3, pp. 699-709, March 1983. \nBibliography \n409 \nFAVOUR, J. D., AND J. M. LEBRUN, \"Transient Synthesis for Mechanical \nV \nTesting,\" Instruments and Control Systems, Vol. 43, pp. 125-27, Sep-\ntember 1970. \nFERRIE, J. F., C. W. NAWROCKI, AND G. C. CARTER, \"Applications of the \n0 \nPartitioned and Modified Chirp Z-Transform to Oceanographic Mea-\nsurements,\" IEEE Trans. Acoust., Speech and Signal Process., Vol. \nASSP-23, No.2, pp. 243-44, April 1975. \nFILHO, W. S., O. F. BRUM, AND R. B. PANERAI, \"Spectral Analysis Of Elec-\nA \ntrical Impedance Measurements On The Lower Limbs,\" IEEE Trans-\nactions on Biomedical Engineering, Vol. BME-30, No.7, pp. 387-92, \nJuly 1983. \nFISHCHER, D., G., GOLUB, O. HALD, C. LEIVA, AND O. WlDLUND, \"On Four-\nF \nier-Toeplitz Methods For Separable Elliptic Problems,\" Math. Comput., \nVol. 28, No. 26, pp. 349-68, April 1974. \nFORMAN, M. L., \"Fast Fourier Transform Technique and Its Application \nD \nto Fourier Spectroscopy,\" J. Opt. Soc. Am., Vol. 56, pp. 978-97, July \n1966. \nFORNBERG, B., \"A Vector Implementation of the Fast Fourier Transform \nT \nAlgorithm,\" Math. Comput., Vol. 36, No. 153, pp. 189-91, January 1981. \nFORNBERG, B., \"On A Fourier Method For the Integration of Hyperbolic \nF \nEquations,\" SIAM J. Numer. Anal., Vol. 12, No.4, pp. 509-28, Sep-\ntember 1975. \nFORTES, J. M. P., AND R. SAPAIO-NETO, \"A Fast Algorithm for Sorting and \nQ \nCounting Third-order Intermodulation Products,\" IEEE Trans. Com-\nmun., Vol. COM-34, No. 12, pp. 1266-72, December 1986. \nFOSKETT, C. T., \"Noise and Finite Register Effects In Infrared Fourier \nD \nTransform Spectroscopy,\" Appl. Spectrosc., Vol. 30, No.5, pp. 531-\n39, September-October 1976. \nFox, J., G. SURACE, AND P. A. THOMAS, \"Self-Testing 2-MU m CMOS Chip \nG \nSet For FFT Applications,\" IEEE Journal of Solid-State Circuits, Vol. \nSC-22, No.1, pp. 15-19, February 1987. \nFRASER, D., \"Array Permutation by Index-Digit Permutation,\" J. Assoc. \nT \nComput. Mach., Vol. 23, No.2, pp. 298-309, April 1976. \nFRASER, D., \"BIT Reversal And Generalized Sorting Of Multidimensional \nN \nArrays,\" Signal Processing, Vol. 9, No.3, pp. 163-76, October 1985. \nFRASER, D., \"Incrementing a Bit-reversed Integer,\" IEEE Trans. on Com-\nT \nputers (Short Notes). Vol. C-18, p. 74, January 1969. \nFREISCHLAD, K. R., AND C. L. KOLIOPOULOS, \"Modal Estimation ofa Wave \nX \nFront From Difference Measurements Using the Discrete Fourier Trans-\nform,\" J. Opt. Soc., Am. A., Vol. 3, No. 11, pp. 1852-61, November \n1986. \nG-AE Subcommittee on Measurement Concepts, \"What is the Fast Fourier \nJ \nTransform?,\" IEEE Trans. Audio and Electroacoustics, Vol. AU-15, pp. \n45-55, June 1967; also Proceedings of \nthe IEEE, Vol. 55, pp. 1664-74, \nOctober 1967. \n410 \nBibliography \nGAMBARDELLA, G., \"Time Scaling and Short-time Spectral Analysis,\" \nK \nAcoustical Society of \nAmerica Journal, Vol. 44, pp. 1745-47, December \n1968. \nGAN, R. G., K. F. EMAN, AND S. M. Wu, \"Extended FFT Algorithm for \nT \nARMA Spectral Estimation,\" IEEE Transactions on Acoustics, Speech, \nand Signal Processing, Vol. ASSP-32, No. I, pp. 168-70, February 1984. \nGAVRILOV, A. B., N. I. GORLOV, M. M. SIMONOV, AND Z. G. KURBANGALEEV, \nL \n\"Methods For Increasing Accuracy Of \nAutomated Pulse Reflectometry,\" \nMeasurement Techniques, Vol. 28, No. 10, pp. 871-74, October 1985. \nGECKINLI, N. C., AND D. YAVUZ, \"Some Novel Windows and a Concise \nK \nTutorial Comparison of Window Families,\" IEEE Trans. Acoust. Speech \nSignal Process., Vol. ASSP-26, pp. 501-7, 1978. \nGENTLEMAN, W. M., AND G. SANDE, \"Fast Fourier Transforms For Fun and \nJ \nProfit,\" AFIPS Proc. Fall Joint Comput. Conj., Washington, D.C., Vol. \n29, pp. 563-78, 1966. \nGENTLEMAN, W. M., \"An Error Analysis of Goertzel's (Watt's) Method of \nW \nComputing Fourier Coefficients,\" Computer J., Vol. 12, pp. 160-65, May \n1969. \nGENTLEMAN, W. M., \"Matrix Multiplication and Fast Fourier Transforms,\" \nT \nBell Syst. Tech. J., Vol. 47, pp. 1099-1103, July-August 1968. \nGHOSH, S. K., AND S. P. S. GUPTA, \"X-Ray Diffraction Line ProfIle Of \nJ \nCold-Worked Hexagonal Alloys Zn-Ag: ETA And Epsilon Phase,\" Me-\ntallurgical Transactions A, Vol. 16A, No.8, pp. 1427-35, August 1985. \nGIANZERO, S., AND B. ANDERSON, \"Integral Transform Solution To The Fun-\nU \ndamental Problem in Resistivity Logging,\" Geophysics, Vol. 47, No.6, \npp. 946-56, June 1982. \nGIBBS, J., \"Windowing Boosts Performance of \nDynamic Signal Analyzers,\" \nK \nEDN, Vol. 26, No. 15, pp. 109-13, August 1981. \nGIRGIS, A. A., AND F. M. HAM, \"A New FFT-Based Digital Frequency \nY \nRelay for Load Shedding,\" IEEE Trans. Power Appar. and Syst., Vol. \nPAS-WI, No.2, pp. 433-39, February 1982. \nGIRGIS, A. A., AND F. M. HAM, \"A Quantitative Study of Pitfalls In The \nJ \nFFT,\" IEEE Trans. Aerosp. and Electron. Syst., Vol. AES-16, No.4, \npp. 434-39, July 1980. \nGLASS, J. M., \"An Efficient Method For Improving Reliability ofa Pipeline \nG \nFFT,\" IEEE Trans. Comput., Vol. C-29, No. II, pp. 1017-20, November \n1980. \nGLASSMAN, J. A., \"A Generalization of the Fast Fourier Transform,\" IEEE \nT \nTrans. Comput., Vol. C-19, pp. 105-16, February 1970. \nGLISSON, T. H., AND C. I. BLACK, \"On Digital Replica Correlation Algorithm \n0 \nwith Applications to Active Sonar,\" IEEE Trans. Audio and Electroa-\ncoustics., Vol. AU-17, No.3, pp. 190-97, September 1969. \nGLISSON, T. H., C. I. BLACK, AND A. P. SAGE, \"The Digital Computation \nK \nof Discrete Spectra Using the Fast Fourier Transform,\" IEEE Trans. \nAudio Electroacoust., Vol. AU-18, pp. 271-87, September 1970. \nBibliography \n411 \nGOLD, B., AND T. BIALLY, \"Parallelism in Fast Fourier Transform Hard-\nG \nware,\" IEEE Trans. Audio. Electroacoust., Vol. AU-21, 1973. \nGOLDBERG\" B. G., \"A Continuous Recursive DFT Analyzer-The Discrete \nK \nCoherent Memory Filter,\" IEEE Trans. Acoust., Speech and Signal Pro-\ncess., Vol. ASSP-28, No.6, pp. 760-62, December 1980. \nGONZALEZ, JR., R. R., AND W. J. KOH, \"Spectral Analysis of Doppler Ul-\nA \ntrasonic Flow Signals By a Personal Computer,\" Computers in Biology \nand Medicine, Vol. 13, No.4, pp. 281-86, 1983. \nGOOD, I. J., \"The Relationship Between Two Fast Fourier Transforms,\" \nT \nIEEE Trans. Comput., Vol. C-20, pp. 310-17, March 1971. \nGORDON, R., AND R. M. RANGAYYAN, \"Geometric Deconvolution: A Meta-\nC \nAlgorithm For Limited View Computed C Tomography,\" IEEE Trans-\nactions on Biomedical Engineering, Vol. BME-30, No. 12, pp. 806-10, \nDecember 1983. \nGOTILlEB, P., AND L. J. DE LORENZA, \"Parallel Data Streams and Serial \nT \nArithmetic for Fast Fourier Transform Processors,\" IEEE Trans. Acoust. \nSpeech and Signal Process., Vol. ASSP-22, No.2, pp. 111-17, April \n1974. \nGOTWALS, J. K., \"Performing the Fast Fourier Transform With a Personal \nT \nComputer,\" 1985 ACM SIGSMALL Symposium on Small Systems, pp. \n142-54, May 1985. \nGRACE, O. D., \"Two Finite Fourier Transforms for Bandpass Signals,\" \nS \nIEEE Trans. Audio and Electroacoustics, Vol. AU-18, pp. 501-2, De-\ncember 1970. \nGRACHEV, I. D., M. K. H. SALAKHOV, AND I. S. FISHMAN, \"Efficient Al-\nD \ngorithms for the Processing of Multidimensional Spectroscopic Data Ar-\nrays,\"Comput. Enhanced Spectrosc. (GB), Vol. 2, No. I, pp. 1-12, Jan-\nuary-March 1984. \nGRANSER, H., \"Deconvolution Of Gravity Data Due To Lateral Density \nC \nDistributions,\" Geoexploration, Vol. 23, No.4, pp. 537-47, December \n1985. \nGRAY, D. A., \"Applications Of Parametric And Non-Parametric Spectral \n0 \nEstimation Techniques To Passive Sonar Data,\" Journal of Electrical \nand Electronics Engineering, Australia, Vol. 5, No.2, pp. 112-19, June \n1985. \nGREENSPAN, R. L., P. H. ANDERSON, \"Channel Demultiplexing by Fourier \nS \nTransform Processing,\" EASCON 74 Proceedings, pp. 360-72, 1974. \nGRiGORYAN, A. M., AND M. M. GRIGORYAN, \"Discrete Two-dimensional \nN \nFourier Transforms in a Tensor Representation: New Orthogonal Func-\ntions,\" Optoelectron. Instrum. & Data Process., No. I, pp. 22-28, 1986. \nGROGINSKY, HERBERT L., \"An FFT Chart-letter, Fast Fourier Transform \nJ \nProcessors; Chart Summarizing Relations Among Variables,\" Proceed-\nings of \nthe IEEE, Vol. 58, pp. 1782-84, October 1970. \nGUENDEL, L., \"Novel High-speed Fourier-Vector Processor,\" Signal Pro-\nG \ncessing, Vol. 9, No.2, pp. 107-20, September 1985. \n412 \nBibliography \nGUESSOUM, A., AND R. M. MERSEREAU, \"Fast Algorithms for the Multidi-\nT \nmensional Discrete Fourier Transform,\" IEEE Trans. Acoust., Speech \n& Signal Process., Vol. ASSP-34, No.4, pp. 937-43, August 1986. \nGUSEV, V. G., AND G. V. LOSKUTOVA, \"Use of the Bivariate Fast Fourier \nB \nTransform Algorithm For Process of Information Received From a Linear \nArray Antenna,\" Radio Engineering and Electronic Physics, Vol. 27, No. \n12, pp. 78-82, December 1982. \nGUTKNECHT, M. H., \"Numerical Experiments on Solving Theodorsen's In-\nF \ntegral Equation For Conformal Maps With the Fast Fourier Transform \nand Various Nonlinear Interative Methods,\" SIAM J. Sci. and Stat. Com-\nput., Vol. 4, No. I, pp. 1-30, March 1983. \nGUTKNECHT, M. H., \"Solving Theodorsen's Integral Equation For Con-\nF \nformal Maps With The Fast Fourier Transform and Various Nonlinear \nIterative Methods,\" Numer. Math. (Germany) Vol. 36, No.4, pp. 405-\n29, 1981. \nHAAS, W. H., AND C. S. LINDQUIST, \"Transitional FFT-Based Filters For \nL \nDelay Estimation,\" Con! Rec Asilomar Con! Circuits Syst Comput 14th \nNovember 1980. \nHAGER, W. W., \"A Modified Fast Fourier Transform for Polynomial Eval-\nF \nuation and the Jenkins-Traub Algorithm,\" Numer. Math. (Germany), \nVol. 50, No.3, pp. 253-61, 1987. \nHAIRER, E., C. LUBRICH, AND M. SCHLICHTE, \"Fast Numerical Solution of \nF \nNonlinear Volterra Convolution Equations,\" SIAM J. Sci. and Stat. \nComput., Vol. 6, No.3, pp. 532-41, July 1985. \nHALBERG, L. I., AND K. E. THIELE, \"Extraction of Blood Flow Information \nA \nUsing Doppler-Shifted Ultrasound,\" Hewlett-Packard Journal, Vol. 37, \nNo.6, pp. 35-40, June 1986. \nHALL, J. F., \"FFT Algorithm For Structural Dynamics,\" Earthquake En-\nV \ngineering & Structural Dynamics, Vol. 10, No.6, pp. 797-811, Novem-\nber-December 1982. \nHALLIWELL, G., \"Using Fourier Transform Analysers To Investigate Com-\nV \nponent Vibration,\" Nuclear Engineering International (GB), Vol. 25, \nNo. 296, pp. 42-45, March 1980. \nHAM, F. M., AND A. A. GIRGIS, \"Measurement Of Power Frequency Fluc-\nJ \ntuations Using The FFT,\" IEEE Trans Ind Electron, Vol. IE-32, No.3, \npp. 199-204, August 1985. \nHANNA, M. T., \"Two-dimensional Filtering of Sensor Array Data,\" 1986 \nN \nIEEE International Symposium on Circuits and Systems, Vol. I, pp. 17-\n20, May 1986. \nHARALICK, R. M., \"A Storage Efficient Way to Implement The Discrete \nT \nCosine Transform,\" IEEE Trans. Comput., Vol. C-25, No.7, pp. 764-\n65, July 1976. \nHARDING, J. c., JR., D. A., WADE, R. A. MARINO, E. G. SAUER, AND S. M. \nD \nKLAINER, \"A Pulsed NQR-FFT Spectrometer For Nitrogen-14,\" J. \nMagn. Resonance, Vol. 36, No. I, pp. 21-33, October 1979. \nBibliography \n413 \nHARMANSSON, B., D. YEVICK, AND A. T. FRIBERG, \"Optical Coherence Cal-\nX \nculations with the Split-step Fast Fourier Transform Method,\" Appl. \nOpt., Vol. 16, pp. 2645-47, August 1986. \nHARRIS, D. B., J. H. MCCLELLAN, D. S. K. CHAN, AND H. W. SCHUESSLER, \nN \n\"Vector Radix Fast Fourier Transform,\" 1977 IEEE Int. Con! on Acous-\ntics, Speech and Sig. Proc. Rec., pp. 548-51, May 1977. \nHARRIS, F. J., \"The Discrete Fourier Transform Applied To Time Domain \nJ \nSignal Processing,\" IEEE Communications Magazine, Vol. 20, No.3, \npp. 13-22, May 1982. \nHARRIS, J. H., \"On the Use of Windows for Harmonic Analysis with the \nK \nDiscrete Fourier Transform,\" Proc. IEEE, Vol. 66, pp. 51-83, January \n1978. \nHARRISON, R. E., \"Calculator Program Speeds Through 32 FFT Points,\" \nT \nElectronic Design, Vol. 28, No. 10, pp. 233-36, May 1980. \nHARTWELL, J. W., \"A Procedure for Implementing the Fast Fourier Trans-\nT \nform on Small Computers,\" IBM Journal of \nResearch and Development, \nVol. 15, pp. 355-63, September 1971. \nHASHMATy-MANESH, D., AND S. C. TAM, \"Application of Winograd's Fast \nX \nFourier Transform (FFT) to the Calculation of Diffraction Optical Trans-\nfer Function (OTF),\" Proc. SPIE Int. Soc. Opt. Eng., Vol. 369, pp. 692-\n95, September 1983. \nHASSANEIN, H., AND M. RUDKO, \"On the Use of Discrete Cosine Transform \nJ \nin Cepstral Analysis,\" IEEE Trans. Acoust., Speech and Signal Process., \nVol. ASSP-32, No.4, pp. 922-25, August 1984. \nHAUBRICH, R. A., AND J. W. TUKEY, \"Spectrum Analysis of Geophysical \nK \nData,\" Proc. IBM Scientific Computing Symp. on Environmental Sci-\nences, pp. 115-28, 1967. \nHAVRILLA, J. J., \"A Technique for Determining the Effects of \nAircraft Com-\nE \nponents on the Pattern of Radar Antennas Before They are Built,\" Pro-\nceedings of \nthe IEEE 1984 National Aerospace and Electronics Confer-\nence, Vol. 1, pp. 304-8, May 1984. \nHAYASHI, K., K. K. DHAR, K. SUGAHARA, AND K. HIRANO, \"Design of \nHigh-\nP \nSpeed Digital Filters Suitable For Multi-DSP Implementation,\" IEEE \nTransactions On Circuits and Systems, Vol. CAS-33, No.2, pp. 202-17, \nFebruary 1986. \nHAYKIN, S. S., AND C. M. THORSTEIN SON , \"Decision-directed Delay-lock \nL \nLoop Using Fast Fourier Transform Crosscorrelation,\" Proc. Inst. \nElectr. Eng. (GB), Vol. 121, No.4, pp. 245-49, April 1974. \nHEARN, T. C., J. MAZUMDAR, AND L. J. MAHAR, \"First Heart Sound Spectra \nA \nin Relation to Anterior Mitral-Leaflet Closing Velocity,\" Medical & Bi-\nological Engineering & Computing, Vol. 20, No.4, pp. 466-72, July 1982. \nHEIDEMAN, M. T., AND C. S. BURRUS, \"On the Number of MUltiplications \nT \nNecessary To Compute A Length-2**N DFT,\" IEEE Transactions on \nAcoustics, Speech, and Signal Processing, Vol. ASSP-34, No.1, pp. 91-\n95, February 1986. \n414 \nBibliography \nHEIDEMAN, M. T., D. H. JOHNSON, AND C. S. BURRUS, \"Gauss And The \nT \nHistory Of The Fast Fourier Transform,\" IEEE ASSP Mag. (USA), Vol. \nI, No.4, pp. 14-21, October 1984. \nHELMS, H. D., \"Fast Fourier Transform Method of Computing Difference \nP \nEquations and Simulating Filters,\" IEEE Trans. Audio and Electroa-\ncoustics, Vol. AU-15, No.2, pp. 85-90, June 1967. \nHELMS, H. D., \"Nonrecursive Digital Filters: Design Methods for Achiev-\nP \ning Specifications on Frequency Response,\" IEEE Trans. Audio Elec-\ntroacoust., Vol. AU-16, No.3, pp. 336-42, September 1968. \nHENERY, R. J., \"Solution ofWiener-Hopflntegral Equations Using the Fast \nF \nFourier Transform,\" J. Inst. Math. and Appl. (GB), Vol. 13, No.1, pp. \n89-96, February 1974. \nHENRICI, P., \"Fast Fourier Methods In Computational Complex Analysis,\" \nF \nSwiss Federal Inst. of Technol., Zurich, Switzerland, SIAM Rev., Vol. \n21, No.4, pp. 481-527, October 1979. \nHERMAN, A. J., R. M. ANANIA, J. H. CHUN, C. A. JACEWITZ, AND R. E. F. \nU \nPEPPER, \"Fast Three-Dimensional Modeling Technique and Fundamen-\ntals of Three-Dimensional Frequency-Domain Migration,\" Geophysics, \nVol. 47, No. 12, pp. 1627-44, December 1982. \nHERMANSSON, B., AND D. YEVICK, \"Numerical Analyses of the Modal Ei-\nX \ngenfunctions of Chirped and Unchirped Multiple-stripe-geometry Laser \nArrays,\" J. Op. Soc. Am. A, Vol. 4, No.2, pp. 379-90, February 1987. \nHERMANSSON, B., D. YEVICK, AND A. T. FRIBERG, \"Optical Coherence Cal-\nX \nculations With The Split-Step Fast Fourier Transform Method,\" Appl. \nOpt., Vol. 25, No. 16, pp. 2645-47, August 1986. \nHEUTE, U., AND P. VARY, \"Digital Filter Bank With Polyphase Network \nS \nand FFT Hardware: Measurements and Applications,\" Signal Pro-\ncessing, Vol. 3, No.4, pp. 307-19, October 1981. \nHEUTE, U., \"Impact Of FFT Coefficient Errors On Polyphase Filter \nW \nBanks,\" Signal Processing, Vol. 7, No.2, pp. 119-33, October 1984. \nHEUTE, U., \"Results of a Deterministic Analysis of FFT Coefficient Er-\nW \nrors,\" Signal Processing, Vol. 3, No.4, pp. 321-31, October 1981. \nHIGGINS, R. J., \"Fast Fourier Transform: An Introduction With Some Min-\nJ \nicomputer Experiments,\" AM. J. Phys. (USA), Vol. 44, No.8, pp. 766-\n73, August 1976. \nHINICH, M. J., AND C. S. CLAY, \"The Application of the Discrete Fourier \nU \nTransform in the Estimation of Power Spectra, Coherence and Bispectra \nof \nGeophysical Data,\" Review of \nGeophysics, Vol. 6, pp. 347-62, August \n1968. \nHIROSAKI, B., \"An Orthogonally Multiplexed QAM System Using the Dis-\nQ \ncrete Fourier Transform,\" IEEE Trans. Commun., Vol. COM-29, No.7, \npp. 982-89, July 1981. \nHITCHEN, M., J. B. HARNESS, AND A. J. MEARNS, \"Thermal Entrainment \nA \nDevice for Cardiovascular Investigation,\" Journal of \nMedical Engineer-\ning & Technology, Vol. 4, No.4, pp. 179-82, July 1980. \nBibliography \n415 \nHODGE,A.J., W.J. ADELMAN, R. B. WALTZ,ANDC.L. TYNDALE, \"Analysis \nA \nof Periodic Structure In Model Subcellular Macromolecular Arrays By \nFourier Processing Of Single Line Video Signals In Scanning Transmis-\nsion Electron Microscopy,\" IEEE Transactions on Biomedical Engi-\nneering, Vol. BME-29, No.6, pp. 439-47, June 1982. \nHOLM, W. A., AND J. D. ECHARD, \"FFT Signal Processing For Non-Co-\nH \nherent Radar Systems,\" Proceedings of \nICASSP 82. IEEE International \nConference on Acoustics, Speech and Signal Processing, Vol. 1, pp. 363-\n66, Paris, France, May 1982. \nHOLZER, F., AND R. REIBOLD, \"Numerical Analysis of Ultrasonic Trans-\n0 \nducer Vibrations From Optically Measured Beam Profiles,\" Acustica, \nVol. 60, No.3, pp. 236-43, May 1986. \nHONMA, H., AND M. SAGAWA, \"Improving The Accuracy And Error Anal-\nW \nysis In Floating-Point FFT Computation,\" Electronics and Communi-\ncations in Japan, Vol. 67, No. 11, pp. 9-18, November 1984. \nHORNSBY, J. S., \"Full-Wave Analysis of Microstrip Resonator and Open-\nE \nCircuit End Effect,\" IEEE Proceedings, Part H: Microwaves, Optics and \nAntennas, Vol. 129, No.6, pp. 338-41, December 1982. \nHORTA, L. G., AND JER-NAN JUANG, \"Identifying Approximate Linear \nJ \nModels for Simple Nonlinear Systems,\" Journal of \nGuidance, Control, \nand Dynamics, Vol. 9, No.4, pp. 385-90, July-August 1986. \nHOWARD, A. Q. JR., \"On Approximating Fourier Integrals Transforms By \nU \nTheir Discrete Counterparts in Certain Geophysical Applications,\" IEEE \nTrans. Antennas and Propag., Vol. AP-23, No.3, pp. 264-66, March \n1975. \nHOWARD, S. J., \"Method For Continuing Fourier Spectra Given By The \nK \nFast Fourier Transform,\" J. Opt. Soc. Am., Vol. 71, No.1, pp. 95-98, \nJanuary 1981. \nHOYER, E. A., AND W. R. BERRY, \"An Algorithm for the Two-Dimensional \nN \nFFT,\" 1977 IEEE Int. Conf. on Acoustics, Speech and Sig. Proc., Rec., \npp. 552-55, May 1977. \nHSI-PING, L., AND D. D. KOSLOFF, \"Numerical Evaluation of the Hilbert \nF \nTransform by the Fast Fourier Transform (FFT) Technique,\" Geophys. \nJ. R. Astron. Sco. (GB), Vol. 67, No.3, pp. 791-99, December 1981. \nHsu, Y. C., \"A Method Of \nZip For Tracking Target Velocity and Increasing \nH \nVelocity Resolution,\" 1983 IEEE International Symposium of Circuits \nand Systems, Vol. 1, pp. 101-4, Newport Beach, CA, May 1983. \nHUANG, T. S., \"Two-Dimensional Windows,\" IEEE Trans. Audio and Elec-\nN \ntroacoustics, Vol. AU-20, No.1, pp. 88-89, March 1972. \nHUNG, C. C., AND R. MITTRA, \"Secondary Pattern and Focal Region Dis-\nE \ntribution of Reflector Antennas Under Wide-Angle Scanning,\" IEEE \nTrans. Antennas and Propag. , Vol. AP-31, No.5, pp. 756-63, September \n1983. \nHUNT, B. R., \"Digital Image Processing,\" Proc. IEEE, Vol. 63, No.4, pp. \nX \n693-708, April 1975. \n416 \nBibliography \nIKEDA, N., T. AKOI, H. KOGA, K. TAEKIM, AND K. KIDO, \"Recognition of \nR \nVowels Using the Local Peaks in FFT Spectrum,\" J. Acoust. Soc. Jpn. \n(Japan), Vol. 41, No. 12, pp. 886-90, December 1985. \nIKEDA, Y., AND M. NORIGOE, \"New Realization ofDFT Applied to CCITT \nS \nNo.5 Telephone Signaling System,\" IEEE J. Sel. Areas Commun., Vol. \nSAC-2, No.2, pp. 334-39, March 1984. \nIMAI, M., AND S. INOKUCHI, \"Frequency Identification By Complex Spec-\nR \ntrum (Speech),\" ICASSP 86 Proceedings. IEEE-IECEJ-ASJ Interna-\ntional Conference on Acoustics, Speech and Signal Processing (Cat. No. \n86CH2243-4), Vol. 1, pp. 117-20, Tokyo, Japan, April 1986. \nINADA, H., \"Backscattered Short Pulse Response of Surface Waves From \nJ \nDielectric Spheres,\" Appl. Opt., Vol. 13, No.8, pp. 1928-33, August \n1974. \nJAIN, A. K., AND J. JASIULEK, \"A Class ofFFT Based Algorithms For Linear \nF \nEstimation and Boundary Value Problems,\" Proceedings of the 1981 \nJoint Automatic Control Conference, FA-6B/I-5, Vol. 2, Charlottesville, \nVA, June 1981. \nJAIN, A. K., AND J. JASIULEK, \"Fast Fourier Transform Algorithms For \nF \nLinear Estimation, Smoothing, And Riccati Equations,\" IEEE Trans-\nactions on Acoustics, Speech, and Signal Processing, Vol. ASSP-31, No. \n6, pp. 1435-46, December 1983. \nJAIN, A. K., \"A Fast Karhunen-Loeve Transform For A Class of Random \nF \nProcesses,\" IEEE Trans. Commun., Vol. COM-24, No.9, pp. 1023-29, \nSeptember 1976. \nJAMES, D. V., \"Quantization Errors in the Fast Fourier Transform,\" IEEE \nW \nTrans. Acoust., Speech and Signal Process., Vol. ASSP-23, No.3, pp. \n277-83, June 1975. \nJAMIESON, L. H., P. T. MUELLER, JR., AND H. J. SIEGEL, \"FFT Algorithms \nT \nFor SIMD Parallel Processing Systems,\" J. Parallel and Distrib. Com-\nput., Vol. 3, No.1, pp. 48-71, March 1986. \nJANSEN, B. H., J. R. BOURNE, AND J. W. WARD, \"Autoregressive Estimation \nA \nof Short Segment Spectra For Computerized EEG Analysis,\" IEEE \nTransactions on Biomedical Engineering, Vol. BME-28, No.9, pp. 630-\n38, September 1981. \nJENKE, L. M., \"Application of Digital Fourier Analysis in Processing Dy-\nV \nnamic Aerodynamic Heating Measurements,\" AIAA J., Vol. 17, No.6, \npp. 641-42, June 1979. \nJENKINS, G. M., AND D. G. WAITS, Spectral Analysis and its Applications. \nK \nSan Francisco: Holden-Day, 1968. \nJENKINS, W. K., \"Inherent Phase Distortion In Rectangular Format FFT \nH \nProcessing Algorithms For Synthetic Aperture Radar,\" Proc of \nthe Int \nSymp on Network Theory, pp. 101-5, 1979. \nJESSHOPE, C. R., ''The Implementation of Fast Radix 2 Transforms on Array \nT \nProcessors,\" IEEE Trans. Comput., Vol. C-29, No.1, pp. 20-28, January \n1980. \nBibliography \n417 \nJOHNSON, D. G., AND J. I. SEWELL, \"Improved Z Plane Polynomial Inter-\nM \npolative Analysis of Switched-Capacitor Networks,\" IEEE Trans. Cir-\ncuits and Syst., Vol. CAS-31, No.7, pp. 666-68, July 1984. \nJOHNSON, H. W., AND C. S. BURRUS, \"On The Structure of Efficient DFT \nT \nAlgorithms,\" IEEE Trans. Acoust., Speech & Signal Process., Vol. \nASSP-33, No.1, pp. 248-54, February 1985. \nJOHNSON, L. R., AND A. K. JAIN, \"Efficient Two-Dimensional FFT Algo-\nN \nI;thm,\" IEEE Transactions On Pattern Analysis and Machine Intelli-\ngence, Vol. PAMI-3, No.6, pp. 698-701, November 1981. \nJOHNSON, M. M., \"Direct Application of the Fast Fourier Transform to \nX \nOpen Resonator Calculations,\" Appl. Opt., Vol. 13, No. 10, pp. 2326-\n28, October 1974. \nJOHNSON, S. A., Y. ZHOU, M. K. TRACY, M. J. BERGGREN, AND F. STRENGER, \n0 \n\"Inverse Scattering Solutions by a Sinc Basis, Multiple Source, Moment \nMethod. III. Fast Algorithms,\" Ultrason. Imaging, Vol. 6, No.1, pp. \n103-16, January 1984. \nJOHNSTON, J. A., \"Generating Multipliers For A Radix-4 Parallel FFT al-\nT \ngorithm,\" Signal Processing, Vol. 6, No.1, pp. 61-66, January 1984. \nJOHNSTON, J. A., \"Input/Output Memory For Digital Convolution Via The \nT \nParallel Pipeline Fast Fourier Transformer,\" Signal Processing, Vol. 10, \nNo.2, pp. 193-99, March 1986. \nJOHNSTON, J. A., \"Parallel Pipeline Fast Fourier Transformer,\" IEEE Pro-\nH \nceedings, Part F (GB): Communications, Radar and Signal Processing, \nVol. 130, No.6, pp. 564-72, October 1983. \nJONES, R. H., \"A Reappraisal of the Periodogram in Spectral Analysis,\" \nK \nTechnometrics, Vol. 7, No.4, pp. 531-42, November 1965. \nJONES, W. R., \"Precision FFT Correlation Techniques for Nondeterministic \nL \nWaveform,\" IEEE EASCON Conv. Record, pp. 375-80, October 1974. \nKABAL, P., AND B. SAYAR, \"Performance of Fixed-point FFTs: Rounding \nW \nand Scaling Considerations,\" ICASSP 86 Proceedings. International \nConference on Acoustics, Speech and Signal Processing, Vol. 1, pp. 221-\n24, Tokyo, Japan, April 1986. \nKAHANER, D. K., \"Matrix Description of the Fast Fourier Transform,\" \nT \nIEEE Trans. Audio Electroacoust., Vol. AU-18, No.4, pp. 442-50, De-\ncember 1970. \nKAMANGAR F. A., AND K. R. RAo, \"Fast Algorithms for the 2D-discrete \nN \nCosine Transform,\" IEEE Trans. Comput., Vol. C-31, pp. 899-906,1982. \nKANDA, M., \"Effects of Resistive Loading Of 'TEM' Horns,\" IEEE Trans-\nE \nactions On Electromagnetic Compatibility, Vol. EMC-24, No.2, Pt. 2, \npp. 245-55, May 1982. \nKANDA, M., \"Transients in a Resistively Loaded Linear Antenna Compared \nE \nWith Those In A Conical Antenna and A TEM Horn,\" IEEE Trans. An-\ntennas and Propag., Vol. AP-28, No.1, pp. 132-36, January 1980. \nKANEKO, T., AND H. YAMAUCHI, \"Addressing Technique for Bit-reversal \nG \n418 \nBibliography \nTransfer,\" Trans. Inst. Electron. and Commun. Eng. (Japan), Part (D), \nVol. J69D, No.7, pp. 1124-26, July 1986. \nKANEKO, T. K., AND B. LIU, \"Accumulation of Round-off Errors in Fast \nW \nFourier Transforms,\" J. Assoc. Comput. Mach., Vol. 17, No.4, pp. 637-\n54, October 1970. \nKARLSSON, L., \"Numerical Analysis of Damped Transient Beam Vibration \nV \nby Use of Fourier Transforms,\" Int. J. Numer. Methods Eng. (GB), Vol. \n21, No.4, pp. 683-89, April 1985. \nKATAYAMA, H., \"Diversity Becoming More Apparent In Selection Of FFT \nI \nAnalyzers,\" JEE, Journal of \nElectronic Engineering, Vol. 22, No. 218, \npp. 54-58, February 1985. \nKATYL R. H., \"FFT Calculation of Magnetic Fields In Air Coils,\" IEEE \nJ \nTransactions on Magnetics, Vol. MAG-16, No.3, pp. 545-49, May 1980. \nKATYL, R. H., \"FFT Calculation of \nTwo-Step Semiconductor Impurity Dif-\nJ \nfusion,\" IEEE Transactions on Electron Devices, Vol. ED-27, No.5, pp. \n991-93, May 1980. \nKAY, S. M., AND S. L. MARPLE, JR., \"Spectrum Analysis-A Modem Per-\nK \nspective,\" Proc. IEEE, Vol. 69, pp. 1380-1419, 1981. \nKEMERAIT, R. c., AND D. G. CHILDERS, \"Signal Detection and Extraction \nK \nBy Cepstrum Techniques,\" IEEE Trans. Informat. Theory, 1T-18, No. \n6, pp. 745-59, November 1972. \nKENNEDY, P. D., \"FFT Signal Processing for Noncoherent Airborne Ra-\nH \ndars,\" Proceedings of the 1984 IEEE National Radar Conference, pp. \n79-83, Atlanta, GA, March 1984. \nKERLEY, L. M., \"Teaching Concepts of Data Structures Via the Fast Four-\nT \nier Transform,\" SIGCSE Bull., Vol. 18, No.3, pp. 26-35, September \n1986. \nKEYS, R. G., \"An Algorithm For Computing The Nth Roots of Unity in \nT \nBit-Reversed Order,\" IEEE Trans. Acoust., Speech and Signal Process., \nVol. ASSP-28, No.6, pp. 762-63, December 1980. \nKILPATRICK, D., J. V. TYBERG, AND W. W. PARMLEY, \"Blood Velocity Mea-\nA \nsurement By Fiber Optic Laser Doppler Anemometry,\" IEEE Trans-\nactions on Biomedical Engineering, Vol. BME-29, No.2, pp. 142-45, \nFebruary 1982. \nKIM, C. E., AND M. G. STRINTZIS, \"High Speed Multidimensional Convo-\nP \nlution,\" IEEE Trans. Pattern Anal. and Mach. Intell., Vol. PAMI-2, No. \n3, pp. 269-73, May 1980. \nKIN-CHUE, N. G., \"On the Accuracy of Numerical Fourier Transforms,\" \nF \nJ. Comput. Phys., Vol. 16, No.4, pp. 396-400, December 1974. \nKING, R. E., \"Digital Image Processing in Radioisotope Scanning,\" IEEE \nA \nTrans. Bio-Med. Eng., BME-21, No.5, pp. 414-16, September 1974. \nKIRCHNER, P. D., W. J. SCHAFF, G. N. MARACAS, L. F. EASTMAN, T. I. \nD \nCHAPPELL, AND C. M. RANSOM, \"The Analysis of Exponential and Non-\nexponential Transients in Deep-Level Transient Spectroscopy,\" J. Appl. \nPhys., Vol. 52, No. 11, pp. 6462-70, November 1981. \nBibliography \n419 \nKlTAI, K., AND K. SIEMENS, \"Discrete Fourier transform via Walsh trans-\nT \nform,\" IEEE Trans. Acoust. Speech Signal Process., Vol. ASSP-27, p. \n288, 1979. \nKNIGHT, W. R., AND R. KAISER, \"A Simple Fixed-Point Error Bound For \nW \nThe Fast Fourier Transform,\" IEEE Trans. Acoust., Speech and Signal \nProcess., Vol. ASSP-27, No.6, pp. 615-20, December 1979. \nKOGA, H., T. AOKI, N. IKEDA, K. T. KIM, AND K. KIDO, \"Recognition Of \nR \nVowels In Spoken Words Using Local Peaks In FFT Spectra,\" Mem. \nTohoku Inst. Techno/. Ser. I (Japan), No.6, pp. 47-55, March 1986. \nKOLBA, D. P., AND I. W. PARKS, \"A Prime Factor FFT Algorithm Using \nT \nHigh-speed Convolution,\" IEEE Trans. Acoust. Speech Signal Process., \nVol. ASSP-25, pp. 281-94, 1977. \nKONV ALINKA, I. S., \"Iterative N \nonparametric Spectrum Estimation,\" IEEE \nK \nTrans. Acoust., Speech and Signal Process., Vol. ASSP-32, No.1, pp. \n59-69, February 1984. \nKRON, D. G., AND J. J. LAMBIOTIE, JR., \"Computing the Fast Fourier Trans-\nT \nform on a Vector Computer,\" Math. Comput., Vol. 33, No. 147, pp. \n977-92, July 1979. \nKOSLOFF, D. D., AND E. BAYSAL, \"Forward Modeling By a Fourier \nU \nMethod,\" Geophysics, Vol. 47, No. 10, pp. 1402-12, October 1982. \nKRAUSE, L. 0., \"Digital Dechannelizer and Detoner for 14-Channel 8-Ary \nQ \nFSK,\" Sixteenth Asilomar Conference on Circuits, Systems and Com-\nputers, pp. 457-60, Pacific Grove, CA, November 1983. \nKRYTER, R. c., \"Application of the Fast Fourier Transform Algorithm to \nV \nOn-line Reactor Diagnosis,\" IEEE Trans. Nuclear Sci., Vol. 16, pp. 210-\n17, February 1969. \nKUMARESAN, R., AND P. K. GUPTA, \"A Prime Factor FFT Algorithm With \nT \nReal Valued Arithmetic,\" Proceedings of the IEEE, Vol. 73, No.7, pp. \n1241-43, July 1985. \nLACKOFF, M. R., AND L. R. LEBLANC, \"Frequency-Domain Seismic De-\nC \nconvolution Filtering (For Swallow Water Profiles),\" J. Acoust. Soc. \nAm., Vol. 57, No.1, pp. 151-59, January 1975. \nLADD, S. A., \"Software Solution To Vibration Analysis,\" Mechanical En-\nV \ngineering, Vol. 108, No.2, pp. 73-75, February 1986. \nLAM, P. T. c., S. W. LEE, ANDR. ACOSTA, \"Secondary Pattern Computation \nE \nOf An Arbitrarily Shaped Main Reflector,\" NASA Tech memo 87162, p. \n119, November 1985. \nLAM, P. T., S. LEE, C. C. HUNG, AND R. ACOSTA, \"Strategy For Reflector \nE \nPattern Calculation: Let The Computer Do The Work,\" IEEE Trans. \nAntennas & Propag., Vol. AP-34, No.4, pp. 592-95, April 1986. \nLARSEN, T., AND G. DYRIK, \"Fast Fourier Transforms Using a Microcom-\nJ \nputer,\" Electronics and Wireless World, Vol. 91, No. 1595, pp. 80-82, \nSeptember 1985. \nLAWRENCE, N. B., AND J. D. MOORE, \"Performance Results of An MTII \nH \n420 \nBibliography \nFFT CFAR Radar Signal Processor,\" Proc SOUTHEASTCON Reg 3 \nConi'81, pp. 21-25, Huntsville, Ala., April 1981. \nLEBLANC, L. R., \"Narrow-band Sampled Data Techniques for Detection \n0 \nVia the Underwater Acoustic Communications Channel,\" IEEE Trans. \nCommun. Tech., Vol. 17, pp. 481-88, August 1969. \nLEE, H., AND G. WADE, \"Resolution For Images From Fresnel or Fraun-\n0 \nhofer Diffraction Using The FFT,\" IEEE Transactions on Sonics and \nUltrasonics, Vol. SU-29, No.3, pp. 151-56, May 1982. \nLEE, H. S., H. MORI, AND H. AlSO, \"Parallel Processing FFT For VLSI \nG \nImplementation,\" Transactions of \nthe Institute of \nElectronics and Com-\nmunication Engineers of \nJapan, Section E, Vol. E68, No.5, pp. 284-\n91, May 1985. \nLEE, L-S., Y-P HARN, AND Y-C CHEN, \"A Simple Sample Value Scrambler \nQ \nUsing FFT Algorithms for Secure Voice Communications,\" NTC '80. \nIEEE 1980 National Telecommunications Conference, 49.4/1-5, Hous-\nton, TX, November-December 1980. \nLEE, R. Q., AND R. ACOSTA, \"Numerical Method For Approximating An-\nE \ntenna Surfaces Defined By Discrete Surface Points,\" NASA Tech Memo \n87125, p. 19, 1985. \nLEE, W. H., \"Sampled Fourier Transform Hologram Generated By Com-\nX \nputer,\" Applied Optics, Vol. 9, pp. 639-43, March 1970. \nLEGOFF, H., A. RAMADANE, AND P. LEGoFF, \"Modeling Of Simultaneous \nF \nHeat And Mass Transfer In Gas-Liquid Absorption In A Laminar Falling \nFilm,\" International Journal of Heat and Mass Transfer, Vol. 28, No. \n11, pp. 2005-17, November 1985. \nLEHTINEN, M. S., AND P. R. GOTHONI, \"System For Measuring Tremor \nA \nIntensity In Rats,\" IEEE Trans Biomed Eng., Vol. BME-32, No.8, pp. \n549-53, August 1985. \nLEWIS, R. L., \"Numerical Computation of the Far-Field Excited by a Pre-\nE \nscribed Aperture Distribution along Perpendicular Plane Cuts U \nsing the \nFFT: Program Verification by Comparison with Exact Expressions for \nUniformly-Excited Rectangular or Circular Apertures,\" 1980 Interna-\ntional Symposium Digest. Antennas and Propagation, pp. 287-8, Que-\nbec, Canada, June 1980. \nLI, K. K., G. ARJAVALINGAM,A. DIENES, ANDJ. R. WHINNERY, \"Propagation \nE \nof Picosecond Pulses on Microwave Striplines,\" IEEE Transactions on \nMicrowave Theory and Techniques, Vol. MTT-30, No.8, pp. 1270-73, \nAugust 1982. \nLIANG, C. S., AND R. CLAY, \"Computation of Short-pulse Response From \nH \nRadar Targets-An Application of the Fast Fourier Transform Tech-\nnique,\" Proceedings of the IEEE, Vol. 58, No.1, pp. 169-71, January \n1970. \nLIM, J. S., AND N. A. MALIK, \"A New Algorithm For Two-Dimensional \nK \nMaximum Entropy Power Spectrum Estimation,\" IEEE Trans. Acoust., \nSpeech and Signal Process. , Vol. 29, No.3, Pt. 1, pp. 401-13, June 1981. \nBibliography \n421 \nLIN, D. X., AND R. D. ADAMS, \"Determination Of The Damping Properties \nV \nOf Structures By Transient Testing Using Zoom-FFT,\" Journal of \nPhys-\nics E (GB): Scientific Instruments, Vol. 18, No.2, pp. 161-65, February \n1985. \nLINDERMAN, R. W., P. M. CHAU, W. H. Ku, AND P. P. REUSENS, \"CUSP: \nG \nA 2-MU M CMOS Digital Signal Processor,\" IEEE Journal of \nSolid-State \nCircuits, Vol. SC-20, No.3, pp. 761-69, June 1985. \nLIPSHITZ, S. P., T. C. SCOTT, AND J. VANDERKOOY, \"Increasing The Audio \nI \nMeasurement Capability Of FFT Analyzers By Microcomputer Postpro-\ncessing,\" Journal of \nthe Audio Engineering Society, Vol. 33, No.9, pp. \n626-48, September 1985. \nLIU, B., AND A. PELED, \"A New Hardware Realization of High-speed Fast \nG \nFourier Transformers,\" IEEE Trans. Acoust. Speech Signal Process., \nVol. ASSP-23, pp. 543-47, 1975. \nLIU, B., AND F. MINTZER, \"Calculation of Narrow-band Spectra By Direct \nT \nDecimation,\" IEEE Trans. Acoust. Speech Signal Process., Vol. ASSP-\n26, pp. 529-34, 1978. \nLIU, B., AND T. KANEKO, \"Roundoff Error in Fast Fourier Transforms (De-\nT \ncimation in Time),\" Proc. IEEE, Vol. 63, No.6, pp. 991-92, June 1975. \nLIU, B., Digital Filters and the Fast Fourier Transform, Stroudsburg, Penn-\nJ \nsylvania: Dowden, Hutchinson, and Ross, 1975. \nLoo, c., \"Calculations of Intermodulation Noise Due to Hard and Soft \nQ \nLimiting of MUltiple Carriers,\" IEEE International Con! on Commu-\nnications, IEEE, New York, June 1974. \nLOPRESTI, P. V., AND H. L. SURI, \"Fast Algorithm for the Estimation of \nK \nAutocorrelation Functions,\" IEEE Trans. Acoust. Speech Signal Pro-\ncess, Vol. ASSP-22, pp. 449-53, 1974. \nLORD, A., \"Understanding Dual-Channel FFT Measurements,\" Noise and \nV \nVih. Control Worldwide (GB), Vol. 12, No.6, pp. 241-45, September \n1981. \nLUCHINI, P., \"Two-Dimensional Numerical Integration Using a Square \nF \nMesh,\" Computer Physics Communications, Vol. 31, No.4, pp. 303-10, \nMarch 1984. \nMACTAGGART, I. R, AND M. A. JACK, \"Radix-2 FFT Butterfly Processor \nG \nUsing Distributed Arithmetic,\" Electronics Letters, Vol. 19, No.2, pp. \n43-44, January 20, 1983. \nMAKHOUL, J., \"A Fast Cosine Transform in One and Two Dimensions,\" \nT \nIEEE Trans. Acoust. Speech Signal Process., Vol. ASSP-28, No. I, pp. \n27-34, February 1980. \nMALIK, N. H., S. M. E. HAQUE, AND W. SHEPHERD, \"Analysis And Per-\nJ \nformance Of Three-Phase Phase-Controlled Thyristor AC Voltage Con-\ntrollers,\" IEEE Trans Ind Electron, Vol. IE-32, No.3, pp. 192-99, August \n1985. \nMARBLE, A. E., J. W. ASHE, D. H. K. TSANG, D. BELLIVEAU, AND D. N. \nA \nSWINGLER, \"Assessment of Algorithms Used to Compute The Fast Four-\n422 \nBibliography \nier Transform of Left Ventricular Pressure On A Microcomputer,\" M \ned-\nical & Biological Engineering & Computing, Vol. 23, No.2, pp. 190-94, \nMarch 1985. \nMARKEL, J. D., \"FFT Pruning,\" IEEE Trans. Audio Electroacoust. AU-\nT \n19, No.4, pp. 305-11, 1971. \nMARTINSON, L. W., AND R. J. SMITH, \"Digital Matched Filtering With Pipe-\nH \nline Floating Point Fast Fourier Transforms (FFT's),\" IEEE Trans. \nAcoust., Speech and Signal Process., Vol. ASSP-23, No.2, pp. 222-34, \nApril 1975. \nMARUHN, J. A., T. A. WELTON, AND C. Y. WONG, \"Remarks on the Nu-\nF \nmerical Solution of Poisson's Equation for Isolated Charge Distribu-\ntions,\" J. Comput. Phys., Vol. 20, No.3, pp. 326-35, March 1976. \nMATHEWS, J. D., \"Incoherent Scatter Radar Probing Of The 60-100KM At-\nH \nmosphere and Ionosphere,\" IEEE TRANS on Geoscience and Remote \nSensing, Vol. GE-24, No.5, pp. 765-76, September 1986. \nMCCLARY, W. K., \"Fast Seismic Inversion,\" Geophysics, Vol. 48, No. 10, \nC \npp. 1371-72, October 1983. \nMCCLELLAN, J. H. \"Multidimensional Spectral Estimation,\" Proceedings \nK \nof \nthe IEEE, Vol. 70, No.9, pp. 1029-39, September 1982. \nMcDoUGAL, J. R., L. C. SURRATT, AND L. F. STOOPS, \"Computer Aided \nE \nDesign of Small Superdirective Antennas Using Fourier Integral and Fast \nFourier Transform Techniques,\" SWIEECO Record, pp. 421-25, 1970. \nMEHALIC, M. A., P. L. RUSTAN, AND G. P. ROUTE, \"Effects of Architecture \nG \nImplementation on DFT Algorithm Performance,\" IEEE Trans. Acoust., \nSpeech & Signal Process., Vol. ASSP-33, No.3, pp. 684-93, June 1985. \nMEL'KANOVICK, A. F., L. M. KUSHKULEI, AND I. I. ARBIT, \"Examination \n0 \nOf The Spectral And Time Characteristics Of Information Signals Of Ul-\ntrasonic Flaw Detectors,\" Soviet Journal of \nNondestructive Testing, Vol. \n21, No.5, pp. 300-306, May 1985. \nMENCARAGLIA, F., AND V. NATALE, \"Method of Computing Fast Cosine \nT \nTransforms,\" Infrared Physics, Vol. 24, No.6, pp. 551-53, November \n1984. \nMENSA, D. L., \"Wideband Radar Cross Section Diagnostic Measure-\nH \nments,\" IEEE Transactions on Instrumentation and Measurement, Vol. \nIM-33, No.3, pp. 206-14, September 1984. \nMEQUIO, c., R. H. COURSANT, AND P. PESQUE, \"Simulation of the Acousto-\n0 \nElectric Response Of Piezoelectric Structures By Means of a Fast Four-\nier-Transform Algorithm,\" Acta Electronica, Vol. 25, No.4, pp. 311-\n23, 1983. \nMERHAUT, J., \"Impulse Measurement of Hom-type Loudspeaker Drivers,\" \n0 \nJ. Audio Eng. Soc., Vol. 34, No.4, pp. 245-54, April 1986. \nMERMELSTEIN, PAUL, \"Computer-generated Spectrogram Displays for On-\nR \nline Speech Research,\" IEEE Trans. Audio and Electroacoustics, Vol. \nAU-19, pp. 44-47, March 1971. \nMERSEREAU, R. M., AND T. C. SPEAKE, \"A Unified Treatment of Cooley-\nN \nBibliography \nTukey Algorithms For the Evaluation Of The Multidimensional DFT,\" \nIEEE Trans. Acoust. Speech and Signal Process., Vol. ASSP-29, No.5, \npp. 1011-18, October 1981. \n423 \nMETZ, L. S., AND O. O. GANDHI, \"Numerical Calculations of the Potential \nF \nDue to an Arbitrary Charge Density Using The Fast Fourier Transform,\" \nProc. IEEE, Vol. 62, No.7, pp. 1031-32, July 1974. \nMEYER, J. U., AND M. INTAGLIETIA, \"Measurement Of The Dynamics Of \nA \nArteriolar Diameter,\" Annals Of \nBiomedical Engineering, Vol. 14, No. \n2, pp. 109-17, April 1986. \nMIAN, G. A., AND A. P. NAINER, \"A Fast Procedure To Design Equiripple \nP \nMinimum-Phase Fir Filters,\" IEEE Trans. Circuits and Syst., Vol. CAS-\n29, No.5, pp. 327-31, May 1982. \nMILLER, T. J. E., AND P. J. LAWRENSON, \"Penetration Of \nTransient Magnetic \nJ \nFields Through Conducting Cylindrical Structures With Particular Ref-\nerence To Superconducting A. C. Machines,\" Proc. Inst. Electr. Eng. \n(GB), Vol. 123, No.5, pp. 437-43, May 1976. \nMILUTINOVIC, V., J. A. B. FORTES, AND L. H. JAMIESON, \"A Multimicro-\nG \nprocessor Architecture for Real-time Computation of a Class of DFT Al-\ngorithms,\" IEEE Trans. Acoust., Speech & Signal Process., Vol. ASSP-\n34, No.5, pp. 1301-1309, October 1986. \nMITCHELL, L. D., \"Improved Methods For The Fast Fourier Transform \nK \n(FFT) Calculation Of The Frequency Response,\" ASME Pap 8J-DET-8 \nfor MEET, p. 3, September 1981. \nMITIRA, R., AND W. L. Ko, \"New Techniques for Efficient Pattern Com-\nE \nputation of Aperture and Reflector Antennas,\" Electron. Lett. (GB), Vol. \n16, No. 14, pp. 549-51, July 3, 1980. \nMITIRA, R., W. L. Ko, AND M. S. SHESHADRI, \"A Novel Technique for the \nE \nComputation of Secondary Patterns of Reflector Antennas,\" Second In-\nternational Conference on Antennas and Propagation, 11481-5, Part I, \nHeslington, York, England, April 1981. \nMIYA, K., M. UESAKA, AND F. C. MOON, \"Finite Element Analysis of Vi-\nV \nbration of \nTorodial Field Coils Coupled With LaPlace Transform,\" Trans. \nASME J. Appl. Mech., Vol. 49, No.3, pp. 594-600, September 1982. \nMIYAKAWA, H., H. HARASHIMA, K. WATANABE, M. KISHI, AND K. OHYAMA, \nJ \n\"Detection of Multipath Reflected Waves By Two-Antenna Method,\" \nJournal of \nthe Institute of Television Engineers of \nJapan, Vol. 36, No. \n2, pp. 126-31, February 1982. \nMIYAKAWA, Y., N. MIKI, AND N. NAGAI, \"Adaptive Identification ofa Time-\nR \nvarying ARMA Speech Model,\" IEEE Trans. on Acoustics, Speech and \nSignal Processing, Vol. ASSP-34, No.3, pp. 423-33, June 1986. \nMIZUSHINA, S., Y. XIANG, AND T. SUGIURA, \"Large Waveguide Applicator \nA \nFor Deep Regional Hyperthermia,\" IEEE Transactions on Microwave \nTheory and Techniques, Vol. MTT-34, No.5, pp. 644-48, May 1986. \nMOHAN, M. V., AND V. V. RAO, \"EITor Analysis of Adpcm-FET,\" IEEE \nW \nTrans. Acoust. Speech and Signal Process., Vol. ASSP-27, No.4, pp. \n424-26, August 1979. \n424 \nBibliography \nMOHAN, R., AND C. CHEN-SHOU, \"Use of Fast Fourier Transforms in Cal-\nA \nculating Dose Distributions for Irregularly Shaped Fields for Three-di-\nmensional Treatment Planning,\" Med. Phys., Vol. 14, No.1, pp. 70-77, \nJanuary-February 1987. \nMOHARIR, P. S., \"Extending The Scope Of Golub's Method Beyond Com-\nF \nplex Multiplication,\" IEEE Trans Comput, Vol. C-34, No.5, pp. 484-\n87, May 1985. \nMOKRY, M., AND L. H. OHMAN, \"Application Of The Fast Fourier Trans-\nV \nform To Two-Dimensional Wind Tunnel Wall Interference,\" Journal of \nAircraft, Vol. 17, No.6, pp. 402-8, June 1980. \nMONTPETIT, M. J., M. NACHMAN, AND L. G. DURAND, \"Application of Nu-\nA \nmerical Methods for Feature Extraction From Phonopneumograms,\" J. \nClin. Eng., Vol. 10, No.4, pp. 339-45, October-December 1985. \nMOORER, J. A., \"Algorithm Design for Real-time Audio Signal Processing,\" \nT \nICASSP 84, Proceedings of \nthe IEEE International Conference on Acous-\ntics, Speech and Signal Processing, 12B.3/1-4, Vol. 1, San Diego, CA, \nMarch 1984. \nMORGERA, S. D., AND R. SANKAR, \"Digital Signal Processing for Precision \nL \nWide-swath Bathymetry,\" IEEE J. Oceanic Eng., Vol. OE-9, No.2, pp. \n73-84, April 1984. \nMORITANI, T., H. TANAKA, T. YOSHIDA, C. ISHII, T. YOSHIDA, AND M. \nA \nSHINDO, \"Relationship Between Myo-Electric Signals and Blood Lactate \nDuring Incremental Forearm Exercise,\" American Journal of \nPhysical \nMedicine, Vol. 63, No.3, pp. 122-32, June 1984. \nMUIR, R. A., D. M. CHABRIES, AND R. W. CHRISTIANSEN, \"Frequency 00-\n0 \nmain Compensation for Failed Elements in Linear Sonar Arrays,\" \nICASSP 86 Proceedings, IEEE-IECEJ-ASJ International Conference on \nAcoustics, Speech and Signal Processing, Vol. 3, pp. 1865-68, Tokyo, \nJapan, April 1986. \nMUNSON, D. C. JR., \"Floating Point Error Bound in the Prime Factor FFT,\" \nW \nICASSP 80 Proceedings, Part 1. IEEE International Conference on \nAcoustics, Speech and Signal Processing, pp. 69-72, Denver, CO, 1980. \nMUNSON, D. C., JR., AND B. LIU, \"Floating Point Roundoff Error In The \nW \nPrime Factor FFT,\" IEEE Transactions on Acoustics, Speech and Signal \nProcessing, Vol. ASSP-29, No.4, pp. 877-82, August 1981. \nMURO, M., A. NAGATA, K. MURAKAMI, AND T. MORITANI, \"Surface EMG \nA \nPower Spectral Analysis of Neuromuscular Disorders During Isometric \nand Isotonic Contractions,\" American Journal of Physical Medicine, \nVol. 61, No.5, pp. 244-54, October 1982. \nNACCARATO, D. F., AND Y. T. CHIEN, \"A Direct Two-Dimensional FFT \nN \nWith Applications In Image Processing,\" Proceedings of \nthe 1979 IEEE \nComputer Society Conference on Pattern Recognition and Image Pro-\ncessing, pp. 233-38, Chicago, IL, August 1979. \nNAGAI, K., \"Fourier Domain Reconstruction of Synthetic Focus Acoustic \n0 \nImaging System,\" Proceedings of \nthe IEEE, Vol. 72, No.6, pp. 748-49, \nJune 1984. \nBibliography \n425 \nNAGAI, K., \"Measurement of Time Delay Using the Time Shift Property \nL \nof the Discrete Fourier Transform (OFT),\" IEEE Transactions on Acous-\ntics, Speech and Signal Processing, Vol. ASSP-34, No.4, pp. 1006-8, \nAugust 1986. \nNAGAI, K., \"Pruning the Decimation-in-time FFT Algorithm with Fre-\nT \nquency-shift,\" IEEE Trans. on Acoustics, Speech and Signal Processing, \nVol. 34, No.4, pp. 1008-10, 1986. \nNANDAGOPAL, D., J. MAZUMDAR, R. E. BOGNER, AND E. GOLDBLATT, \"Spec-\nA \ntral Analysis of Second Heart Sound in Normal Children by Selective \nLinear Prediction Coding,\" Medical & Biological Engineering & Com-\nputing, Vol. 22, No.3, pp. 229-39, May 1984. \nNARASIMHA, M. J., AND A. M. PETERSON, \"Design of a 24-channel Trans-\nQ \nmultiplier,\" IEEE Trans. Acoust Speech Signal Process., Vol. ASSP-27, \npp. 752-62, 1979. \nNARASIMHA, M. J., AND A. M. PETERSON, \"On the Computation of the Dis-\nT \ncrete Cosine Transform,\" IEEE Trans. Commun., Vol. COM-26, pp. \n934-36, 1978. \nNARASIMHAN, M. S., AND M. KARTHIKEYAN, \"Evaluation of Fourier Trans-\nE \nform Integrals Using FFT With Improved Accuracy and Its Applica-\ntions,\" IEEE Transactions on Antennas and Propagation, Vol. AP-32, \nNo.4, pp. 404-8, April 1984. \nNAWAB, H., AND J. H. MCCLELLAN, \"Bounds On the Minimum Number of \nT \nData Transfers in WFTA and FFT Programs,\" IEEE Trans. Speech and \nSignal Process., Vol. ASSP-27, No.4, pp. 394-8, August 1979. \nN \nEBESNY, K. W., AND N. R. ARMSTRONG, \"Deconvolution of \nAuger Electron \nC \nSpectra For Lineshape Analysis and Quantitation using a Fast Fourier \nTransform Algorithm,\" J. Electron Spectrosc. & Relat. Phenom. (Neth-\nerlands), Vol. 37, No.4, pp. 355-73, March 1986. \nNEILL, T. B. M., \"Nonlinear Analysis of a Balanced Diode Modulator,\" \nM \nElectronics Letters (GB), Vol. 6, No.5, pp. 125-28, March 5, 1970. \nNESBET, R. K., AND D. C. CLARY, \"Fourier Transform Method For The \nF \nClassical Trajectory Problem,\" J. Chern. Phys., Vol. 71, No.3, pp. 1372-\n79, August 1, 1979. \nNESTER, W. H., \"The Fast Fourier Transform and the Butler Matrix,\" IEEE \nT \nTrans. Antennas Propagations, Vol. AP-16, p. 360, (correspondence), \nMay 1968. \nNEWLAND, D. E., Introduction to Random Vibrations and Spectral Anal-\nK \nysis, London England: Longman, 1975. \nNGUYEN, D. T., K. SWANN, AND J. R. McMILLAN, \"Microprogrammed Dig-\nR \nital Filter-Bank For Real-Time Spectral Analysis of Speech,\" Journal of \nElectrical and Electronics Engineering, Australia, Vol. 4, No.3, pp. 219-\n26, September 1984. \nNOBILE, A., AND V. ROBERTO, \"Efficient Implementation of Multidimen-\nT \nsional Fast Fourier Transforms On A Cray X-MP,\" Computer Physics \nCommunications, Vol. 40, No. 2-3 pp. 189-201, June 1986. \n426 \nBibliography \nNORTON, S. J., AND M. LINZER, \"Reconstructing Spatially Incoherent RANÂ· \n0 \nDom Sources In The Nearfield: Exact Inversion Formulas For Circular \nand Spherical Arrays,\" lournal of the Acoustical Society of America, \nVol. 76, No.6, pp. 1731-37, December 1984. \nNUGENT, S. T., AND J. P. FINLEY, \"Spectral Analysis of \nPeriodic AND Normal \nA \nBreathing In Infants,\" IEEE Transactions on Biomedical Engineering, \nVol. BME-30, No. 10, pp. 672-75, October 1983. \nNUSSBAUMER, H. J., AND P. QUANDALLE, \"Computation of Convolutions \nT \nand Discrete Fourier Transforms,\" IBM l. Res. Develop., Vol. 22, pp. \n134-44, 1978. \nNUSSBAUMER, H. J., AND P. QUANDALLE, \"Fast COMputation of Discrete \nT \nFourier Transforms Using Polynomial Transforms,\" IEEE Trans. Acoust. \nSpeech Signal Process., Vol. ASSP-27, pp. 169-81, 1979. \nNUSSBAUMER, H. J., \"Fast Computation of Discrete Fourier Transforms,\" \nT \nIBM Tech. Disclosure Bull., Vol. 22, No.1, pp. 149-50, June 1979. \nNUSSBAUMER, H. J., Fast Fourier Transform and Convolution Algorithms. \nJ \nBerlin, Germany and New York: Springer-Verlag, 1981. \nNUSSBAUMER, H. J., \"New Polynomial Transform Algorithms For Multi-\nP \ndimensional DFTS and Convolutions,\" IEEE Trans. Acoust., Speech and \nSignal Process., Vol. ASSP-29, No.1, pp. 74-84, February 1981. \nNUSSBAUMER, H. J., \"Polynomial Transform Implementation of Digital Fil-\nP \nter Banks,\" IEEE Trans. Acoust., Speech and Signal Process., Vol. \nASSP-31, No.3, pp. 616-22, June 1983. \nNUTIALL, A. H., \"Generation of Dolph-Chebyshev Weights Via A Fast \nP \nFourier Transform,\" Proc. IEEE, Vol. 62, No. 10, p. 1396, October 1974. \nNUTIALL, A. H., \"Some Windows with Very Good Sidelobe Behavior,\" \nK \nIEEE Trans. Acoust. Speech Signal Process., Vol. ASSP-29, pp. 84-87, \n1981. \nNUTIALL, ALBERT, H., \"Alternate Forms for Numerical Evaluation ofCu-\nF \nmulative Probability Distributions Directly from Characteristic Func-\ntions,\" Proceedings of \nthe IEEE, Vol. 58, pp. 1872-73, November 1970. \nNWACHUKWU, E. 0., \"Address Generation In An Array Processor,\" IEEE \nG \nTransactions On Computers, Vol. C-34, No.2, pp. 170-73, February \n1985. \nO'LEARNY, G. C., \"Nonrecursive Digital Filtering Using Cascade Fast \nP \nFourier Transformers,\" IEEE Trans. Audio AND Electroacoustics, Vol. \nAU-18, pp. 177-83, June 1970. \nOKINO, M., AND Y. HIGASHI, \"Measurement of Seabed Topography By Mul-\n0 \ntibeam Sonar Using CFFT,\" IEEE l. Oceanic Eng., Vol. OE-ll, No.4, \npp. 474-9, October, 1986. \nOMER, W., \"Faster Fourier Transforms,\" Electron. and Wireless World \nJ \n(GB), Vol. 92, No. 1605, pp. 57-58, July 1986. \nONO, T., \"FFT Analyzers Draw More Data From a Single Input Signal,\" \nI \nlEE, lournal of \nElectronic Engineering, Japan, Vol. 20, No. 200, pp. 86-\n88, August 1983. \nBibliography \nONO, T., \"Field-Oriented Faster FFT Analyzers Wanted,\" JEE, Journal \nof \nElectronic Engineering, Vol. 23, No. 230, pp. 52-55, February 1986. \n427 \nONOE, M., \"A Method for Computing Large-scale Two-dimensional Trans-\nN \nform Without Transposing Data Matrix,\" Proc. IEEE, Vol. 63, No.1, \npp. 196-97, 1975. \nOPPENHEIM, A. V., AND C. J. WEINSTEIN, \"A Bound on the Output of a \nP \nCircular Convolution with Application to Digital Filtering,\" IEEE Trans. \nAudio and Electroacoustics, Vol. AU-17, pp. 120-24, June 1969. \nOPPENHEIM, A. V., AND C. J. WEINSTEIN, \"Effects of Finite Register Length \nW \nin Digital Filtering and the Fast Fourier Transform,\" Proc. IEEE, Vol. \n60, No.8, pp. 957-76, August 1972. \nOPPENHEIM, A. V., AND R. W. SCHAFER, Digital Signal Processing. Engle-\nJ \nwood Cliffs, New Jersey: Prentice Hall, 1975. \nOPPENHEIM, A. V., \"Speech Spectrogram Using the Fast Fourier Trans-\nR \nform,\" IEEE Spectrum, Vol. 7, pp. 57-62, August 1970. \nOPPENHEIM, A. V., D. JOHNSON, AND K. STEIGLlTZ, \"Computation of \nSpectra \nK \nwith Unequal Resolution Using the Fast Fourier Transform,\" Proc. \nIEEE, Vol. 59, pp. 299-301, February 1971. \nORFANIDIS, S. J., AND T. G. MARSHALL, \"Two-Dimensional Transforms Of \nX \nThe Sample National Television Systems Committee (NTSC) Color \nVideo Signal,\" Optical Engineering, Vol. 20, No.3, pp. 417-20, June \n1981. \nOSAKA, T., AND Y. YATSUDA, \"Study on Time-Dependence of the Oxygen \nJ \nEvolution Reaction On Nickel by FFT Impedance Measurement,\" Elec-\ntrochimica Acta, Vol. 29, No.5, pp. 677-81, May 1984. \nOSINSKll, L. M., AND O. V. GLUSHKO, \"Fast Fourier Transform Pipeline \nG \nSchemes with Arbitrary Overlap oflnput Data Arrays,\" Radioelectron. \n& Commun. Syst., Vol. 29, No.1, pp. 34-39, 1986. \nOVSYANIK, V. P., L. S. KOVALENKO, AND A. N. VOVCHINSKll, \"Investigation \nA \nOf Temporal And Spectral Characteristics Of Some Signals Of Artifacts \nAnd Background Bioelectrical Activity,\" Biomedical Engineering, Vol. \n19, No.1, pp. 4-7, January-February 1985. \nPAPOULlS, A., \"A New Algorithm In Spectral Analysis AND BAND-Limited \nK \nExtrapolation,\" IEEE Trans. Circuits AND Syst., Vol. CAS-22, No.9, \npp. 735-42, September 1975. \nPARAMANAND, S., AND P. RAMAKRISHNAN, \"Powder Signature-A Strategy \nJ \nFor Powder Characterization By Fourier Analysis,\" International Jour-\nnal of \nPowder Metallurgy and Powder Technology, Vol. 21, No.2, pp. \n111-18, April 1985. \nPARKER, R., AND S. A. T. STONEMAN, \"On The Use Of Fast Fourier Trans-\n0 \nforms When High Frequency Resolution Is Required,\" Journal of \nSound \nand Vibration, Vol. 104, No.1, pp. 75-79, January 1986. \nPARMENTER, W. W., AND R. G. CHRISTIANSEN, \"Recovery of Modal Infor-\nV \nmation From a Beam Undergoing Random Vibration,\" Trans. ASME Ser. \nB., Vol. 96, No.4, pp. 1307-13, November 1974. \n428 \nBibliography \nPASTERKAMP, H., R. FENTON, F. LEAHY, AND V. CHERNICK, \"Spectral Anal-\nA \nysis of Breath Sounds In Normal Newborn Infants,\" Medical Instru-\nmentation, Vol. 17, No.5, pp. 355-57, September-October 1983. \nPEARL, J., \"On Coding and Filtering Stationary Signals By Discrete Fourier \nQ \nTransforms,\" IEEE Trans. Informat. Theory, Vol. 1T-19, pp. 229-32, \n1973. \nPEARSON, A. E., AND F. C. LEE, \"On The Identification Of Polynomial \nF \nInput-Output Differential Systems,\" IEEE Trans. Autom. Control, Vol. \nAC-30, No.8, pp. 778-82, August 1985. \nPEARSON, A. E., AND F. C. LEE, \"Parameter Identification Of Linear Dif-\nF \nferential Systems Via Fourier Based Modulating Functions,\" Control \nTheory Adv. Technol., Vol. 1, No.4, pp. 239-66, December 1985. \nPEDERSEN, J. E., \"Fast Dedicated Microprocessor For Real-Time Fre-\nA \nquency Analysis of Ultrasonic Blood-Velocity Measurements,\" Medical \n& Biological Engineering & Computing, Vol. 20, No.6, pp. 681-86, No-\nvember 1982. \nPEl, S-C, AND E-F HUANG, \"In-Order, Partially In-Place Mixed Radix FFT \nT \nAlgorithm,\" IEEE Transactions on Acoustics, Speech, and Signal Pro-\ncessing, Vol. ASSP-31, No.5, pp. 1314-17, October 1983. \nPEl, S-C, AND J-L Wu, \"Split-Radix Fast Hartley Transform,\" Electronics \nT \nLetters, Vol. 22, No.1, pp. 26-27, January 1986. \nPEl, SOO-CHANG, AND SHEN-TAN Lu, \"Design of Minimum-Phase Fir Digital \nP \nFilters by Differential Cepstrum,\" IEEE Transactions on Circuits and \nSystems, Vol. CAS-33, No.5, pp. 570-76, May 1986. \nPELED, A., AND S. WINOGRAD, \"TDM-FDM Conversion Requiring Reduced \nQ \nComputational Complexity,\" IEEE Trans. Commun., Vol. COM-26, pp. \n707-19, 1978. \nPERERA, W. A., AND P. J. W. RAYNER, \"Optimal Design of Discrete Coef-\nT \nficient DFTs for Spectral Analysis, Extension To Multiplierless FFTs,\" \nIEEE Proceedings, Part G: Electronic Circuit and Systems, Vol. 133, \nNo.1, pp. 8-18, February 1986. \nPETERS, W. N., \"Applications of the Two-Dimensional Fast Fourier Trans-\nZ \nform For Optical Systems Analysis,\" Proceedings of the Society of \nPhoto-Optical Instrumentation Engineers, Vol. 193, pp. 70-77, August \n1979. \nPICKERING, W. M., \"On The Solution Of Poisson Equation On A Regular \nF \nHexagonal Grid Using FFT Methods,\" Journal of \nComputational Phys-\nics, Vol. 64, No.2, pp. 320-33, 1986. \nPIN KOWITZ, D., \"Fast Fourier Transform Speeds Signal-to-noise Analysis \nfor AID Converter,\" Digital Design, Vol. 16, No.6, pp. 64-66, May 1986. \nPOLGE, R. J., AND B. K. BHAGAVAN, \"Efficient Fast Fourier Transform \nT \nPrograms for Arbitrary Factors With One Step Loop Unscrambling,\" \nIEEE Trans. Comput., Vol. C-25, No.5, pp. 534-39, May 1976. \nPOLGE, R. J., AND E. R. McKEE, \"Extension of Radix-2 Fast Fourier Trans-\nT \nform (FFT) Program to Include a Prime Factor,\" IEEE Trans. Acoust \nSpeech Signal Process., Vol. ASSP-22, No.5, pp. 388-89, October 1974. \nBibliography \n429 \nPOLGE, R. J., B. K. BHAGAVAN, ANDJ. M. CARSWELL, \"Fast Computational \nT \nAlgorithms For BIT Reversal,\" IEEE Trans. Comput., Vol. C-23, No. \n1, pp. 1-9, January 1974. \nPOMERLEAU, A., \"Real-Data FFT Algorithm For Image Processing Appli-\nN \ncations,\" Canadian Electrical Engineering Journal, Vol. 8, No.2, pp. \n65-72, April 1983. \nPOMERLEAU, A., H. L. BUIJS, AND M. FOURNIER, \"A Two-pass Fixed Point \nW \nFast Fourier Transform Error Analysis,\" IEEE Trans. Acoust Speech \nSignal Process., Vol. ASSP-25, pp. 582-85, 1977. \nPOMERLEAU, A., M. FOURNIER, AND H. L. BUIJS, \"On the Design of a Real \nG \nTime Modular FFT Processor,\" IEEE Trans. Circuits Syst., Vol. CAS-\n23, pp. 630-33, 1976. \nPORTNOFF, M. R., \"Implementation of the Digital Phase Vocoder Using The \nR \nFast Fourier Transform,\" IEEE Trans. Acoust., Speech AND Signal Pro-\ncess., Vol. ASSP-24, No.3, pp. 243-48, June 1976. \nPORTNOFF, M. R., \"Time-frequency Representation of Digital Signals and \nK \nSystems Based on Short-time Fourier Analysis,\" IEEE Trans. Acoust. \nSpeech Signal Process., Vol. ASSP-28, No.1, pp. 55-69, February 1980. \nPRABHU, A. V., V. K. AATRE, T. SOUMINI, AND S. A. KARIPEL, \"Frequency \nT \nZooming Techniques For High Resolution Spectrum Analysis,\" Defense \nScience Journal (India), Vol. 35, No.3, pp. 281-85, July 1985. \nPRAKASH, S., AND V. V. RAO, \"A New Radix-6 FFT Algorithm,\" IEEE \nT \nTrans. Acoust. Speech and Signal Process., Vol. ASSP-29, No.4, pp. \n939-41, August 1981. \nPRAKASH, S., AND V. V. RAO, \"Fixed-Point Error Analysis of \nRadix-4 FFT,\" \nW \nSignal Processing, Vol. 3, No.2, pp. 123-33, April 1981. \nPRAKASH, S., AND V. V. RAO, \"Vector Radix FFT Error Analysis,\" IEEE \nW \nTransactions on Acoustics, Speech, and Signal Processing, Vol. ASSP-\n30, No.5, pp. 808-11, October 1982. \nPRASAD, K. P., AND P. SATYANARAYANA, \"Fast Interpolation Algorithm \nT \nUsing FFT,\" Electronics Letters (GB), Vol. 22, No.4, pp. 185-87, Feb-\nruary 1986. \nPRESCOTT, J., AND R. L. JENKINS, \"An Improved Fast FOUI;er Transform,\" \nT \nIEEE Trans. Acoust Speech Signal Process., Vol. ASSP-22, No.3, pp. \n226-27, June 1974. \nPREUSS, R. D. \"Very Fast Computation of the Radix-2 Discrete Fourier \nT \nTransform,\" IEEE Transactions on Acoustics, Speech, and Signal Pro-\ncessing, Vol. ASSP-30, No.4, pp. 595-607, August 1982. \nPRICE, E. V., \"The Fast Fourier Transform on a Digital Image Processor-\nX \nImplementation and Applications,\" Architecture and Algorithmsfor Dig-\nital Image Processing, San Diego, CA, August 1983. \nPRIDHAM, R. G., AND R. E. KOWALCZK, \"Use of FFT Subroutine in Digital \nP \nFilter Design Program,\" Proceedings of \nthe IEEE (Letters), Vol. 57, p. \n106, January 1969. \n430 \nBibliography \nPRIESTLEY, B., \"Fast Fourier-Transform in Basic,\" ELectronic Engineering, \nT \nVol. 58, No. 711, pp. 33-34, 1986. \nPROKOP'EV, A.I., \"Use of Fast Fourier Transform in Calculation of \nPotential \nJ \nand Field Distributions in Gallium Arsenide Charge-coupled Devices in \nthe Absence of Mobile Charge,\" RadioeLectron. & Commun. Syst., Vol. \n28, No.6, pp. 62-66, 1985. \nQUIRK, M., AND B. LIU, \"On Narrow-BAND Spectrum Calculation By Direct \nK \nDecimation,\" ICASSP 81. Proceedings of the 1981 IEEE InternationaL \nConference on Acoustics, Speech and SignaL Processing, Vol. 1, pp. 85-\n88, Atlanta, GA, March 1981. \nRAABE, H. P., \"Fast Beamforming with Circular Receiving Arrays,\" IBM \nB \nJ. Res. and Dev., Vol. 20, No.4, pp. 398-408, July 1976. \nRABINER, L. R., AND B. GOLD, Theory and Application of \nDigitaL SignaL \nJ \nProcessing. Englewood Cliffs, N.J.: Prentice Hall, 1975. \nRABINER, L. R., R. W. SCHAFER, AND C. M. RADER, \"The Chirp-Z Transform \nT \nAlgorithm,\" IEEE Trans. Audio. ELectroacoust., Vol. AU-17, No.2, pp. \n86-92, June 1969. \nRADCLIFFE, C. J., AND C. D. MOTE, JR., \"Identification AND Control ofRo-\nV \ntating Disk Vibration,\" Trans. ASME J. Dyn. Syst. Meas. and ControL, \nVol. 105, No.1, pp. 39-45, March 1983. \nRADER, C. M., AND N. M. BRENNER, \"A New Principle For Fast Fourier \nT \nTransformation,\" IEEE Trans. Acoust., Speech and SignaL Process., \nVol. ASSP-24, No.3, pp. 268-70, June 1976. \nRADER, C. M., \"An Improved Algorithm for High Speed Autocorrelation \nT \nwith Applications to Spectral Estimation,\" IEEE Trans. Audio \nand \nELectro-\nacoustics, Vol. 18, pp. 439-41, December 1970. \nRADER, C. M., \"Discrete Fourier Transforms When the Number of Data \nT \nSamples is Prime,\" Proc. IEEE (Letters), Vol. 56, pp. 1107-1108, June \n1968. \nRAJALA, S. A., A. N. RIDDLE, AND W. E. SNYDER, \"Application ofthe One-\nX \nDimensional Fourier Transform for Tracking Moving Objects In Noisy \nEnvironments,\" Comput. Vision, Graphics and Image Process., Vol. 21, \nNo.2, pp. 280-93, February 1983. \nRAJAONA, R. D., AND P. SULMONT, \"A Method of \nSpectral Analysis Applied \nK \nto Periodic and Pseudoperiodic Signals,\" J. Comput. Phys., Vol. 61, No. \nI, pp. 186-93, October 1985. \nRAMIREZ, R. W., \"Fast Fourier Transform Makes Correlation Simpler,\" \nP \nELectronics, Vol. 48, No. 13, pp. 98-103, June 1975. \nRAMIREZ, R. W., \"The FFT: Fundamentals and Concepts,\" Englewood \nJ \nCliffs, NJ: Prentice Hall, 1985. \nRAo, P. N., AND G. BOOPATHY, \"Analysis and Classification of Commu-\nQ \nnication Signals,\" Defense Science JournaL (India), Vol. 35, No.3, pp. \n367-74, July 1985. \nREAD, R., AND J. MEEK, \"Digital Filters with Poles Via FFT,\" IEEE Trans. \nP \nAudio and ELectroacoustics, Vol. AU-19, pp. 322-23, December 1971. \nBibliography \n431 \nREDDY, B. R. S., AND I. S. N. MURTHY, \"ECG Data Compression Using \nA \nFourier Descriptor,\" IEEE Trans. Biomed. Eng., Vol. BME-33, No.4, \npp. 428-34, April 1986. \nREDDY, D. C., K. S. RAO, AND K. J. R. MURTY, \"Waveform Analysis For \nA \nthe Detection Of Airways Obstruction In Man,\" Medical & Biological \nEngineering & Computing, Vol. 22, No.6, pp. 481-85, November 1984. \nREDDY, N. S., AND M. N. S. SWAMY, \"Resolution of Range and Doppler \nH \nAmbiguities in Medium PRF Radars in Multiple-target Environment,\" \nICASSP 84, Proceedings of \nthe IEEE International Conference on Acous-\ntics, Speech and Signal Processing, Vol. 3, pp. 47.61l-4, March 1984. \nREDDY, R. S., I. S. N. MURTHY, AND P. C. CHATTERJEE, \"Rhythm Analysis \nA \nUsing Vectorcadiograms,\" IEEE Transactions on Biomedical Engineer-\ning, Vol. BME-32, No.2, pp. 97-104, February 1985. \nREDDY, V. U., AND M. SUNDARAMURTHY, \"Effect of Correlation Between \nT \nTruncation Errors on Fixed-Point Fast Fourier Transform Error Analy-\nsis,\" IEEE Transactions on Circuits and Systems, Vol. CAS-27, No.8, \npp. 712-16, August 1980. \nREDINBO, G. R., AND K. K. RAo, \"Expediting Factor-type Fast Finite Field \nT \nTransform Algorithms,\" IEEE Trans.lnf Theory, Vol. 1T-32, No.2, pp. \n186-94, March 1986. \nREED, F. A., AND P. L. FEINTUCH, \"A Comparison of LMS Adaptive Can-\nQ \ncellers Implemented In The Frequency Domain And The Time Domain,\" \nIEEE Trans. Circuits and Syst., Vol. CAS-28, No.6, pp. 610-15, June \n1981. \nREED, I. S., T. K. TRUONG, B. BENJAUTHRIT, AND C. Wu, \"A Fast Algorithm \nT \nFor Computing A Complex-Number Theoretic Transform For Long Se-\nquences,\" IEEE Trans. Acoust., Speech and Signal Process., Vol. ASSP-\n29, No.1, pp. 122-24, February 1981. \nRENDERS, H., J. SCHOUKENS, AND G. VILAIN, \"High-Accuracy Spectrum \nK \nAnalysis of Sampled Discrete Frequency Signals By Analytical Leakage \nCompensation,\" IEEE Transactions on Instrumentation and Measure-\nment, Vol. IM-33, No.4, pp. 287-92, December 1984. \nRENNIE, L. J., \"The Tap III Beamforming System,\" IEEE; San Diego Sec-\nB \ntion of \nthe Marine Techno!. Soc. Oceans '79, pp. 6-13, San Diego, CA, \nSeptember 1979. \nREQUICHA, A. A. G., \"Direct Computation of Distribution Functions From \nF \nCharacteristic Functions Using the Fast Fourier Transform,\" Proceed-\nings of \nthe IEEE, Vol. 58, No.7, pp. 1154-55, July 1970. \nRESCH, F. J., AND R. ABEL, \"Spectral Analysis Using Fourier Transform \nK \nTechniques,\" Int. J. Numer. Methods Eng. (GB), Vol. 9, No.4, pp. 869-\n902, 1975. \nRIAD, S. M., AND R. B. STAFFORD, \"Impulse Response Evaluation Using \nC \nFrequency Domain Optimal Compensation Deconvolution,\" 23rd Mid-\nwest Symposium on Circuits AND Systems, pp. 521-5, 1980. \nRIBLET, G. P., \"Use Of The FFT To Speed Analysis Of Planar Symmetrical \nE \n432 \nBibliography \n3- AND 5-Ports By The Integral Equation Method,\" IEEE Transactions \non Microwave Theory and Techniques, Vol. MTT-33, No. 10, pp. 1073-\n75, October 1985. \nRICHARDS, T. L., AND K. ATTENBOROUGH, \"Accurate FFT Based Hankel \n0 \nTransforms for Predictions of Outdoor Sound Propagation,\" J. Sound \nand Vih. (GB), Vol. 109, No.1, pp. 157-67, August 1986. \nRIEDEL, N. K., D. A. McANINCH, C. FISHER, AND N. B. GOLDSTEIN, \"Signal \nG \nProcessing Implementation For An IBM-PC-Based Workstation,\" IEEE \nMicro, Vol. 5, No.5, pp. 52-67, October 1985. \nRITTGERS, S. E., W. W. PUTNEY, AND R. W. BARNES, \"Real-Time Spectrum \nA \nAnalysis and Display of Directional Doppler Ultrasound Blood Velocity \nSignals,\" IEEE Transactions on Biomedical Engineering, Vol. BME-27, \nNo. 12, pp. 723-28, December 1980. \nROBERTS, K. B., P. D. LAWRENCE, AND A. EISEN, \"Dispersion of the So-\nA \nmatosensory Evoked Potential (SEP) in Multiple Sclerosis,\" IEEE Trans-\nactions on Biomedical Engineering, Vol. BME-30, No.6, pp. 360-64, \nJune 1983. \nROBINSON, E. A., \"Historical Perspective of Spectrum Estimation,\" Pro-\nE \nceedings of \nthe IEEE, Vol. 70, No.9, pp. 885-907, September 1982. \nROBINSON, E. A., T. S. DURRANI, AND L. G. PEARDON, Geophysical Signal \nU \nProcessing. Englewood Cliffs, NJ: Prentice Hall, 1986. \nRODDY, D., \"A Method of Using Simpson's Rule in the OFT,\" IEEE Trans. \nF \nAcoust. Speech and Signal Process., Vol. ASSP-29, No.4, pp. 936-37, \nAugust 1981. \nROSTE, T., O. HAABERG, AND T. A. RAMSTAD, \"A Radix-4 FFT Processor \nS \nFor Application in a 60-Channel Transmultiplexer Using TTL Technol-\nogy,\" IEEE Trans. Acoust., Speech and Signal Process., Vol. ASSP-27, \nNo.6, Pt. 2, pp. 746-51, December 1979. \nROTHWEILLER, J. H., \"Implementation of the In-Order Prime Factor Trans-\nT \nform For Variable Sizes,\" IEEE Transactions on Acoustics, Speech, and \nSignal Processing, Vol. ASSP-30, No.1, pp. 105-7, February 1982. \nROWLANDS, R. 0., \"The Odd Discrete Fourier Transform,\" Proc. IEEE Int. \nT \nConf. Acoust. Speech Signal Process., Philadelphia, Pennsylvania, pp. \n130-33, 1976. \nRUDNICK, P., \"Digital Beamforming in the Frequency Domain,\" Journal of \nB \nthe Acoustical Society of \nAmerica, Vol. 46, No.5, pp. 1089-90, Novem-\nber 1969. \nRUSSEL, R. F., D. P. GAINES, AND F. W. SEDENQUIST, \"Improved Radar \nH \nRange Resolution Using Frequency Agility and the Fast Fourier Trans-\nform,\" Conference Proceedings of \nIEEE Southeastcon 84, pp. 261-65, \nApril 1984. \nRUSSEL, R. F., F. W. SEDENQUIST, AND D. P. GAINES, \"Frequency Agile/ \nH \nPolarimetric Radar-simulation and Testing,\" Proceedings of the 1984 \nIEEE National Radar Conference, pp. 58-62, Atlanta, GA, March 1984. \nBibliography \n433 \nRUSSELL, R. F., AND F. W. SEDENQUIST, \"Digital Simulation of Polarimetric \nZ \nRadars,\" Simulation (USA), Vol. 43, No.5, pp. 242-46, November 1984. \nSABLlK, M. J., R. E. BEISSNER, AND A. CHOY, \"An Alternative Numerical \nE \nApproach for Computing Eddy Currents: Case of the Double-layered \nPlate,\" IEEE Trans. Magn., Vol. MAG-20, No.3, pp. 500-506, May \n1984. \nSADJADI, F. A., J. J. HWANG, E. L. HALL, AND M. J. ROBERTS, \"Measure-\nL \nment of Two Phase Flow Velocities Using Image Correlation,\" IEEE \nProceedings of \nthe 5th International Conference on Pattern Recognition, \npp. 386-92, Miami Beach, FL, December 1980. \nSAID, S. M., AND K. R. DIMOND, \"Improved Implementation of FFT AI-\nG \ngorithm on a High-Performance Processor,\" Electronics Letters, Vol. 20, \nNo.8, pp. 347-49, April 12, 1984. \nSAKAMOTO, T., T. SUGIMOTO, AND M. NAKAMURA, \"Clutter Rejection Signal \nH \nProcessor Using High Speed FFT For Radar System,\" Noise and Clutter \nRejection in Radars and Imaging Sensors. Proceedings of the 1984 In-\nternational Symposium, pp. 518-21, Tokyo, Japan, October, 1984. \nSAKURAI, K., K. KOGA, AND T. MURATANI, \"Speech Scrambler Using the \nR \nFast Fourier Transform Technique,\" IEEE Journal on Selected Areas in \nCommunications, Vol. SAC-2, No.3, pp. 434-42, May 1984. \nSALA, K. L., R. W. YIP, R. LESAGE, \"Application of Fast Fourier Transform \nD \nand Convolution Techniques To Picosecond Continuum Spectroscopy,\" \nApplied Spectroscopy, Vol. 37, No.3, pp. 273-79, May-June 1983. \nSANKARAN, R., K. A. MURALEEDHARAN, AND K. P. P. PILLAI, \"Transient \nY \nPerformance of Linear Induction Machines Following Reconnection of \nSupply,\" Proc. Inst. Electr. Eng. (GB), Vol. 126, No. 10, pp. 979-83, \nOctober 1979. \nSARKAR, T. K., E. ARVAS, AND S. M. RAO, \"Application Of FFT And The \nE \nConjugate Gradient Method For The Solution Of Electromagnetic Ra-\ndiation From Electrically Large And Small Conducting Bodies,\" IEEE \nTransactions on Antennas and Propagation, Vol. AP-34, No.5, pp. 635-\n40, May 1986. \nSAZONOV, N. A., \"Quantization Noise for Digital Signal Processing By Har-\nH \nmonic Analysis in an Aperture-synthesis Radar System,\" Telecommun. \nand Radio Eng. Part 2, Vol. 40, No.4, pp. 59-62, April 1985. \nSCHAFER, R. W., AND L. R. RABINER, \"Design and Simulation of a Speech \nR \nAnalysis-synthesis System Based on Short-time Fourier Analysis,\" IEEE \nTrans. Audio Electroacoust., Vol. AU-21, pp. 165-74, 1973. \nSCHAFFER, J. P., E. J. SHAUGHNESSY, AND P. L. JONES, \"The Deconvolution \nC \nOf Doppler-Broadened Positron Annihilation Measurements Using Fast \nFourier Transforms and Power Spectral Analysis,\" Nuclear Instruments \n& Methods In Physics Research, Section B: (Netherlands) Beam Inter-\nactions with Materials and Atoms, Vol. 233, No. I, pp. 75-79, Sep-\ntember-October 1984. \n434 \nBibliography \nSCHAFFER, J. P., AND P. L. JONES, \"An Evaluation of the Fast Fourier-\nC \nTransform Power Spectrum Deconvolution Method As Applied To Dop-\npler-Broadened Positron-Annihilation Spectra of High-Purity Alumi-\nnum,\" Journal of \nPhysics F-Metal Physics, (GB), Vol. 16, No. ll, pp. \n1885-96, November 1986. \nSCHEUERMAN, H., AND H. GOCKLER, \"A Comprehensive Survey of Digital \nQ \nTransmultiplexing Methods,\" Proc. IEEE, Vol. 69, No. 69, pp. 1419-50, \nNovember 1981. \nSCHLEHER, D. C. \"Numerical Evaluation of Logarithmic Receiver Thresh-\nY \nolds,\" Electron. Lett. (GB), Vol. 16, No. 23, pp. 875-76, November 6, \n1980. \nSCHREIER, P. G., \"PC-based Spectrum Analysis Packages Reach Toward \nG \nDSP-Chip Performance,\" Electron. Test, Vol. 8, No.8, pp. 39-44, Au-\ngust 1985. \nSCHUTTE, J., \"New Fast Fourier Transform Algorithm For Linear System \nD \nAnalysis Applied In Molecular Beam Relaxation Spectroscopy,\" Rev. \nSci. Instrum., Vol. 52, No.3, pp. 400-404, March 1981. \nSEBERN, M. J., J. D. HORGAN, R. C. MEADE, C. M. KRONENWETTER, P. P. \nX \nRUETZ, AND EN-LIN YEH, \"Minicomputer Enhancement of Scintillation \nCamera Images Using Fast Fourier Transform Techniques,\" J. Nucl. \nMed., Vol. 17, No.7, pp. 647-52, July 1976. \nSERDA, L. A., \"Fast Two-Dimensional Discrete Fourier Transform,\" \nN \nTrans. in: Radioelectron. and Commun. Syst., Vol. 26, No.7, pp. 15-\n19, 1983. \nSEVERUD, L. K., M. J. ANDERSON, AND D. A. BARTA, \"Seismic Damping \nV \nFactors Of Small Bore Piping As Influenced By Insulation and Support \nElements,\" Journal of \nPressure Vessel Technology, Transactions of \nthe \nASME, Vol. 107, No.2, pp. 142-47, May 1985. \nSHAARAWI, A. M., AND S. M. RIAD, \"Computing the Complete FFT of a \nF \nStep-like Waveform,\" IEEE Trans. Instrum. & Meas., Vol. IM-35, No. \nI, pp. 91-92, March 1986. \nSHANKARAREDDY, B. R., AND I. S. N. MURTHY, \"ECG Data Compression \nA \nUsing Fourier Descriptors,\" IEEE Transactions on Biomedical Engi-\nneering, Vol. BME-33, No.4, pp. 428-34, April 1986. \nSHAYG, S. A., AND Y. H. HAN, \"Rayleigh Spectrometer,\" Rev. Sci. In-\nD \nstrum., Vol. 45, No.2, pp. 280-85, February 1974. \nSHCHERBAKOV, M. A., \"Identification of Discrete Nonlinear Systems With \nJ \nPseudorANDom Inputs,\" Soviet Automatic Control, Vol. 16, No.4, pp. \n16-26, July-August 1983. \nSHEROV, E. M., AND V. A. MAMONTOV, \"Interpolation with the Aid of Fast \nD \nFourier Transforms in Fourier Spectroscopy,\" Trans. J. Appl. Spectro., \nVol. 24, No.6, June 1976. \nSHIOJIRI, E., AND Y. FUJII, \"Transmission Capability of An Optical Fiber \nE \nCommunication System Using Index Nonlinearity,\" Applied Optics, Vol. \n24, No.3, pp. 358-60, February 1985. \nBibliography \n435 \nSHIRLEY, R. S., \"Application of a Modified Fast Fourier Transform to Cal-\nJ \nculate Human Operator Describing Functions,\" IEEE Trans. Man-Ma-\nchine Systems, Vol. MMS-IO, pp. 140-44, December 1969. \nSHUNI C., AND C. S. BURRUS, \"A Prime Factor FFT Algorithm Using Dis-\nT \ntributed Arithmetic,\" IEEE Trans. Acoust. Speech and Signal Process., \nVol. ASSP-30, No.2, pp. 217-27, April 1982. \nSILBERBERG, M., \"Improving the Efficiency of Laplace-transform Inversion \nM \nfor Network Analysis,\" Electronics Letters (GB), Vol. 6, No.4, pp. 105-\n6, February 19, 1970. \nSILVERMAN, H. F., \"A High-quality Digital Filterbank for Speech Recog-\nR \nnition Which Runs in Real Time on a Standard Microprocessor,\" IEEE \nTrans. Acoust., Speech & Signal Process., Vol. ASSP-34, No.5, pp. \n1064-73, October 1986. \nSILVERMAN, H. F., A. E. PEARSON, \"On Deconvolution using the Discrete \nC \nFourier Transform,\" IEEE Transactions on Audio and Electroacoustics, \nVol. AU-21 , No.2, pp. 112-18, April 1973. \nSINGHAL, K., \"Interpolation Using the Fast Fourier Transform,\" Proceed-\nF \nings of \nthe IEEE, Vol. 60, No. 12, p. 1558, December 1972. \nSINGLETON, R. c., AND T. C. POULTER, \"Spectral Analysis of the Call of \nK \nthe Male Killer Whale,\" IEEE Trans. Audio and Electroacoustics, Vol. \nAU-15, No.2, pp. 104-13, June 1967. \nSINGLETON, R. C., \"A Method for Computing the Fast Fourier Transform \nT \nwith Auxiliary Memory and Limited High-speed Storage,\" IEEE Trans. \nAudio Electroacoust., Vol. AU-15, pp. 91-98, June 1967. \nSINGLETON, R. C., \"Algol Procedures for the Fast Fourier Transform,\" \nT \nCommun. ACM, Vol. 11, No. 11, pp. 773-76, Algorithm 338, November \n1968. \nSINGLETON, R. c., \"An Algol Procedure for the Fast Fourier Transform \nT \nwith Arbitrary Factors,\" Commun. ACM, Vol. 11, pp. 776-79, Algorithm \n339, November 1968. \nSINGLETON, R. C., \"An Algorithm for Computing the Mixed Radix Fast \nT \nFourier Transform,\" IEEE Trans. Audio Electroacoust., Vol. AU-17, \nNo.2, pp. 93-103, June 1969. \nSINGLETON, R. c., \"On Computing the Fast Fourier Transform,\" C \nommun. \nT \nACM. Vol. 10, pp. 647-54, October 1967. \nSINHA, B., J. DATTAGUPTA, AND A. SEN, \"Improvement In The Speed Of \nT \nFFT Processors Using Segmented Memory And Parallel Arithmetic \nUnits,\" Signal Processing, Vol. 8, No.2, pp. 267-74, April 1985. \nSKARJUNE, R., \"Deviation and Implementation of an Efficient Fast Fourier \nT \nTransform Algorithm (EFFT),\" Comput. & Chem. (GB), Vol. 10, No. \n4, pp. 241-51, 1986. \nSKINNER, D. P., \"Pruning The Decimation In-Time FFT Algorithm,\" IEEE \nT \nTrans. Acoust., Speech and Signal Process., Vol. ASSP-24, No.2, pp. \n193-94, April 1976. \n436 \nBibliography \nSKOLLERMO, G., \"A Fourier Method for the Numerical Solution of Poisson's \nF \nEquation,\" Math. Comput., Vol. 29, No. 131, pp. 697-711, July 1975. \nSKORMIN, V., \"Frequency Approach to Mathematical Modeling of a Nu-\nV \nclear Power Plant Piping System,\" Journal of Vibration, Acoustics, \nStress, and Reliability in Design, Vol. 107, No.1, pp. 106-11, January \n1985. \nSLOANE, E. A., \"Comparison of Linearly and Quadratically Modified Spec-\nK \ntral Estimates of Gaussian Signals,\" IEEE Trans. Audio and Electroa-\ncoustics, Vol. AU-17, pp. 133-37, June 1969. \nSLOATE, H., \"Matrix Representations for Sorting and the Fast FOUl;er \nT \nTransform,\" IEEE Trans. Circuits and Syst., Vol. CAS-21, No. I, pp. \n109-16, January 1974. \nSMITH, D. E., \"The Acquisition of Electrochemical Response Spectra By \nD \nOn-Line Fast Fourier Transform Data Processing In Electrochemistry,\" \nAnal. Chern., Vol. 48, No.2, pp. 22IA-40, February 1976. \nSMITH, D. E., \"The Enhancement of Electroanalytical Data By On-Line \nJ \nFast Fourier Transform Data Processing In Electrochemistry,\" Anal. \nChern., Vol. 48, No.6, pp. 517-26, May 1976. \nSMITH, J. R., R. McLEAN, AND J. R. ROBBIE, \"Assessment of Hydroturbine \nY \nModels for Power-Systems Studies,\" IEEE Proc. C (GB), Vol. 130, No. \nI, pp. 1-7, January 1983. \nSMITH, W. W., \"Zipping Through FFTs, Software Tools Turn PCs into \nG \nSignal Processors,\" Electron. Des., Vol. 33, No.7, pp. 175-81, March \n1985. \nSODERSTRAND, M. A., T. G. JOHNSON, AND G. A. CLARK, \"Hardware Re-\nP \nalizations of Frequency-sampling Adaptive Filters,\" 1986 IEEE Inter-\nnational Symposium on Circuits and Systems, Vol. 3, pp. 900-903, May \n1986. \nSOOHOO, J., AND G. E. MEVERS, \"Cavity Mode Analysis Using the Fourier \nE \nTransform Method,\" Proc. IEEE, Vol. 62, No. 12, pp. 1721-23, Decem-\nber 1974. \nSORENSEN, H. V., M. T. HEIDEMAN, AND C. S. BURRUS, \"On Computing \nT \nthe Split-Radix FFT,\" IEEE Transactions on Acoustics, Speech, and \nSignal Processing, Vol. ASSP-34, No. I, pp. 152-56, February 1986. \nSOUMEKH, M., \"Image Reconstruction Techniques in Tomographic Imaging \nX \nSystems,\" IEEE Trans. Acoust., Speech & Signal Process., Vol. ASSP-\n34, No.4, pp. 952-62, August 1986. \nSPEAKE, T. c., AND R. M. MERSEREAU, \"Evaluation Of Two-Dimensional \nN \nDiscrete Fourier Transforms Via Generalized FFT Algorithms,\" Pro-\nceedings-ICASSP, IEEE International Conference on Acoustics, \nSpeech and Signal Processing, Vol. 3, pp. 1006-9, Atlanta, Georgia, \nMarch 1981. \nSpecial Issue on Fast Fourier Transforms and Its Application to Digital \nK \nFiltering and Spectral Analysis, IEEE Trans. Audio Electroacoust., Vol. \nAU-15, 1967. \nBibliography \n437 \nSpecial Issue on Fast Fourier Transforms, IEEE Trans. Audio Electroa-\nJ \ncoust., Vol. AU-17, June 1969. \nSpecial Issue on TDM-FDM Conversion, IEEE Transactions on Commu-\nS \nnications, Vol. Com-30, No.7, pp. 489-741, May 1978. \nSpecial Issue on Transmultiplexers, IEEE Transactions on Communica-\nS \ntions, Vol. Com-30, No.7, pp. 1457-1656, July 1982. \nSpecial Issue on Two-Dimensional Digital Signal Processing, IEEE Trans. \nN \nComput., Vol. C-21, 1972. \nSPYRAKOS, C. c., AND D. E. BESKOS, \"Dynamic Response of Frameworks \nV \nBy Fast Fourier Transform,\" Computers and Structures, Vol. 15, No. \n5, pp. 495-505, 1982. \nSREENIVAS, T. V., AND P. V. S. RAo, \"High-Resolution Narrow-BAND Spec-\nK \ntra By FFT Pruning,\" IEEE Transactions on Acoustics, Speech, and \nSignal Processing, Vol. ASSP-28, No.2, pp. 254-57, April 1980. \nSTANGHAN, C. J., AND B. M. MACDONALD, \"Electrical Characterization of \nL \nPackages for High-Speed Integrated Circuits,\" IEEE Transactions on \nComponents, Hybrids and Manufacturing Technology, Vol. CHMT-8, \nNo.4, pp. 468-73, December 1985. \nSTANLEY, W. D., AND S. J. PETERSON, \"Fast Fourier Transforms on Your \nT \nHome Computer,\" Byte, Vol. 3, No. 12, pp. 14, 16, 18,20,22,24-25, \nDecember 1978. \nSTASINSKI, R., \"Comments on 'Bounds on the Minimum Number of Data \nT \nTransfer in WFTA AND FFT Programs',\" IEEE Trans. Acoust., Speech \nand Signal Process., Vol. ASSP-32, No.6, pp. 1255-57, December 1984. \nSTEARNS, S. D., \"Tests of Coherence Unbiasing Methods,\" IEEE Trans. \nK \nAcoust. Speech and Signal Process., Vol. ASSP-29, No.2, pp. 321-23, \nApril 1981. \nSTEFFEN, P. L., \"Exact Calculation of the Impulse Response of Quarter-\nE \nPlane Filters in a Finite Region By Means of the DFT,\" IEEE Trans. \nAcoust., Speech and Signal Process., Vol. ASSP-30, No.4, pp. 608-12, \nAugust 1982. \nSTEPHANISHEN, P. R., AND H. W. CHEN, \"Nearfield Pressures and Surface \nV \nIntensity for Cylindrical Vibrators,\" J. Acoust. Soc. Am., Vol. 76, No. \n3, pp. 942-48, September 1984. \nSTEPHANISHEN, P. R., AND K. C. BENJAMIN, \"Forward and Backward Pro-\n0 \njection of Acoustic Fields Using FFT Methods,\" Journal of \nthe Acoust-\nical Society of \nAmerica, Vol. 71, No.4, pp. 803-12, April 1982. \nSTIGALL, P. D., R. E. ZIEMER, AND L. HUDEC, \"Performance Study of 16-\nG \nBit Microcomputer-Implemented FFT Algorithms,\" IEEE Micro, Vol. 2, \nNo.4, pp. 61-66, November 1982. \nSTIGALL, P. D., R. E. ZIEMER, AND V. T. PHAM, \"Performance Study of a \nQ \nMicrocomputer-Implemented FSK Receiver,\" IEEE Micro, Vol. 1, No. \n1, pp. 43-51, February 1981. \nSTOCKHAM, T. G., \"High Speed Convolution and Correlation,\" AFIPS \nJ \n438 \nBibliography \nProc. Spring Joint Comput. Con/., Washington, DC.: Spartan, Vol. 28, \npp. 229-33, 1966. \nSTONE, H. c., \"Parallel Processing with the Perfect Shuffie,\" IEEE Trans. \nT \non Computers. Vol. C-20, pp. 153-61, February 1971. \nSTRADER, N. R., II, \"Effects of Subharmonic Frequencies on DFT Coef-\nK \nficients,\" Proc. IEEE, Vol. 68, No.2, pp. 285-86, February 1980. \nSTRANG, G., \"Proposal For Toeplitz Matrix Calculations,\" Studies in Ap-\nF \nplied Mathematics, Vol. 74, No.2, pp. 171-76, April 1986. \nSTRUZINSKI, W. A., AND E. D. LOWE, \"Performance Comparison of Four \n0 \nNoise Background Normalization Schemes Proposed For Signal Detec-\ntion Systems,\" Journal of the Acoustical Society of \nAmerica, Vol. 76, \nNo.6, pp. 1738-42, December 1984. \nSUDHAKAR, R., R. c., AGARWAL, AND S. C. DUTTA Roy, \"Fast Computation \nT \nof Fourier Transform at Arbitrary Frequencies,\" IEEE Trans. Circuits \nand Syst., Vol. CAS-28, No. 10, pp. 972-80, October 1981. \nSUGITA, M., \"FFT Spectrum Analyzers Find Wide Applications In Acous-\ntic, Noise And Vibration Analysis,\" JEE, Journal of Electronic Engi-\nneering, Vol. 23, No. 230, pp. 56-58, February 1986. \nSUNDARAMURTHY, M., AND V. U. REDDY, \"Some Results in Fixed Point Fast \nW \nFourier Transform Error Analysis,\" IEEE Trans. Comput., Vol. C-26, \npp. 305-8, 1977. \nSUOBANK, D. W., A. P. YOGANATHAN, E. C. HARRISON, AND W. H. COR. \nA \nCORAN, \"Quantitative Method For the In Vitro Study Of \nSounds Produced \nBy Prosthetic Aortic Heart Valves. Part I. Analytical Considerations,\" \nMedical & Biological Engineering & Computing, Vol. 22, No. I, pp. 32-\n39, January 1984. \nSUZUKI, Y., T. SONE, AND K. I. KIDO, \"A New FFT Algorithm of Radix 3, \nT \n6 AND 12,\" IEEE Transactions on Acoustics, Speech, and Signal Pro-\ncessing, Vol. ASSP-34, No.2, pp. 380-83, April 1986. \nSVERDLIK, M. B., \"Matrix Interpretation AND Computational Efficiency of \nT \nFFT [Fast Fourier Transform] Algorithms,\" Radio Engineering and Elec-\ntronic Physics, Vol. 29, No.2, pp. 60-68, February 1984. \nSVERDLIK, M. B., \"Matrix Interpretation Of The FFT Algorithm For Mu-\nT \ntually Simple Factors,\" Ratio Engineering and Electronic Physics, Vol. \n28, No. 10, pp. 36-43, October 1983. \nSWARTZLANDER, E. JR., D. V. SATISH CHANDRA, H. T. NAGLE, JR., AND S. \nW \nA. STARKS, \"Sign/Logarithm Arithmetic For FFT Implementation,\" \nIEEE Transactions on Computers, Vol. C-32, No.6, pp. 526-34, June \n1983. \nSWARTZLANDER, E., \"Systolic FFT Processors,\" Systolic Arrays. First In-\nG \nternational Workshop, pp. 133-40, 1987. \nSWARTZLANDER, E. E., JR., W. K. W. WENDELL, AND S. J. JOSEPH, \"Radix \nG \n4 Delay Commutator For Fast Fourier Transform Processor Implemen-\ntation,\" IEEE Journal of Solid-State Circuits, Vol. SC-19, No.5, pp. \n702-9, October 1984. \nBibliography \n439 \nSWARZTAUBER, P. N., \"FFT Algorithms For Vector Computers,\" Parallel \nT \nComput, Vol. I, No. I, pp. 45-63, August 1984. \nSWARZTAUBER, P. N., \"Symmetric FFTs,\" Math. Comput., Vol. 47, No. \nT \n175, pp. 323-46, July 1986. \nSWORD, C. K., AND M. SIMAAN, \"Estimation of Mixing Parameters For Can-\nE \ncellation Of Discretized Eddy Current Signals Using Time And Fre-\nquency Domain Techniques,\" Journal of Nondestructive Evaluation, \nVol. 5, No. I, pp. 27-35, March 1985. \nSYSOYEV, V. U., \"Convolution Of A Multifrequency Signal Using Trun-\nT \ncated FFT Algorithms,\" Telecommunications and Radio Engineering, \nVol. 38-39, No. 10, pp. 94-96, October 1984. \nSZAPIEL, S. \"Point-spread Function Computation: Analytic End Correction \nX \nin the Quasi-digital Method,\" J. Opt. Soc. Am. A., Vol. 4, No.4, pp. \n625-8, April 1987. \nSZIKLAS, E. A., AND A. E. SIEGMAN, \"Diffraction Calculations Using Fast \nF \nFourier Transform Methods,\" Proc. IEEE, Vol. 62, No.3, pp. 410-12, \nMarch 1974. \nTAKAMURA, H., Y. OHTA, AND T. MATSUMOTO, \"Symbolic Analysis of \nLinear \nM \nNetworks Using Matrix Partition Method and FFT Algorithm,\" Elec-\ntronics and Communications in Japan, Vol. 65, No. ll, pp. 19-28, No-\nvember 1982. \nTANG, D. T., AND D. LI, \"Time Interval Damage Potential Of Seismic Test-\nV \ning Waveforms,\" Journal of \nPressure Vessel Technology, Transactions \nof \nthe ASME, Vol. 107, No.4, pp. 373-79, November 1985. \nTAYLOR, C. W., K. Y. LEE, AND D. P. DAVE, \"Automatic Generation Con-\nY \ntrol Analysis with Governor Deadband Effects,\" IEEE Trans. Power \nAppar. and Syst., Vol. PAS-98, No.6, pp. 2030-36, November -De-\ncember 1979. \nTAYLOR, F. J., A. S. RAM NARAYAN , AND J. WASSERMAN, \"Non-Invasive \nA \nAneurysm Detection Using Digital Signal Processing,\" Journal of \nBiomedical Engineering, Vol. 5, No.3, pp. 201-10, July 1983. \nTAYLOR, F. J., G. PAPADOURAKIS, A. SKAVANTZOS, AND A. STOURAITIS, \"A \nT \nRadix-4 FFT Using Complex RNS Arithmetic,\" IEEE Transactions On \nComputers, Vol. C-34, No.6, pp. 573-76, June 1985. \nTAYLOR, T. D., R. S. HIRSH, AND M. M. NADWORNY, \"Comparison of FFT, \nV \nDirect Inversion and Conjugate Gradient Methods For Use in Pseudo-\nSpectral Methods,\" Computers & Fluids, Vol. 12, No. I, pp. 1-9,1984. \nTEGOPOULOS, J. A., AND E. E. KRIEZIS, \"Eddy Currents in Linear Con-\nE \nducting Media,\" Studies in Electrical and Electronic Engineering, Vol. \n16, Eddy Currents in Linear Conduct Media. Elsevier, Amsterdam, Neth \nAND New York, NY, USA, p. 304, 1985. \nTEMES, G. c., \"A Worst Case Error Analysis for the FFT,\" IEEE Int. \nW \nSymp. Circuits Syst., Munich, West Germany, pp. 98-101, 1976. \nTEMPERTON, C., \"Self-Sorting Mixed-Radix Fast Fourier Transforms (Nu-\nJ \nmerical Weather Prediction),\" J. Comput. Phys., Vol. 52, No. I, pp. 1-\n23, October 1983. \n440 \nBibliography \nTEtEwSKY A. K., \"Accelerate Your PC's Arithmetic With An Array Pro-\nG \ncessor,\" EDN, Vol. 30, No. 22, pp. 155-64, October 1985. \nTHEILHEIMER, F., \"A Matrix Version of the Fast Fourier Transform,\" IEEE \nT \nTrans. Audio Electroacoust., Vol. AU-17, No.2, pp. 158-61, June 1969. \nTHRANE, N. B., ANDN. D. KJAER, \"Frequency Analysis UsingZoom-FFT,\" \nK \nNoise & Vibration Control Worldwide, (GB), Vol. 12, No, 1, pp. 13-15, \nJanuary-February 1981. \nTILLOTSON, T. C., AND E. O. BRIGHAM, \"Simulation with the Fast Fourier \nZ \nTransform,\" Instruments and Control Systems. Vol. 42, pp. 169-71, Sep-\ntember 1969. \nTIWARI, P. K., M. IBRAHIM, AND O. P. N. CALLA, \"Implementation and \nH \nTesting of FFT Hardware through Microprocessor,\" Journal of \nthe In-\nstitution of \nEngineers (India), Part ET: Electronics & Telecommunication \nEngineering Division, Vol. 66, Pt. 4, pp. 57-61, May 1986. \nTOLIMIERI, R., AND S. WINOGRAD, \"Computing The Ambiguity Surface,\" \nH \nIEEE Trans. Acoust., Speech & Signal Process., Vol. ASSP-33, No.5, \npp. 1239-45, October 1985. \nTOM, V. T., \"Adaptive Filter Techniques for Digital Image Enhancement,\" \nX \nProc. SPIE Int. Soc. Opt. Eng., Vol. 528, pp. 29-42, January 1985. \nTORTOLI, P., G. MANES, AND C. ATZENI, \"Velocity Profile Reconstruction \n0 \nusing Ultrafast Spectral Analysis Of Doppler Ultrasound,\" IEEE Trans-\nactions On Sonics and Ultrasonics, Vol. SU-32, No.4, pp. 555-61, July \n1985. \nTRAN-THONG, AND B. LIU, \"Fixed-point Fast Fourier Transform Error Anal-\nW \nysis,\" IEEE Trans. Acoust. Speech Signal Process., Vol. ASSP-24, pp. \n563-73, 1976. \nTRAN-THONG, AND B. LIU, \"Accumulation of Roundoff Errors in Floating \nW \nPoint FFT,\" IEEE Trans. Circuits Syst., Vol. CAS-24, pp. 132-43, 1974. \nTRETTER, S. A., \"Tracking the Frequency Translation of a Sum of Or-\nQ \nthogonal Sinusoids,\" IEEE Trans. Aerosp. & Electron. Syst., Vol. AES-\n22, No.2, pp. 211-14, March 1986. \nTRIDER, R. c., \"A Fast Fourier Transform (FFT) Based Sonar Signal Pro-\n0 \ncessor,\" 1976 IEEE International Conference on Acoustics, Speech and \nSignal Processing, pp. 389-93, April 1976. \nTSANG, S. H. L., M. W. BENSON, AND R. H. GRANBERG, \"Open and Blocked \nE \nDistributed Air Transmission Lines By the Fast Fourier Transform \nMethod,\" Journal of Dynamic Systems, Measurement AND Control, \nTrans. ASME, Vol. 107, No.3, pp. 213-19, September 1985. \nTSENG, B. D., G. A. J \nULLIEN, AND W. C. MILLER, \"Implementation of FFT \nG \nStructures Using the Residue Number System,\" IEEE Trans. Comput., \nVol. C-28, No. 11, pp. 831-45, November 1979. \nTSENG, B. D., W. C. MILLER, AND G. A. JULLIEN, \"Analysis of Quantization \nT \nError in a Rom Oriented FFT Processor,\" 25th Midwest Symposium on \nCircuits and Systems, pp. 6-8, 1982. \nTSENG, F-I, AND T. K. SARKAR, \"Detection of Branch Points By Modified \nJ \nBibliography \nFFT,\" IEEE Transactions on Geoscience and Remote Sensing, Vol. GE-\n21, No.4, pp. 468-72, October 1983. \n441 \nTSENG, F-I, AND T. K. SARKAR, \"Experimental Determination of Resonant \nE \nFrequencies By Transient Scattering From Conducting Spheres and Cyl-\ninders,\" IEEE Transactions On Antennas and Propagation, Vol. AP-32, \nNo.9, pp. 914-18, September 1984. \nTSENG, F., AND T. K. SARKAR, \"Enhancement of Poles in Spectral Anal-\nK \nysis,\" IEEE Trans. Geosci. and Remote Sensing, Vol. GE-20, No.2, pp. \n161-68, April 1982. \nTSENG, F. I., T. K. SARKAR, AND D. D. WEINER, \"A Novel Window for \nK \nHarmonic Analysis,\" IEEE Trans. Acoust., Speech and Signal Process., \nVol. ASSP-29, No.2, pp. 177-88, April 1981. \nTUFTs, D. W., H. S. HERSEY, AND W. E. MOSIER, \"Effects of FFT Coef-\nW \nficient Quantization on Bin Frequency Response,\" Proc. IEEE, Vol. 60, \npp. 146-47, 1972. \nTWOGOOD, R. E., AND M. P. EKSTROM, \"An Extension Of Eklundh's Matrix \nX \nTransposition Algorithm and Its Application In Digital Image Pro-\ncessing,\" IEEE Trans. Comput., Vol. C-25, No.9, pp. 950-52, September \n1976. \nUENO, M., \"A Systematic Design Formulation for Butler Matrix Applied \nE \nFFT Algorithm,\" IEEE Trans. Antennas and Propag., Vol. AP-29, No. \n3, pp. 496-501, May 1981. \nULRIKSSON, B., \"Conversion Of Frequency-Domain Data To The Time Do-\nT \nmain,\" Proceedings of \nthe IEEE, Vol. 74, No. I, pp. 74-77, January 1986. \nULRIKSSON, B., \"Synthesis Procedure For Designing 90 Degree Directional \nE \nCouplers With a Large Number of Sections,\" IEEE Transactions On \nMicrowave Theory and Techniques, Vol. 30, No.8, pp. 1216-19, August \n1982. \nULRIKSSON, B., \"Time Domain Reflectometer Using A Semiautomatic Net-\nL \nwork Analyzer And The Fast Fourier Transform,\" IEEE Transactions \non Microwave Theory and Techniques, Vol. MTT-29, No.2, pp. 172-74, \nFebruary 1981. \nUMAPATHI, REDDY, V., AND M. SUNDARAMURTHY, \"New Results in Fixed-\nW \nPoint Fast Fourier Transform Error Analysis,\" 1976 IEEE International \nConference on Acoustics, Speech and Signal Processing, pp. 120-25, \nPhiladelphia, PA, April 1976. \nVAN DER AUWERAER, H., AND R. SNOEYS, \"FFT Implementation Alternatives \nG \nin Advanced Measurement Systems,\" IEEE Micro, Vol. 7, No. I, pp. \n39-49, February 1987. \nVARG, P., AND U. HEUTE, \"A Short-time Spectrum Analyzer with Poly-\nK \nphase-network and DFT,\" Signal Process., Vol. 2, pp. 55-65, 1980. \nVASUDEVAN, N., AND A. K. MAL, \"Response Of An Elastic Plate To Lo-\nV \ncalized Transient Sources,\" Journal of \nApplied Mechanics, Transactions \nASME, Vol. 52, No.2, pp. 356-62, June 1985. \nVEENKANT, R. L., \"A Serial Minded FFT,\" IEEE Trans. Audio, Electro-\nT \nacoust., Vol. AU-20, pp. 180-85, 1972. \n442 \nBibliography \nVERLY, J. G., AND T. PEL!, \"Circular Harmonic Analysis of PSF's Corre-\nP \nsponding to Separable Polar-Coordinate Frequency Responses With Em-\nphasis On Fan Filtering,\" IEEE Transactions on Acoustics, Speech, and \nSignal Processing, Vol. ASSP-33, No.1, pp. 300-307, February 1985. \nVERNET, J. L., \"Real Signals Fast Fourier Transform: Storage Capacity and \nT \nStep Number Reduction By Means of an Odd Discrete Fourier Trans-\nform,\" Proc. IEEE, Vol. 59, No. 10, pp. 1531-32, October 1971. \nVETTERLI, M., AND A. LIGTENBERG, \"A Discrete Fourier-cosine Transform \nG \nChip,\" IEEE J. Sel. Areas Commun., Vol. SAC-4, No. I, pp. 49-61, \nJanuary 1986. \n\"Vibration Monitor Simplifies Machinery,\" Diagnostics Diesel and Gas \nTurbine Worldwide, Vol. 16, No.6, July-August 1984. \nVISHNYAKOV, Y. M., AND G. A. KUKHAREV, \"Fast Fourier Transform Pro-\nN \ncedures For The Processing Of Two-Dimensional Signals Without Trans-\nposition Operations,\" Programming AND Computer Software, Vol. 8, No. \n3, pp. 124-28, May-June 1982. \nVITYAZEV, V. V., AND A. I. STEPASHKIN, \"Synthesis of a Digital Filter-\nP \nDemodulator Based On The Double Fast Fourier Transform,\" Telecom-\nmunications and Radio Engineering, Vol. 35/36, No.7, pp. 51-54, July \n1982. \nVLASENKO, V. A., AND Y. M. LAPPA, \"A Matrix Approach to the Construc-\nT \ntion of Fast Multidimensional Discrete Fourier Transform Algorithms,\" \nRadioelectron. & Commun. Syst., Vol. 29, No. I, pp. 87-90, 1986. \nW \nACKERSREUTHER, G., \"On Two-Dimensional Polyphase Filter Banks,\" \nN \nIEEE Transactions on Acoustics, Speech, and Signal Processing, Vol. \nASSP-34, No. I, pp. 192-99, February 1986. \nWALLACH, Y., Al\"D A. SHIMOR, \"Alternating Sequential-parallel Versions \nT \nof the FFT,\" IEEE Trans. on Acoustics, Speech and Signal Processing, \nVol. ASSP-28, No.2, pp. 236-42, April 1980. \nWALTERS, L. c., \"Interpolation In FFTs,\" J Inst Electron Radio Eng. (GB), \nJ \nVol. 55, No. 11-12, pp. 415-19, November-December 1985. \nWEALE, J. R., \"Use of FFT In Microstrip Capacitance Calculations,\" Elec-\nE \ntronics Letters, Vol. 21, No.3, p. 86, January 1985. \nWEBB, R., \"Frequency Domain Instrumentation Techniques For The Con-\nV \ndition Monitoring of Rotating Machinery,\" Noise and Vib. Control \nWorldwide (GB), Vol. 14, No.8, pp. 215-19, October 1983. \nWEBBER, C. L., JR., \"A C-language Program for the Computation of Power \nT \nSpectra on a Laboratory Microcomputer,\" Comput \n. Methods and Pro-\ngrams Biomed. (Netherlands), Vol. 22, No.3, pp. 285-91, June 1986. \nWEE, W. G., AND TSUNG-TAO-HsIEH, \"An Application Of The Projection \nX \nTransform Technique In Image Transmission,\" IEEE Trans. Syst., Man \nand Cybern., Vol. SMC-6, No.7, pp 486-93, July 1976. \nWEINSTEIN, C. J., \"Roundoff Noise in Floating Point Fast Fourier Trans-\nW \nform Computation,\" IEEE Trans. Audio Electroacoust., Vol. AU \n-17, No. \n3, pp. 209-15, September 1969. \nBibliography \n443 \nWELCH, P. D., \"A Fixed Point Fast Fourier Transform Error Analysis,\" \nW \nIEEE Trans. Audio Electroacoust., Vol. AU-17, pp. 151-57, June 1969. \nWELCH, P. D., 'The Use of Fast Fourier Transform for the Estimation of \nK \nPower Spectra: A Method Based on Time Averaging Over Short, Mod-\nified Periodograms,\" IEEE Trans. Audio Electroacoust., Vol. AU-15, pp. \n70-73, June 1967. \nWELLACH, Y., AND A. SHINOR, \"Alternating Sequential-parallel Versions of \nT \nthe FFT,\" IEEE Trans. on Acoustics, Speech and Signal Processing, \nVol. ASSP-28, No.2, pp. 236-42, April 1980. \nWHELCHEL, J. E. W., AND D. F. GUINN, \"FFT Organizations for High Speed \nT \nDigital Filtering,\" IEEE Trans. Audio and Electroacoustics, Vol. AU-\n18, No.2, pp. 159-68, June 1970. \nWHITE, P. H., \"Application of the Fast Fourier Transform to Linear Dis-\nJ \ntributed System Response Calculations,\" Acoustical Society of \nAmerica \nJournal, Vol. 46, Pt. 2, pp. 273-74, July 1969. \nWHITE, S. A., \"A Simple FFT Butterfly Arithmetic Unit,\" IEEE Trans. \nG \nCircuits and Syst., Vol. CAS-28, No.4, pp. 352-55, April 1981. \nWILKEN, W., ANDJ. WEMPEN, \"An FFT-Based, High Resolution Measuring \n0 \nTechnique with Application to Outdoor Ground Impedance at Grazing \nIncidence,\" Noise Control Eng. J., Vol. 27, No.2, pp. 52-60, Sep-\ntember-October 1986. \nWILLEY, T., R. CHAPMAN, H. YOHO, T. S. DURRANI, AND D. PREIS, \"Systolic \nC \nImplementations For Deconvolution, DFT and FFT,\" IEEE Proceed-\nings, Part F, Communications, Radar. and Signal Processing (GB): Vol. \n132, No.6, pp. 466-72, October 1985. \nWILLIAMS, E. G., AND J. D. MAYNARD, \"Numerical Evaluation Of The Ray-\n0 \nleigh Integral For Planar Radiators Using The FFT,\" Journal of the \nAcoustical Society of \nAmerica, Vol. 72, No.6, pp. 2020-30, December \n1982. \nWILLIAMS, E. G., \"Numerical Evaluation Of The Radiation From Unbaf-\n0 \nfled, Finite Plates Using The FFT,\" Journal of \nthe Acoustical Society of \nAmerica, Vol. 74, No. I, pp. 343-47, July 1983. \nWILLIAMS, J. R., \"Fast Beamforming Algorithm,\" Journal of \nthe Acoustical \nSociety of \nAmerica, Vol. 44, No.5, pp. 1454-55, 1968. \nWILLIS, H. L., AND J. V. AANSTOOS, \"Some Unique Signal Processing Ap-\nY \nplications In Power System Planning,\" IEEE Trans. Acoust., Speech and \nSignal Process., Vol. ASSP-27, No.6, Pt. 2, pp. 685-97, December 1979. \nWITTE, H., S. GLASER, AND M. ROTHER, \"New Spectral Detection and Elim-\nA \nination Test Algorithms of ECG and EOG Artifacts In Neonatal EEG \nRecordings,\" Medical & Biological Engineering & Computing, Vol. 25, \nNo.2, pp. 127-30, March 1987. \nWITTIG, L. E., AND A. K. SINHA, \"Simulation Of Multicorrelated RANDom \nF \nProcesses Using The FFT Algorithm,\" J. Acoust. Soc. AM., Vol. 58, \nNo.3, pp. 603-34, September 1975. \nWOLD, E. H., AND A. M. DESPAIN, \"Pipeline AND Parallel-Pipeline FFT \nG \n444 \nBibliography \nProcessors For VLSI Implementations,\" IEEE Transactions on Com-\nputers, Vol. C-33, No.5, pp. 414-26, May 1984. \nWOLINSKI, K., \"Analysis of Errors in Mixed Fast Fourier Transform Al-\nW \ngorithms With Decimation in Frequency For Fixed Point Arithmetic,\" \nProceedings of \nICASSP 82. IEEE International Conference on Acoustics, \nSpeech and Signal Processing, pp. 2089-93, 1982. \nWOOD, S. L., AND M. MORF, \"A Fast Implementation of a Minimum Var-\nA \niance Estimator For Computerized Tomography Image Reconstruction,\" \nIEEE Trans. Biomed. Eng., Vol. BME-28, No.2, pp. 56-68, February \n1981. \nY \nAHYA, R-S, \"Microwave Holography of Large Reflector Antennas-Simu-\nE \nlation Algorithms,\" IEEE Transactions on Antennas and Propagation, \nVol. AP-33, No. 11, pp. 1194-1203, October 1985. \nYAMAGUCHI, T., AND N. ARAKAWA, \"Effects of Finite Kernel Word Length \nW \nIn Signal Processing,\" ICASSP 84. Proceedings of \nthe IEEE International \nConference on Acoustics, Speech and Signal Processing, 30.12/1-4 Vol. \n2, San Diego, CA, March 1984. \nY \nARLAGADDA, R., J. B. BEDNAR, AND T. L. WATT, \"Fast Algorithms For Ip \nC \nDeconvolution,\" IEEE Trans. Acoust., Speech & Signal Process., Vol. \nASSP-33, No.1, pp. 174-82, February 1985. \nYEH, c., AND F. MANSHADI, \"On Weakly Guiding Single-Mode Optical \nE \nWaveguides,\" Journal of \nLightwave Technology, Vol. LT-3, No.1, pp. \n199-205, February 1985. \nYEH, H.-G., \"Power Spectrum Estimation By Using Digital Frequency \nK \nTracking Filter,\" Proceedings of \nthe 1986 American Control Conference, \nVol. 3, pp. 1642-44, June 1986. \nYEH, M., J. L. MELSA, AND D. L. COHN, \"A Direct FFT Scheme For In-\nQ \nterpolation, Decimation, Amplitude Modulation and Single Side Band \nModulation,\" Sixteenth Asilomar Conference on Circuits, Systems and \nComputers, pp. 437-41, Pacific Grove, CA, November 1983. \nYEVICK, D., AND B. HERMANSSON, \"Band Structure Calculation With The \nJ \nSplit-Step Fast Fourier Transform Technique,\" Solid State Communi-\ncations, Vol. 54, No.2, pp. 197-99, April 1985. \nYEVICK, D., AND B. HERMANSSON, \"New Approach To Perturbed Optical \nX \nWaveguides,\" Opt. Lett., Vol. 11, No.2, pp. 103-5, February 1986. \nYEW, C. H., AND C. S. CHEN, \"Study of Linear Wave Motions Using FFT \nV \nAND Its Potential Application To Non-Destructive Testing,\" International \nJournal of \nEngineering Science, (GB), Vol. 18, No.8, pp. 1027-36, 1980. \nYING, S. P., AND E. E. DENNISON, \"Vibration Diagnosis For Turbine-Gen-\nK \nerators,\" Noise & Vibration Control Worldwide, Vol. 12, No.2, pp. 50-\n52, March 1981. \nYIP, P., \"Some Aspects of the Zoom Transform,\" IEEE Trans. Comput., \nT \nVol. C-25, No.3, pp. 287-96, March 1976. \nYLITALO, J., E. ALASARELA, A. TAURIANINEN, K. TERVOLA, AND J. KOIVI-\n0 \nKANGAS, \"Three-dimensional Ultrasound C-scan Imaging Using Holo-\nBibliography \ngraphic Reconstruction,\" IEEE Trans. Ultrason., Ferroelectr. and Freq. \nControl, Vol. UFFC-33, No.6, pp. 731-39, November 1986. \n445 \nYOKOTA, Y., M. TOMITA, H. HASHIMOTO, AND H. ENDOH, \"Construction Of \nX \nAn On-Line System For FFT Processing AND Analysis Of Atomic Res-\nolution Microscopic Images And Its Applications,\" Ultramicroscopy, \nVol. 6, No.4, pp. 313-21, 1981. \nYONG, A., AND M. JUANATEY, \"Microprogrammable Peripheral Unit And \nG \nThe FFT,\" Software & Microsystems, Vol. 4, No.2, pp. 35-39, Apl;1 \n1985. \nYONG, CHING LIM, \"An Interpolation Technique for Computing the DFT of \nT \na Sparse Sequence,\" IEEE Trans. Acoust., Speech & Signal Process., \nVol. ASSP-33, No.6, pp. 1456-60, December 1985. \nYOST, M., F. J. BREMNER, R. J. HELMER, AND M.-E. C. CHINO, \"The Effect \nA \nof Smoothing Functions on Data Obtained from a FFT,\" Behav. Res. \nMethods Instrum. & Comput., Vol. 18, No.2, pp. 263-66, April 1986. \nYOST, R. A., \"On Nonuniform Windowing M-Ary FSK Data in a DFT-\nQ \nBased Detector,\" IEEE Trans. Commun., Vol. COM-28, No. 12, pp. \n2014-19, December 1980. \nZELENKEVICH, V. M., V. A. KAPLUN, AND A. B. TEREKHOVICH, \"Automating \nE \nThe Design of Antenna-Radome Radiating Systems,\" Telecommunica-\ntions and Radio Engineering, Vol. 37-38, No.8, pp. 58-60, August 1983. \nZHONG, L., AND WEN-HONG CHIN, \"A Novel Method for Evaluating the \nX \nResolution and the Wavefront of Plane Diffraction Grating with FFT,\" \nProc. SPIE Int. Soc. Opt. Eng., Vol. 599, pp. 297-302, 1986. \nZIEMER, R. E. \"Computer Evaluation Of A Broadband M-ary Signaling \nS \nScheme and FFT-Processing Receiver For Data Transmission In HF \nChannels,\" 1976 International Conference on Communications, pp. 44/ \n1-6, Philadelphia, PA, June 1976. \nINDEX \nAlgorithm, FFT: \narbitrary factors, 156, 160 \nbase 2, 148 \nbase 4, 157, 162 \nbase 4 + \n2, 158 \nBASIC, 145 \nCooley-Tukey, 152, 154, 158, 160, 163 \nmatrix formulation, 131 \nPASCAL,I47 \nreal data, 188 \nSande-Tukey, 155 \nTwiddle factor, 162 \nAliasing: \nfrequency domain, 83, 172,276,313,324,329 \ntime domain, 83, 95, 200, 282, 321 \nAntenna design analysis, 349 \nAutocorrelation, 69, 365 \nBand-limited, 83, 98, 101 \nBand-pass, 86, 299, 305,315,320,329 \nBase 2 FFT. 148 \nBase 4 FFT, 157, 162 \nBase 4 + 2 FFT, 158 \nBaseband, 86, 324, 327. 329 \nBeamforming, 376 \nBias, 368 \nBit-reversing, 140 \nCanonic FFT forms, 154 \nCepstrum analysis, 341 \nCircular convolution, 123 \nComputer Program, BASIC: \nDolph-Chebyshev, 186 \n446 \nFFT.145 \nFFT convolution, 209 \nFFT of two real functions, 191 \nFFT, two times capability, 194 \ntwo-dimensional FFT, 255 \ntwo-dimensional FFT convolution, 265 \nConfidence intervals, 371, 373 \nConvolution: \ndiscrete (see Discrete convolution) \nFFT (see FFT convolution) \nfrequency theorem, 64, 112 \ngraphical evaluation, 51, 256 \nimpulse functions, 57, 61, 257 \nintegral, 50 \nlimits of integration, 54 \nlinear-sy~tems, 59, 302 \ntime theorem, 60, 112 \ntwo-dimensional, 255 \ntwo-dimensional theorem, 259 \nCooley-Tukey, 154, 158, 160, 163 \nCorrelation: \ncomparison to convolution, 66 \ndiscrete (see Discrete correlation) \nFFT, 225, 264, 358 \ngraphical evaluation, 67 \ntheorem, 65, 113 \ntwo-dimensional, 260 \nCross-correlation, 69, 358 \nCross-spectrum, 358 \nDecimation, 155, 323 \nDecomposition, 42, 110, 189 \nDeconvolution, 345 \nDegrees of freedom, 372 \nDirection finding, 355 \nDiscrete convolution: (see also FFT \nconvolution) \ncircular, 123 \ndefinition, 118 \nfrequency-convolution theorem, 112 \ngraphical development, 119 \nrelationship to continuous convolution, 121 \ntime-convolution theorem, 112 \ntwo-dimensional, 261 \nDiscrete correlation: \ndefinition, 127 \ngraphical evaluation, 127 \ntheorem, 113 \ntwo-dimensional, 264 \nDiscrete Fourier transform: \ndefinition, 97 \nFourier transform relationship, 98 \ngraphical development, 90 \nproperties, 107, 115 \ntheoretical development, 92 \ntwo-dimensional, 240 \nDiscrete inverse Fourier transform, 97, 108, 195 \nDown-conversion, 37, 317, 326 \nDown-sampling, 313, 323, 326 \nDual node pair, 138 \nEcho removal, 341 \nEnd effect, 124 \nFFT antenna-pattern computation, 353 \nFFT averaging, 339 \nFFT band-pass filters: \ncrossover, 30 I \ndefinition, 299 \ndemultiplexing, 315 \ngraphical development, 294, 296 \nhopped filters, 303 \nin-phase filters, 299 \nmultichannel (see Multichannel FFT band-pass \nfiltering) \nquadrature (see Quadrature) \nshifted filters, 303 \nFFT beamforming, 376 \nFFT complex sampling, 310, 317, 336 \nFFT computation flowchart, 141 \nFFT computer program (see Computer program, \nBASIC) \nFFT convolution: \ncomputer program, 209, 265 \ncomputing procedure, 208 \ncomputing speed, 209, 220, 263 \ndata restructuring, 206, 263 \nefficient, 223 \noverlap-add, 217, 222 \noverlap-save, 212, 221 \nsectioning, 211 \nselect-save, 221 \ntwo-dimensional, 260 \nFFT correlation, 225, 264, 358 \nFFT deconvolution, 347 \nFFT digital filters (see Filter design) \nFFT power-spectrum analysis, 365 \nFFT signal detection, 183, 337 \nFFT system simulation, 360 \nFFT time-ditTerence-of-arrival, 357 \nFilter design: \nband-pass (see FFT band-pass filters) \ndeconvolution, 346 \nfrequency domain specification, 280 \nintegrate and sample (see FFT band-pass \nfilters) \ninverse, 346 \nmatched, 340 \nmultichannel band-pass (see Multichannel FFT \nband-pass filtering) \nnotch,284 \nrecursive, 272 \nripple, 90, 100, 178,276,284,315 \ntime domain aliasing, 282 \ntime domain specification, 273 \nFlow chart (see Signal flow graph) \nFourier integral, 9 \nFourier series, 74, 77 \nFourier transform: \ndefinition, 9, 22 \ndiscrete (see Discrete Fourier transform) \nexistence, 13, 17 \nFFT computation, 167 \ninterpretation, 4 \ninverse, II \npairs, 23 \nproperties, 30, 46 \nrelationship to discrete transform, 98 \nsimultaneous, 45 \ntable, 24 \ntwo-dimensional, 233 \nFrequency: \nconvolution theorem, 64, 112 \nsampling theorem, 86 \nscaling, 33 \nshifting, 37, 108 \nFunctions: \nband-limited, 83, 98, 101 \nband-pass, 86, 299, 305, 315, 320, 329 \nbaseband, 86, 324, 327, 329 \ncomplex, 44, III \ndecomposition, 42, 110, 189 \ndiscontinuous, 22, 103, 167, 177 \nDolph-Chebyshev, 183, 288 \neven, 40, 109 \nHanning (see Hanning function) \nImpulse (see Impulse function) \nnoncausal, 175,250, 353 \nodd, 41, 110 \nrectangular (see Rectangular function) \nsampling, 80, 90, 92, 98, 103, 258, 322 \nseparable, 237 \ntesting, 389 \ntruncation (see Weighting function; Window) \nHanning function, 181,252, 276, 285, 302, 315, \n346, 369 \nHopped FFTs, 303 \nImpUlse function: \nconvolution, 57, 257, 391 \ndefinition, 18, 386 \nmultiplication, 80, 257, 323, 391 \nproperties, 390 \nIn-phase: \nfilter bank, 299 \nreference sinusoid, 328 \n447 \nIn-place computation, 138, 155 \nInterferometer, 355 \nInterpolation, 199, 333, 358 \nInverse discrete Fourier transform, 97, 195,252 \nInverse Fourier transform: \nalternate formula, 40, 108 \ndefinition, 11, 22 \ntwo-dimensional, 239 \nLagged products, 127,367 \nLaplace transform, 23, 199 \nLeakage (see Sidelobes) \nLinearity, 30, 59, 107 \nMatched filters, 340 \nMatrix FFT formulation, 131 \nModulation, 37, 65, 313, 322, 324, 329, 333 \nMonopulse system, 357 \nMultichannel FFT band-pass filtering: \ncomplex samples, 310, 336 \nde \nmultiplexing, 315 \ngraphical development, 304 \nsample rate considerations, 313 \ntheoretical development, 305 \nMultipath removal, 341 \nNyquist sampling rate, 84, 172,254,322,336 \nOverlap-add sectioning, 217 \nOverlap effect, 121, 124, 206 \nOverlap-save sectioning, 212 \nParseval's theorem, 23 \nPASCAL FFT computer program, 147 \nPeriodogram, 367 \nPhase-interferometer, 355 \nPower spectrum, 365 \nQuadrature: \nFFT complex sampling, 336 \nfilter bank, 299 \nreference sinusoid, 329 \nsampling, 313, 327 \nsignal reconstruction, 331 \nRecord length (see Weighting function; Window) \nRectangular function, 90, 98, 103, 172, 178, 181, \n187, 276, 282, 295, 303 \nResolution, 170, 180, 187,243,300,317,337 \nRipple, 90, 94, 172, 346 (see also Sidelobes and \nFilter design) \nSampling: \nband-pass sampling theorem, 322 \nbandwidth,314 \ncomplex, 310, 336 \ndown-sampling, 313, 323, 326 \nfrequency domain, 86, 92, 94, 106, 195, 281 \nquadrature (see Quadrature) \ntheorem, frequency domain, 86 \ntheorem. time domain, 83 \n448 \ntime domain, 80, 90, 92, 98, 103, 258, 322 \ntwo-dimensional, 254 \nSande-Tukey, 155 \nScaling: \nfrequency domain, 33 \ntime domain, 32 \nShifted FFTs, 303 \nSidelobes, 103, 178, 181, 185, 187, 276, 284, 300, \n317,368 \nSignal detection, 183, 337 \nSignal flow graph: \nbase 2, N = 16, 137 \nbase 4, N = 16, 159 \nbase 4 + 2, N=8, 189 \ncanonic forms, 152, 156 \nSimulation, 360 \nSingular function (see Impulse function) \nSmoothed periodogram, 369, 371 \nSpatial, 233, 243 \nSpectral analysis, 365 \nSymmetry, 32, 107 \nTemporal, 233 \nTesting function, 389 \nTime-difference-of-arrival, 357 \nTime scaling, 32 \nTime shifting, 35, 107 \nTransmission bandwidth, 314 \nTruncation function (see Weighting function; \nWindow) \nTwo-dimensional: \narray processing, 379 \nconvolution, 255 \nconvolution theorem, 259 \ncorrelation, 260 \nFFT (see Two-dimensional FFT) \nsampling, 254 \nwindows, 250 \nTwo-dimensional FFT: \nalternate computing procedure, 246 \nArray processing, 379 \nBASIC computer program, 255 \ncomputational reorganization, 243 \nconvolution, 260 \ngraphical development, 241 \ninverse, 252 \nperiodicity, 247 \nsuccessive one-dimensional transforms, 238 \nwindows, 250 \nUnscrambling, 141 \nWP computation, 140 \nWP factorization, 149 \nWaveforms (see Functions) \nWeighting function: \ncomparisons, 182 \nDolph-Chebyshev, 183 \nfilter bank interpretation, 300 \nHanning (see Hanning function) \nrectangular (see Rectangular function) \ntwo-dimensional, 250 \nWindow (see also Weighting function): \nbandwidth, 182, 368 \nBartlett, 181,367 \nDaniel,369 \nspectral, 368 \n"
}